// Copyright Â© 2020-2022 Wei Shen <shenwei356@gmail.com>
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
// THE SOFTWARE.

package cmd

import (
	"bufio"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"regexp"
	"runtime"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/pkg/errors"
	"github.com/shenwei356/kmcp/kmcp/cmd/index"
	"github.com/shenwei356/unik/v5"
	"github.com/shenwei356/util/bytesize"
	"github.com/shenwei356/util/math"
	"github.com/shenwei356/util/pathutil"
	"github.com/spf13/cobra"
	"github.com/twotwotwo/sorts"
	"github.com/twotwotwo/sorts/sortutil"
	"github.com/vbauerster/mpb/v5"
	"github.com/vbauerster/mpb/v5/decor"
	"github.com/zeebo/xxh3"
	"gopkg.in/yaml.v2"
)

var indexCmd = &cobra.Command{
	Use:   "index",
	Short: "Construct database from k-mer files",
	Long: `Construct database from k-mer files

We build the index for k-mers (sketches) with a modified compact bit-sliced
signature index (COBS). We totally rewrite the algorithms, data structure,
and file format, and have improved the indexing and searching speed.

Input:
  The output directory generated by "kmcp compute".

Database size and searching accuracy:
  0. Use --dry-run to adjust parameters and check the final number of 
     index files (#index-files) and the total file size.
  1. -f/--false-positive-rate: the default value 0.3 is enough for a
     query with tens of k-mers (see BIGSI/COBS paper).
     Small values could largely increase the size of the database.
  2. -n/--num-hash: large values could reduce the database size,
     at the cost of a slower searching speed. Values <=4 are recommended.
  3. The value of block size -b/--block-size better to be multiple of 64.
     The default value is:  (#unikFiles/#threads + 7) / 8 * 8
  4. Use flag -x/--block-sizeX-kmers-t, -8/--block-size8-kmers-t,
     and -1/--block-size1-kmers-t to separately create indexes for
     inputs with a huge number of k-mers, for precise control of
     database size.

References:
  1. COBS: https://arxiv.org/abs/1905.09624

Taxonomy data:
  1. No taxonomy data are included in the database.
  2. Taxonomy information are only needed in "profile" command.
  
Performance tips:
  1. The number of blocks (.uniki files) is better be smaller than
     or equal to the number of CPU cores for faster searching speed. 
     We can set the flag -j/--threads to control the blocks number.
     When more threads (>= 1.3 * #blocks) are given, extra workers are
     automatically created.
  2. #threads files are simultaneously opened, and the max number
     of opened files is limited by the flag -F/--max-open-files.
     You may use a small value of -F/--max-open-files for 
     hard disk drive storages.
  3. When the database is used in a new computer with more CPU cores,
     'kmcp search' could automatically scale to utilize as many cores
     as possible.

Next step:
  1. Use 'kmcp search' for searching.
  2. Use 'kmcp utils ref-info' to check the number of k-mers and FPR
     of each genome chunk.

Examples:
  1. For bacterial genomes:

       kmcp index -f 0.3 -n 1 -j 32 -I gtdb-k21-n10-l150/ -O gtdb.kmcp

  2. For viruses, use -x and -8 to control index size of the largest chunks:

       kmcp index -f 0.05 -n 1 -j 32 -x 100K -8 1M \
           -I genbank-viral-k21-n10-l150/ -O genbank-viral.kmcp

`,
	Run: func(cmd *cobra.Command, args []string) {
		opt := getOptions(cmd)

		var fhLog *os.File
		if opt.Log2File {
			fhLog = addLog(opt.LogFile, opt.Verbose)
		}
		timeStart := time.Now()
		defer func() {
			if opt.Verbose || opt.Log2File {
				log.Info()
				log.Infof("elapsed time: %s", time.Since(timeStart))
				log.Info()
			}
			if opt.Log2File {
				fhLog.Close()
			}
		}()

		// ---------------------------------------------------------------
		// basic flags

		var err error

		dryRun := getFlagBool(cmd, "dry-run")
		if dryRun {
			opt.Verbose = true
		}

		outDir := getFlagString(cmd, "out-dir")
		if outDir == "" {
			checkError(fmt.Errorf("flag -O/--out-dir is needed"))
		}

		inDir := getFlagString(cmd, "in-dir")
		if inDir == "" {
			checkError(fmt.Errorf("flag -I/--in-dir is needed"))
		}

		if filepath.Clean(inDir) == filepath.Clean(outDir) {
			checkError(fmt.Errorf("intput and output paths should not be the same: %s", outDir))
		}

		readFromDir := inDir != ""
		if readFromDir {
			var isDir bool
			isDir, err = pathutil.IsDir(inDir)
			if err != nil {
				checkError(errors.Wrapf(err, "checking -I/--in-dir"))
			}
			if !isDir {
				checkError(fmt.Errorf("value of -I/--in-dir should be a directory: %s", inDir))
			}
		}

		reFileStr := getFlagString(cmd, "file-regexp")
		var reFile *regexp.Regexp
		if reFileStr != "" {
			if !reIgnoreCase.MatchString(reFileStr) {
				reFileStr = reIgnoreCaseStr + reFileStr
			}
			reFile, err = regexp.Compile(reFileStr)
			checkError(errors.Wrapf(err, "parsing regular expression for matching file: %s", reFileStr))
		}

		force := getFlagBool(cmd, "force")

		alias := getFlagString(cmd, "alias")

		// ---------------------------------------------------------------
		// index flags

		sBlock00 := getFlagInt(cmd, "block-size")

		fpr := getFlagPositiveFloat64(cmd, "false-positive-rate")
		if fpr >= 1 {
			checkError(fmt.Errorf("value of -f/--false-positive-rate too big: %f", fpr))
		}
		numHashes := getFlagPositiveInt(cmd, "num-hash")
		if numHashes > 4 {
			checkError(fmt.Errorf("value of -n/--num-hash too big: %d", numHashes))
		}

		// At the beginning, I set the size of bloom as power of 2, so we can accelerate
		// location computation by replacing 'hash % size' with 'hash & (size-1)'.
		// But after a long time, I found it would increase the index size, and
		// the speed is actually not faster. So we use the classical way, while the code
		// in 'index' and 'search' command are not removed.

		// faster := getFlagBool(cmd, "faster")
		faster := false

		maxOpenFiles := getFlagPositiveInt(cmd, "max-open-files")
		// maxWriteFiles := getFlagPositiveInt(cmd, "max-write-files")

		// block-sizeX-kmers-t
		kmerThresholdXStr := getFlagString(cmd, "block-sizeX-kmers-t")
		kmerThresholdXFloat, err := bytesize.ParseByteSize(kmerThresholdXStr)
		if err != nil {
			checkError(fmt.Errorf("invalid size: %s", kmerThresholdXStr))
		}
		if kmerThresholdXFloat <= 0 {
			checkError(fmt.Errorf("value of flag -8/--block-sizeX-kmers-t should be positive: %d", kmerThresholdXFloat))
		}

		kmerThresholdX := uint64(kmerThresholdXFloat)

		blockSizeX := getFlagPositiveInt(cmd, "block-sizeX")
		if blockSizeX <= 8 {
			checkError(fmt.Errorf("value of flag -X/--block-sizeX should be greater than 8: %d", blockSizeX))
		}
		if blockSizeX%8 != 0 {
			checkError(fmt.Errorf("value of flag -X/--block-sizeX should be a multiple of 8: %d", blockSizeX))
		}

		// block-size8-kmers-t
		kmerThreshold8Str := getFlagString(cmd, "block-size8-kmers-t")
		kmerThreshold8Float, err := bytesize.ParseByteSize(kmerThreshold8Str)
		if err != nil {
			checkError(fmt.Errorf("invalid size: %s", kmerThreshold8Str))
		}
		if kmerThreshold8Float <= 0 {
			checkError(fmt.Errorf("value of flag -8/--block-size8-kmers-t should be positive: %d", kmerThreshold8Float))
		}
		kmerThreshold8 := uint64(kmerThreshold8Float)
		if kmerThresholdX >= kmerThreshold8 {
			checkError(fmt.Errorf("value of flag -x/--block-sizeX-kmers-t (%d) should be small than -8/--block-size8-kmers-t (%d)", kmerThresholdX, kmerThreshold8))
		}

		// block-size1-kmers-t
		kmerThreshold1Str := getFlagString(cmd, "block-size1-kmers-t")
		kmerThreshold1Float, err := bytesize.ParseByteSize(kmerThreshold1Str)
		if err != nil {
			checkError(fmt.Errorf("invalid size: %s", kmerThreshold1Str))
		}
		if kmerThreshold1Float <= 0 {
			checkError(fmt.Errorf("value of flag -1/--block-size1-kmers-t should be positive: %d", kmerThreshold1Float))
		}
		kmerThreshold1 := uint64(kmerThreshold1Float)

		if kmerThreshold8 >= kmerThreshold1 {
			checkError(fmt.Errorf("value of flag -8/--block-size8-kmers-t (%d) should be small than -1/--block-size1-kmers-t (%d)", kmerThreshold8, kmerThreshold1))
		}

		// ---------------------------------------------------------------
		// note: 2021-08-12
		// RAMBO index is not suitable for single machine,
		// It's require bigger space, the speed is slow, and it bring higher false positive rate.
		// So I decide to hide this from users.

		// index flags
		//  numRepeats := getFlagPositiveInt(cmd, "num-repititions")
		numRepeats := 1
		// numBuckets := getFlagNonNegativeInt(cmd, "num-buckets")
		numBuckets := 0
		if numRepeats < 1 {
			checkError(fmt.Errorf("value of -R/--num-repititions should be >= 1"))
		}
		if numBuckets > 0 && sBlock00 > numBuckets {
			checkError(fmt.Errorf("value of -b/--block-size (%d) should be small than -B/--num-buckets (%d)", sBlock00, numBuckets))
		}
		// seed := getFlagPositiveInt(cmd, "seed")
		seed := 1

		// ---------------------------------------------------------------
		// out dir

		if outDir == "" {
			if inDir != "" {
				outDir = filepath.Clean(inDir) + ".kmcp-db"
			} else {
				outDir = "kmcp-db"
			}
		}
		if !dryRun {
			makeOutDir(outDir, force)
		}
		if alias == "" {
			alias = filepath.Base(outDir)
		}

		// ---------------------------------------------------------------
		// cached file info

		fileInfoCache := filepath.Join(inDir, fileUnikInfos)

		var hasInfoCache bool
		var InfoCacheOK bool

		var k int = -1
		var ks []int
		var hashed bool
		var canonical bool
		var scaled bool
		var scale uint32
		var meta0 Meta

		var files []string
		var nfiles int
		var n uint64
		var namesMap0 map[string]interface{}

		var reader0 *unik.Reader

		// a method to retrieve basic information from an .unik file
		getInfo := func(file string, first bool) UnikFileInfo {
			infh, r, _, err := inStream(file)
			checkError(err)

			reader, err := unik.NewReader(infh)
			checkError(errors.Wrap(err, file))

			var meta Meta

			if len(reader.Description) > 0 {
				err := json.Unmarshal(reader.Description, &meta)
				if err != nil {
					checkError(fmt.Errorf("unsupported metadata: %s", reader.Description))
				}
			}

			if first {
				reader0 = reader

				ks = meta.Ks
				sortutil.Ints(ks)
				k = ks[len(ks)-1]

				hashed = reader.IsHashed()
				if !hashed {
					checkError(fmt.Errorf(`flag 'hashed' is supposed to be true, are the files created by 'kmcp compute'? %s`, file))
				}

				canonical = reader.IsCanonical()
				if !canonical {
					checkError(fmt.Errorf(`files with 'canonical' flag needed: %s`, file))
				}
				scaled = reader.IsScaled()
				scale = reader.GetScale()

				meta0 = meta
			} else {
				checkCompatibility(reader0, reader, file, &meta0, &meta)
				if scaled && scale != reader.GetScale() {
					checkError(fmt.Errorf(`scales not consistent, please check with "kmcp utils unik-info": %s`, file))
				}
			}

			if reader.Number == 0 {
				checkError(fmt.Errorf("binary file not sorted or no k-mers number found: %s", file))
			}

			checkError(r.Close())
			return UnikFileInfo{Path: file, Name: meta.SeqID, Index: meta.FragIdx, Kmers: reader.Number}
		}

		fileInfos0 := make([]UnikFileInfo, 0, 1024)

		// the fileInfoCache is added in later version, so it's optional in code
		// but it always exist now.
		hasInfoCache, err = pathutil.Exists(fileInfoCache)
		if err != nil {
			checkError(fmt.Errorf("check .unik file info file: %s", err))
		}

		if opt.Verbose || opt.Log2File {
			log.Infof("kmcp v%s", VERSION)
			log.Info("  https://github.com/shenwei356/kmcp")
			log.Info()
		}

		if hasInfoCache {
			if opt.Verbose || opt.Log2File {
				log.Infof("loading .unik file infos from file: %s", fileInfoCache)
			}

			// read
			fileInfos0, err = readUnikFileInfos(fileInfoCache)
			if err != nil {
				checkError(fmt.Errorf("fail to read fileinfo cache file: %s", err))
			}

			nfiles = len(fileInfos0)
			if opt.Verbose || opt.Log2File {
				log.Infof("  %d cached file infos loaded", nfiles)
			}

			if len(fileInfos0) == 0 {
				InfoCacheOK = false
			} else {
				namesMap0 = make(map[string]interface{}, 1024)

				for _, info := range fileInfos0 {
					n += info.Kmers
					namesMap0[info.Name] = struct{}{}
				}

				// read some basic data
				getInfo(fileInfos0[0].Path, true)

				InfoCacheOK = true
			}
		}

		if hasInfoCache && InfoCacheOK {
			// do not have to check file again
		} else {
			// in old days when the 'compute' do not output file info table,
			// all .unik files are parsed to retrieve basic informations.

			// ---------------------------------------------------------------
			// input files

			if opt.Verbose || opt.Log2File {
				log.Info("checking input files ...")
			}

			if readFromDir {
				files, err = getFileListFromDir(inDir, reFile, opt.NumCPUs)
				checkError(errors.Wrapf(err, "err on walking dir: %s", inDir))
				if len(files) == 0 {
					log.Warningf("  no files matching patttern: %s", reFileStr)
				}
			} else {
				files = getFileListFromArgsAndFile(cmd, args, true, "infile-list", true)
				if opt.Verbose || opt.Log2File {
					if len(files) == 1 && isStdin(files[0]) {
						log.Info("  no files given, reading from stdin")
					}
				}
			}
			if opt.Verbose || opt.Log2File {
				log.Infof("  %d input file(s) given", len(files))
			}
			nfiles = len(files)

			if nfiles == 0 {
				return
			}

			// ---------------------------------------------------------------
			// check unik files and read k-mers numbers

			if opt.Verbose || opt.Log2File {
				log.Info("checking .unik files ...")
			}

			var pbs *mpb.Progress
			var bar *mpb.Bar
			var chDuration chan time.Duration
			var doneDuration chan int

			if opt.Verbose || opt.Log2File {
				pbs = mpb.New(mpb.WithWidth(40), mpb.WithOutput(os.Stderr))
				bar = pbs.AddBar(int64(len(files)),
					mpb.BarStyle("[=>-]<+"),
					mpb.PrependDecorators(
						decor.Name("checking .unik file: ", decor.WC{W: len("checking .unik file: "), C: decor.DidentRight}),
						decor.Name("", decor.WCSyncSpaceR),
						decor.CountersNoUnit("%d / %d", decor.WCSyncWidth),
					),
					mpb.AppendDecorators(
						decor.Name("ETA: ", decor.WC{W: len("ETA: ")}),
						decor.EwmaETA(decor.ET_STYLE_GO, 10),
						decor.OnComplete(decor.Name(""), ". done"),
					),
				)

				chDuration = make(chan time.Duration, opt.NumCPUs)
				doneDuration = make(chan int)
				go func() {
					for t := range chDuration {
						bar.Increment()
						bar.DecoratorEwmaUpdate(t)
					}
					doneDuration <- 1
				}()
			}

			// first file
			file := files[0]
			var t time.Time
			if opt.Verbose || opt.Log2File {
				t = time.Now()
			}

			info := getInfo(file, true)
			n += info.Kmers
			if opt.Verbose || opt.Log2File {
				bar.Increment()
				bar.DecoratorEwmaUpdate(time.Since(t))
			}

			fileInfos0 = append(fileInfos0, info)
			namesMap0 = make(map[string]interface{}, 1024)
			namesMap := make(map[uint64]interface{}, nfiles)
			namesMap[xxh3.HashString(fmt.Sprintf("%s%s%d", info.Name, sepNameIdx, info.Index))] = struct{}{}
			namesMap0[info.Name] = struct{}{}

			// left files
			var wgGetInfo sync.WaitGroup
			chInfos := make(chan UnikFileInfo, opt.NumCPUs)
			tokensGetInfo := make(chan int, opt.NumCPUs)
			doneGetInfo := make(chan int)
			go func() {
				var ok bool
				var nameHash uint64
				for info := range chInfos {
					fileInfos0 = append(fileInfos0, info)
					n += info.Kmers

					nameHash = xxh3.HashString(fmt.Sprintf("%s%s%d", info.Name, sepNameIdx, info.Index))
					if _, ok = namesMap[nameHash]; ok {
						log.Warningf("duplicated name: %s", info.Name)
					} else {
						namesMap[nameHash] = struct{}{}
						if _, ok = namesMap0[info.Name]; !ok {
							namesMap0[info.Name] = struct{}{}
						}
					}
				}
				doneGetInfo <- 1
			}()

			for _, file := range files[1:] {
				wgGetInfo.Add(1)
				tokensGetInfo <- 1
				go func(file string) {
					defer func() {
						wgGetInfo.Done()
						<-tokensGetInfo
					}()
					var t time.Time
					if opt.Verbose || opt.Log2File {
						t = time.Now()
					}

					chInfos <- getInfo(file, false)

					if opt.Verbose || opt.Log2File {
						chDuration <- time.Duration(float64(time.Since(t)) / float64(opt.NumCPUs))
					}
				}(file)
			}

			wgGetInfo.Wait()
			close(chInfos)
			<-doneGetInfo

			if opt.Verbose || opt.Log2File {
				close(chDuration)
				<-doneDuration
				pbs.Wait()
			}

			if opt.Verbose || opt.Log2File {
				log.Infof("  finished checking %d .unik files", nfiles)
			}
		}

		// ------------------------------------------------------------------------------------
		// .unik info

		if !hasInfoCache || !InfoCacheOK { // dump to info file
			log.Infof("write unik file info to file: %s", fileInfoCache)
			dumpUnikFileInfos(fileInfos0, fileInfoCache)
		}

		// ------------------------------------------------------------------------------------
		// begin creating index
		if opt.Verbose || opt.Log2File {
			log.Info()
			log.Infof("-------------------- [main parameters] --------------------")
			log.Infof("  number of hashes: %d", numHashes)
			log.Infof("  false positive rate: %f", fpr)

			log.Infof("  k-mer size(s): %s", strings.Join(IntSlice2StringSlice(meta0.Ks), ", "))
			if meta0.Minimizer {
				log.Infof("  minimizer window: %d", meta0.MinimizerW)
			}
			if meta0.Syncmer {
				log.Infof("  closed syncmer size: %d", meta0.SyncmerS)
			}
			if meta0.SplitSeq {
				log.Infof("  split seqequence size: %d, overlap: %d", meta0.SplitSize, meta0.SplitOverlap)
			}
			if scaled {
				log.Infof("  down-sampling scale: %d", scale)
			}

			bytesize.FullUnit = false
			log.Infof("  block-sizeX-kmers-t: %s", bytesize.ByteSize(kmerThresholdX))
			log.Infof("  block-sizeX        : %d", blockSizeX)
			log.Infof("  block-size8-kmers-t: %s", bytesize.ByteSize(kmerThreshold8))
			log.Infof("  block-size1-kmers-t: %s", bytesize.ByteSize(kmerThreshold1))
			bytesize.FullUnit = true
			log.Infof("-------------------- [main parameters] --------------------")
			log.Info()
			log.Infof("building index ...")
		}

		// ------------------------------------------------------------------------------------

		singleRepeat := numRepeats == 1
		var singleSet bool
		if numBuckets == 0 { // special case, just like BIGSI/cobs
			singleSet = true
			numBuckets = len(fileInfos0)
			numRepeats = 1
		}
		numBucketsUint64 := uint64(numBuckets)

		var fileSize0 float64

		var totalIndexFiles int

		var pbs *mpb.Progress

		// repeatedly randomly shuffle names into buckets
		for rr := 0; rr < numRepeats; rr++ {
			dirR := fmt.Sprintf("R%03d", rr+1)
			runtime.GC()

			buckets := make([][]UnikFileInfo, numBuckets)
			var bIdx int
			var h1, h2 uint32
			for jj, info := range fileInfos0 {
				if singleSet {
					bIdx = jj
				} else {
					h1, h2 = baseHashes(xxh3.HashString(info.Path))
					bIdx = int(uint64(h1+h2*uint32(rr+seed)) % numBucketsUint64) // add seed
				}

				if buckets[bIdx] == nil {
					buckets[bIdx] = make([]UnikFileInfo, 0, 8)
				}
				buckets[bIdx] = append(buckets[bIdx], info)
			}

			fileInfoGroups := make([]UnikFileInfoGroup, len(buckets))
			for bb, infos := range buckets {
				var totalKmers uint64
				for _, info := range infos {
					totalKmers += info.Kmers
				}
				fileInfoGroups[bb] = UnikFileInfoGroup{Infos: infos, Kmers: totalKmers}
			}

			// sort by group kmer size
			sorts.Quicksort(UnikFileInfoGroups(fileInfoGroups))

			nFiles := len(fileInfoGroups)
			var sBlock int
			if sBlock00 <= 0 { // block size from command line
				sBlock = (int(float64(nFiles)/float64(opt.NumCPUs)) + 7) / 8 * 8 // just ensure it's 8n
			} else {
				sBlock = sBlock00
			}

			var skipBlockX bool
			if blockSizeX >= sBlock {
				skipBlockX = true
				log.Warningf("ignore -X/--block-size (%d) which is >= -b/--block-size (%d)", blockSizeX, sBlock)
				blockSizeX = sBlock
			}

			if sBlock < 8 {
				sBlock = 8
			} else if sBlock > nFiles {
				sBlock = nFiles
			}

			if opt.Verbose || opt.Log2File {
				log.Info()
				if singleRepeat {
					log.Infof("  block size: %d", sBlock)
					log.Infof("  number of index files: %d (may be more)", opt.NumCPUs)
				} else {
					log.Infof("  [Repeat %d/%d] block size: %d", rr+1, numRepeats, sBlock)
				}
				log.Info()
			}

			if opt.Verbose {
				pbs = mpb.New(mpb.WithWidth(40), mpb.WithOutput(os.Stderr))
			}

			// really begin
			nIndexFiles := int((nFiles + sBlock - 1) / sBlock) // may be more if using -m and -M
			indexFiles := make([]string, 0, nIndexFiles)

			var barW *mpb.Bar // bar for dumping index files
			var chDurationW chan time.Duration
			var doneDurationW chan int
			if opt.Verbose && !dryRun {
				prefix := "[saved index files]"
				barW = pbs.AddBar(int64(opt.NumCPUs),
					mpb.PrependDecorators(
						decor.Name(prefix, decor.WC{W: len(prefix) + 1, C: decor.DidentRight}),
						decor.CountersNoUnit("%d / %d", decor.WCSyncWidth),
					),
					// mpb.AppendDecorators(decor.Percentage(decor.WC{W: 5})),
					mpb.AppendDecorators(
						decor.Name("ETA: ", decor.WC{W: len("ETA: ")}),
						decor.EwmaETA(decor.ET_STYLE_GO, 1),
						decor.OnComplete(decor.Name(""), ". done"),
					),
					mpb.BarFillerClearOnComplete(),
					mpb.BarPriority(math.MaxInt),
				)

				chDurationW = make(chan time.Duration, opt.NumCPUs)
				doneDurationW = make(chan int)
				go func() {
					for t := range chDurationW {
						barW.Increment()
						barW.DecoratorEwmaUpdate(t)
					}
					doneDurationW <- 1
				}()

			}

			ch := make(chan string, nIndexFiles)
			done := make(chan int)
			go func() {
				for f := range ch {
					indexFiles = append(indexFiles, f)
				}
				done <- 1
			}()

			var fileSize float64
			chFileSize := make(chan float64, nIndexFiles)
			doneFileSize := make(chan int)
			go func() {
				for f := range chFileSize {
					fileSize += f
				}
				doneFileSize <- 1
			}()

			var prefix string

			var b int
			var wg0 sync.WaitGroup
			// maxConc := opt.NumCPUs
			maxConc := 2
			if dryRun {
				maxConc = 1 // just for logging in order
			} else if (sBlock / 8) < opt.NumCPUs {
				maxConc = opt.NumCPUs / (sBlock / 8)
				if maxConc < 2 {
					maxConc = 2
				}
			}

			tokens0 := make(chan int, maxConc)
			tokensOpenFiles := make(chan int, maxOpenFiles)
			// tokensWriteFiles := make(chan int, maxWriteFiles)

			sBlock0 := sBlock // save for later use

			batch := make([][]UnikFileInfo, 0, sBlock)
			var flagX, flag8, flag bool
			var lastInfos []UnikFileInfo
			var infoGroup UnikFileInfoGroup

			for i := 0; i <= nFiles; i++ {
				if i == nFiles { // process lastInfo
					// add previous file to batch
					if flag || flag8 || flagX {
						if lastInfos != nil {
							batch = append(batch, lastInfos)
							lastInfos = nil
						}
					}
				} else {
					infoGroup = fileInfoGroups[i]
					infos := infoGroup.Infos
					if infoGroup.Kmers == 0 { // skip empty buckets
						continue
					}

					if flag || flag8 || flagX {
						// add previous file to batch
						if lastInfos != nil {
							batch = append(batch, lastInfos)
							lastInfos = nil
						}

						if flag { // single
							lastInfos = infos // leave this file process in the next round
							// and we have to process files aleady in batch
						} else if infoGroup.Kmers > kmerThreshold1 { // !flag && (flag8 || flagx) -> flag
							// meet a very big file the first time
							flag = true       // mark
							lastInfos = infos // leave this file process in the next round
							// and we have to process files aleady in batch
						} else if skipBlockX {
							batch = append(batch, infos)
							if len(batch) < sBlock { // not filled
								continue
							}
						} else {
							if infoGroup.Kmers > kmerThreshold8 { // !flag && flagx -> flag8
								if flag8 { // keep the same
									batch = append(batch, infos)
									if len(batch) < sBlock { // not filled
										continue
									}
								} else {
									// meet a big file > kmerThreshold8
									sBlock = 8
									flag8 = true // mark
									lastInfos = infos
									// and we have to process files aleady in batch
								}
							} else { //  flagX
								batch = append(batch, infos)
								if len(batch) < sBlock { // not filled
									continue
								}
							}
						}
					} else if skipBlockX {
						if infoGroup.Kmers > kmerThreshold8 {
							if infoGroup.Kmers > kmerThreshold1 {
								// meet a very big file the first time
								flag = true // mark
							} else {
								// meet a big file > kmerThresholdX
								sBlock = blockSizeX
								flagX = true // mark
							}
							lastInfos = infos // leave this file process in the next round
							// and we have to process files aleady in batch
						} else {
							batch = append(batch, infos)
							if len(batch) < sBlock { // not filled
								continue
							}
						}
					} else {
						if infoGroup.Kmers > kmerThresholdX {
							if infoGroup.Kmers > kmerThreshold1 {
								// meet a very big file the first time
								flag = true // mark
							} else if infoGroup.Kmers > kmerThreshold8 {
								// meet a big file > kmerThreshold8
								sBlock = 8
								flag8 = true // mark
							} else {
								// meet a big file > kmerThresholdX
								sBlock = blockSizeX
								flagX = true // mark
							}
							lastInfos = infos // leave this file process in the next round
							// and we have to process files aleady in batch
						} else {
							batch = append(batch, infos)
							if len(batch) < sBlock { // not filled
								continue
							}
						}
					}

				}

				if len(batch) == 0 {
					if lastInfos == nil {
						break
					} else {
						continue
					}
				}

				b++

				if singleRepeat {
					prefix = fmt.Sprintf("%s [block #%03d]", time.Now().Format("15:04:05.000"), b)
				} else {
					prefix = fmt.Sprintf("%s [Repeat %d/%d][block #%03d]", time.Now().Format("15:04:05.000"), rr+1, numRepeats, b)
				}

				wg0.Add(1)
				tokens0 <- 1

				var bar *mpb.Bar
				if opt.Verbose && !dryRun {
					if b > opt.NumCPUs { // update count
						barW.SetTotal(int64(b), false)
					}

					bar = pbs.AddBar(int64((len(batch)+7)/8),
						mpb.PrependDecorators(
							decor.Name(prefix, decor.WC{W: len(prefix) + 1, C: decor.DidentRight}),
							decor.CountersNoUnit("%d / %d", decor.WCSyncWidth),
						),
						mpb.AppendDecorators(decor.Percentage(decor.WC{W: 5})),
						mpb.BarFillerClearOnComplete(),
					)
				}

				go func(batch [][]UnikFileInfo, b int, prefix string, bar *mpb.Bar) {
					startTime := time.Now()

					var wg sync.WaitGroup
					tokens := make(chan int, opt.NumCPUs/maxConc)

					// max elements of UNION of all sets,
					// but it takes time to compute for reading whole data,
					// so we use sum of elements, which is slightly higher than actual size.
					var maxElements uint64
					var totalKmer uint64
					for _, infos := range batch {
						totalKmer = 0
						for _, info := range infos {
							totalKmer += info.Kmers
						}
						if maxElements < totalKmer {
							maxElements = totalKmer
						}
					}

					nInfoGroups := len(batch)
					nBatchFiles := int((nInfoGroups + 7) / 8)

					sigsBlock := make([][]byte, 0, nBatchFiles)

					namesBlock := make([][]string, 0, nInfoGroups)
					gsizesBlock := make([][]uint64, 0, nInfoGroups)
					// kmersBlock := make([][]uint64, 0, nInfoGroups)
					indicesBlock := make([][]uint32, 0, nInfoGroups)
					sizesBlock := make([]uint64, 0, nInfoGroups)

					chBatch8 := make(chan batch8s, nBatchFiles)
					doneBatch8 := make(chan int)

					buf := make(map[int]batch8s)

					go func() {
						var id int = 1
						for batch2 := range chBatch8 {
							if batch2.id == id {
								sigsBlock = append(sigsBlock, batch2.sigs)
								namesBlock = append(namesBlock, batch2.names...)
								gsizesBlock = append(gsizesBlock, batch2.gsizes...)
								// kmersBlock = append(kmersBlock, batch2.kmers...)
								indicesBlock = append(indicesBlock, batch2.indices...)
								sizesBlock = append(sizesBlock, batch2.sizes...)
								if opt.Verbose && !dryRun {
									bar.Increment()
								}
								id++
								continue
							}
							for {
								if _batch, ok := buf[id]; ok {
									sigsBlock = append(sigsBlock, _batch.sigs)
									namesBlock = append(namesBlock, _batch.names...)
									gsizesBlock = append(gsizesBlock, _batch.gsizes...)
									// kmersBlock = append(kmersBlock, _batch.kmers...)
									indicesBlock = append(indicesBlock, _batch.indices...)
									sizesBlock = append(sizesBlock, _batch.sizes...)
									if (opt.Verbose || opt.Log2File) && !dryRun {
										bar.Increment()
									}
									delete(buf, id)
									id++
								} else {
									break
								}
							}
							buf[batch2.id] = batch2
						}
						if len(buf) > 0 {
							ids := make([]int, 0, len(buf))
							for id := range buf {
								ids = append(ids, id)
							}
							sort.Ints(ids)
							for _, id := range ids {
								_batch := buf[id]

								sigsBlock = append(sigsBlock, _batch.sigs)
								namesBlock = append(namesBlock, _batch.names...)
								gsizesBlock = append(gsizesBlock, _batch.gsizes...)
								// kmersBlock = append(kmersBlock, _batch.kmers...)
								indicesBlock = append(indicesBlock, _batch.indices...)
								sizesBlock = append(sizesBlock, _batch.sizes...)
								if (opt.Verbose || opt.Log2File) && !dryRun {
									bar.Increment()
								}
							}
						}

						doneBatch8 <- 1
					}()

					numSigs := CalcSignatureSize(uint64(maxElements), numHashes, fpr)
					if faster {
						numSigs = roundup64(numSigs)
					}

					var eFileSize float64
					eFileSize = 24
					for _, infos := range batch {
						eFileSize += 8 // length of Names (4) and indices (4)
						for _, info := range infos {
							// name + "\n" (1) + indice (4) + size (8)
							eFileSize += float64(len(info.Name) + 13)
						}
					}
					eFileSize += float64(numSigs * uint64(nBatchFiles))

					if (opt.Verbose || opt.Log2File) && dryRun {
						if singleRepeat {
							log.Infof("  %s #files: %d, max #k-mers: %d, #signatures: %d, file size: %8s",
								prefix, len(batch), maxElements, numSigs, bytesize.ByteSize(eFileSize))
						} else {
							log.Infof("  %s #buckets: %d, max #k-mers: %d, #signatures: %d, file size: %8s",
								prefix, len(batch), maxElements, numSigs, bytesize.ByteSize(eFileSize))
						}
					}

					// split into batches with 8 files
					var bb, jj int
					for ii := 0; ii < nInfoGroups; ii += 8 {
						if dryRun {
							continue
						}
						jj = ii + 8
						if jj > nInfoGroups {
							jj = nInfoGroups
						}
						wg.Add(1)
						tokens <- 1
						bb++

						var outFile string

						// 8 files
						go func(_batch [][]UnikFileInfo, bb int, numSigs uint64, outFile string, id int) {
							defer func() {
								wg.Done()
								<-tokens
							}()

							names := make([][]string, 0, 8)
							gsizes := make([][]uint64, 0, 8)
							// kmers := make([][]uint64, 0, 8)
							indices := make([][]uint32, 0, 8)
							sizes := make([]uint64, 0, 8)
							for _, infos := range _batch {
								_names := make([]string, len(infos))
								_gsizes := make([]uint64, len(infos))
								// _kmers := make([]uint64, len(infos))
								_indices := make([]uint32, len(infos))
								var _size uint64

								sorts.Quicksort(UnikFileInfosByName(infos))

								for iii, info := range infos {
									_names[iii] = info.Name
									_gsizes[iii] = info.GenomeSize
									// _kmers[iii] = info.Kmers
									// _indices[iii] = info.Index

									// for the compatibility of .uniki file,
									//  we bit-pack the index of chunks and the number of chunks into one uint32,
									// with the high 16 bits storing the number of chunks and the low 16 bits
									// storing the index of chunks.
									_indices[iii] = info.Index + info.Indexes<<16 // add number of indexes

									_size += info.Kmers
								}
								names = append(names, _names)
								gsizes = append(gsizes, _gsizes)
								// kmers = append(kmers, _kmers)
								indices = append(indices, _indices)
								sizes = append(sizes, uint64(_size))
							}

							sigs := make([]byte, numSigs)

							numSigsM1 := numSigs - 1

							// every file in 8 file groups
							for _k, infos := range _batch {
								for _, info := range infos {
									tokensOpenFiles <- 1

									var infh *bufio.Reader
									var r *os.File
									var reader *unik.Reader
									var err error
									var code uint64
									var loc int

									infh, r, _, err = inStream(info.Path)
									checkError(errors.Wrap(err, info.Path))

									reader, err = unik.NewReader(infh)
									checkError(errors.Wrap(err, info.Path))
									singleHash := numHashes == 1

									if reader.IsHashed() {
										if singleHash {
											if faster {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													// sigs[code%numSigs] |= 1 << (7 - _k)
													sigs[code&numSigsM1] |= 1 << (7 - _k) // &Xis faster than %X when X is power of 2
												}
											} else {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													sigs[code%numSigs] |= 1 << (7 - _k)
													// sigs[code&numSigsM1] |= 1 << (7 - _k) // &Xis faster than %X when X is power of 2
												}
											}
										} else {
											if faster {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													// for _, loc = range hashLocations(code, numHashes, numSigs) {
													for _, loc = range hashLocationsFaster(code, numHashes, numSigsM1) {
														sigs[loc] |= 1 << (7 - _k)
													}
												}
											} else {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													for _, loc = range hashLocations(code, numHashes, numSigs) {
														// for _, loc = range hashLocationsFaster(code, numHashes, numSigsM1) {
														sigs[loc] |= 1 << (7 - _k)
													}
												}
											}
										}
									} else {
										if singleHash {
											if faster {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													// sigs[hash64(code)%numSigs] |= 1 << (7 - _k)
													sigs[hash64(code)&numSigsM1] |= 1 << (7 - _k) // &Xis faster than %X when X is power of 2
												}
											} else {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													sigs[hash64(code)%numSigs] |= 1 << (7 - _k)
													// sigs[hash64(code)&numSigsM1] |= 1 << (7 - _k) // &Xis faster than %X when X is power of 2
												}
											}
										} else {
											if faster {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													// for _, loc = range hashLocations(code, numHashes, numSigs) {
													for _, loc = range hashLocationsFaster(hash64(code), numHashes, numSigsM1) {
														sigs[loc] |= 1 << (7 - _k)
													}
												}
											} else {
												for {
													code, _, err = reader.ReadCodeWithTaxid()
													if err != nil {
														if err == io.EOF {
															break
														}
														checkError(errors.Wrap(err, info.Path))
													}

													for _, loc = range hashLocations(code, numHashes, numSigs) {
														// for _, loc = range hashLocationsFaster(hash64(code), numHashes, numSigsM1) {
														sigs[loc] |= 1 << (7 - _k)
													}
												}
											}
										}
									}

									r.Close()

									<-tokensOpenFiles
								}
							}

							chBatch8 <- batch8s{
								id:     id,
								sigs:   sigs,
								names:  names,
								gsizes: gsizes,
								// kmers:   kmers,
								indices: indices,
								sizes:   sizes,
							}
						}(batch[ii:jj], bb, numSigs, outFile, bb)
					}

					wg.Wait()
					close(chBatch8)
					<-doneBatch8

					blockFile := filepath.Join(outDir,
						dirR,
						fmt.Sprintf("_block%03d%s", b, extIndex))

					if !dryRun {
						// save to index file

						// tokensWriteFiles <- 1

						outfh, gw, w, err := outStream(blockFile, false, opt.CompressionLevel)
						checkError(err)

						// writer, err := index.NewWriter(outfh, k, canonical, !faster, uint8(numHashes), numSigs, namesBlock, gsizesBlock, kmersBlock, indicesBlock, sizesBlock)
						writer, err := index.NewWriter(outfh, k, canonical, !faster, uint8(numHashes), numSigs, namesBlock, gsizesBlock, indicesBlock, sizesBlock)
						checkError(err)

						if nBatchFiles == 1 {
							checkError(writer.WriteBatch(sigsBlock[0], len(sigsBlock[0])))
						} else {
							row := make([]byte, nBatchFiles)
							for ii := 0; ii < int(numSigs); ii++ {
								for jj = 0; jj < nBatchFiles; jj++ {
									row[jj] = sigsBlock[jj][ii]
								}
								checkError(writer.Write(row))
							}
						}

						checkError(writer.Flush())
						outfh.Flush()
						if gw != nil {
							gw.Close()
						}
						w.Close()

						// <-tokensWriteFiles
					}

					ch <- filepath.Base(blockFile)
					chFileSize <- eFileSize

					if opt.Verbose && !dryRun {
						// os.Stdout.WriteString(fmt.Sprintf("%s, %s\n", time.Since(startTime), time.Duration(float64(time.Since(startTime))/float64(maxConc))))
						chDurationW <- time.Duration(float64(time.Since(startTime)) / float64(maxConc))
					}

					wg0.Done()
					<-tokens0
				}(batch, b, prefix, bar)

				batch = make([][]UnikFileInfo, 0, sBlock)
			}

			wg0.Wait()

			close(ch)
			close(chFileSize)
			<-done
			<-doneFileSize

			if opt.Verbose && !dryRun {
				barW.SetTotal(int64(b), true)
				close(chDurationW)
				<-doneDurationW
				pbs.Wait()
			}

			totalIndexFiles += len(indexFiles)

			sortutil.Strings(indexFiles)
			dbInfo := NewUnikIndexDBInfo(indexFiles)
			dbInfo.Alias = alias
			dbInfo.Ks = ks
			dbInfo.Hashed = hashed
			dbInfo.Kmers = n
			dbInfo.FPR = fpr
			dbInfo.BlockSize = sBlock0
			dbInfo.NumNames = len(fileInfoGroups)
			dbInfo.CompactSize = !faster
			dbInfo.NumHashes = numHashes
			dbInfo.Canonical = canonical
			dbInfo.Scaled = scaled
			dbInfo.Scale = scale
			dbInfo.Minimizer = meta0.Minimizer
			dbInfo.MinimizerW = uint32(meta0.MinimizerW)
			dbInfo.Syncmer = meta0.Syncmer
			dbInfo.SyncmerS = uint32(meta0.SyncmerS)
			dbInfo.SplitSeq = meta0.SplitSeq
			dbInfo.SplitSize = meta0.SplitSize
			dbInfo.SplitNum = meta0.SplitNum
			dbInfo.SplitOverlap = meta0.SplitOverlap

			if !dryRun {
				var n2 int
				n2, err = dbInfo.WriteTo(filepath.Join(outDir, dirR, dbInfoFile))
				checkError(err)
				fileSize += float64(n2)

				// write name_mapping.tsv
				func() {
					outfh, gw, w, err := outStream(filepath.Join(outDir, dirR, dbNameMappingFile), false, opt.CompressionLevel)
					checkError(err)
					defer func() {
						outfh.Flush()
						if gw != nil {
							gw.Close()
						}
						w.Close()
					}()

					var line string
					for name := range namesMap0 {
						line = fmt.Sprintf("%s\t%s\n", name, name)
						fileSize += float64(len(line))
						outfh.WriteString(line)
					}
				}()
			} else { // compute file size of __db.yaml and __name_mapping.tsv
				// __db.yaml
				data, err := yaml.Marshal(dbInfo)
				if err != nil {
					checkError(fmt.Errorf("fail to marshal database info"))
				}
				fileSize += float64(len(data))

				// __name_mapping.tsv
				var line string
				for name := range namesMap0 {
					line = fmt.Sprintf("%s\t%s\n", name, name)
					fileSize += float64(len(line))
				}
			}

			// ------------------------------------------------------------------------------------
			fileSize0 += fileSize
		}

		if opt.Verbose || opt.Log2File {
			log.Info()
			log.Infof("kmcp database with %d k-mers saved to %s", n, outDir)
			log.Infof("total file size: %s", bytesize.ByteSize(fileSize0))
			log.Infof("total index files: %d", totalIndexFiles)
		}
	},
}

func init() {
	RootCmd.AddCommand(indexCmd)

	indexCmd.Flags().StringP("in-dir", "I", "",
		formatFlagUsage(`Directory containing .unik files. Directory symlinks are followed.`))

	indexCmd.Flags().StringP("file-regexp", "", ".unik$",
		formatFlagUsage(`Regular expression for matching files in -I/--in-dir, case ignored.`))

	indexCmd.Flags().StringP("out-dir", "O", "",
		formatFlagUsage(`Output directory. (default: ${indir}.kmcp-db)`))

	indexCmd.Flags().StringP("alias", "a", "",
		formatFlagUsage(`Database alias/name. (default: basename of --out-dir). You can also manually edit it in info file: ${outdir}/__db.yml.`))

	indexCmd.Flags().Float64P("false-positive-rate", "f", 0.3,
		formatFlagUsage(`False positive rate of the bloom filters, range: (0, 1).`))

	indexCmd.Flags().IntP("num-hash", "n", 1,
		formatFlagUsage(`Number of hash functions in bloom filters.`))

	indexCmd.Flags().IntP("block-size", "b", 0,
		formatFlagUsage(`Block size, better be multiple of 64 for large number of input files. (default: min(#.files/#theads, 8))`))

	indexCmd.Flags().StringP("block-sizeX-kmers-t", "x", "10M",
		formatFlagUsage(`If k-mers of single .unik file exceeds this threshold, block size is changed to --block-sizeX. Supported units: K, M, G.`))

	indexCmd.Flags().IntP("block-sizeX", "X", 256,
		formatFlagUsage(`If k-mers of single .unik file exceeds --block-sizeX-kmers-t, block size is changed to this value.`))

	indexCmd.Flags().StringP("block-size8-kmers-t", "8", "20M",
		formatFlagUsage(`If k-mers of single .unik file exceeds this threshold, block size is changed to 8. Supported units: K, M, G.`))

	indexCmd.Flags().StringP("block-size1-kmers-t", "1", "200M",
		formatFlagUsage(`If k-mers of single .unik file exceeds this threshold, an individual index is created for this file. Supported units: K, M, G.`))

	// smaller big sizes is more important and brings extra speedup.
	// indexCmd.Flags().BoolP("faster", "", false, `roundup size of index files to increase searching speed at the cost of bigger database and high memory occupation`)

	// indexCmd.Flags().IntP("num-repititions", "R", 1, `[RAMBO] number of repititions`)
	// indexCmd.Flags().IntP("num-buckets", "B", 0, `[RAMBO] number of buckets per repitition, 0 for one set per bucket`)
	// indexCmd.Flags().IntP("seed", "", 1, `[RAMBO] seed for randomly assigning names to buckets`)

	indexCmd.Flags().BoolP("force", "", false,
		formatFlagUsage(`Overwrite existed output directory.`))

	indexCmd.Flags().IntP("max-open-files", "F", 256,
		formatFlagUsage(`Maximum number of opened files, please use a small value for hard disk drive storage.`))

	// indexCmd.Flags().IntP("max-write-files", "W", 4, `maximum number of writing files, please use a small value for hard disk drive storage`)

	indexCmd.Flags().BoolP("dry-run", "", false,
		formatFlagUsage(`Dry run, useful for adjusting parameters (highly recommended).`))

	indexCmd.SetUsageTemplate(usageTemplate("[-f <fpr>] [-n <hashes>] [-j <blocks>] -I <compute output> -O <kmcp db>"))

}

// batch8 contains data from 8 files, just for keeping order of all files of a block
type batch8s struct {
	id int

	sigs   []byte
	names  [][]string
	gsizes [][]uint64
	// kmers   [][]uint64
	indices [][]uint32
	sizes   []uint64
}

var sepNameIdx = "-id"

// compute exact max elements by reading all file. not used.
func maxElements(opt Options, tokensOpenFiles chan int, batch [][]UnikFileInfo) (maxElements int64) {
	var _wg sync.WaitGroup
	_tokens := make(chan int, opt.NumCPUs)
	_ch := make(chan int64, len(batch))
	_done := make(chan int)
	go func() {
		var totalKmer int64
		for totalKmer = range _ch {
			if maxElements < totalKmer {
				maxElements = totalKmer
			}
		}
		_done <- 1
	}()
	for _, infos := range batch {
		_wg.Add(1)
		_tokens <- 1
		go func(infos []UnikFileInfo) {
			defer func() {
				_wg.Done()
				<-_tokens
			}()

			_m := make(map[uint64]struct{}, mapInitSize)

			for _, _info := range infos {
				tokensOpenFiles <- 1

				infh, r, _, err := inStream(_info.Path)
				if err != nil {
					checkError(err)
				}
				defer r.Close()

				reader, err := unik.NewReader(infh)
				if err != nil {
					checkError(err)
				}

				var code uint64
				for {
					code, _, err = reader.ReadCodeWithTaxid()
					if err != nil {
						if err == io.EOF {
							break
						}
						checkError(errors.Wrap(err, _info.Path))
					}
					_m[code] = struct{}{}
				}

				<-tokensOpenFiles
			}

			_ch <- int64(len(_m))
		}(infos)
	}
	_wg.Wait()
	close(_ch)
	<-_done

	return
}
