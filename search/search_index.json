{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping Citation KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. Bioinformatics, btac845, https://doi.org/10.1093/bioinformatics/btac845 Table of contents Documents What can we do? 1. Accurate metagenomic profiling 2. Fast sequence search against large scales of genomic datasets 3. Fast genome similarity estimation Features Installation Commands Quickstart KMCP vs COBS Support License Acknowledgments Documents https://bioinf.shenwei.me/kmcp Installation Databases Tutorials Taxonomic profiling Sequence and genome searching Usage Benchmarks FAQs What can we do? 1. Accurate metagenomic profiling KMCP utilizes genome coverage information by splitting the reference genomes into chunks and stores k-mers in a modified and optimized COBS index for fast alignment-free sequence searching. KMCP combines k-mer similarity and genome coverage information to reduce the false positive rate of k-mer-based taxonomic classification and profiling methods. The read mapping process in KMCP is referred to as pseudo-mapping , which is similar to but different from the lightweight algorithm in Sailfish (Patro et al., 2014), pseudoalignment in Kallisto (Bray et al., 2016), quasi-mapping in RapMap (Srivastava et al., 2016), and lightweight mapping in Salmon (Patro et al., 2017). All of these methods seek to elide the computation of base-to-base alignment using distinct strategies (Srivastava et al., 2016). In KMCP, each reference genome is pre-split into chunks of equal size, and the k-mers of a query, as a whole, are compared to each genome chunk to find all possible ones sharing a predefined proportion of k-mers with the query . Like quasi-mapping in RapMap, KMCP tracks the target and position for each query. However, the read position in KMCP is approximate and in a predefined resolution (the number of genome chunks). Benchmarking results based on simulated and real data demonstrate that KMCP, despite a longer running time than some other methods, not only allows the accurate taxonomic profiling of prokaryotic and viral populations but also provides more confident pathogen detection in clinical samples of low depth . Genome collections with custom taxonomy , e.g., GTDB uses its own taxonomy and MGV uses ICTV taxonomy , are also supported by generating NCBI-style taxdump files with taxonkit create-taxdump . You can even merge the GTDB taxonomy (for prokaryotic genomes from GTDB) and NCBI taxonomy (for genomes from NCBI) . 2. Fast sequence search against large scales of genomic datasets KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. We reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a smaller index size and much faster searching speed (2x for genome search and 10x for short reads) faster than COBS (check the tutorial and benchmark ). Also check the algorithm and data structure differences between KMCP and COBS . 3. Fast genome similarity estimation KMCP can also be used for fast similarity estimation of assemblies/genomes against known reference genomes. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Sourmash). KMCP supports multiple k-mer sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ), and Closed Syncmers ) for genome similarity estimation. And KMCP is 5x-7x faster than Mash/Sourmash (check the tutorial and benchmark ). Features Easy to install Statically linked executable binaries for multiple platforms (Linux/Windows/macOS, AMD64/ARM64). No dependencies, no configurations. conda install -c bioconda kmcp Easy to use Supporting shell autocompletion . Detailed usage , database , tutorials , and FAQs . Building database is easy and fast ~25 min for 47894 genomes from GTDB-r202 on a sever with 40 CPU threads and solid disk drive. Fast searching speed (for sequence/genome search) The index structure is modified from COBS, while KMCP is 2x-10x faster in sequence searching. Automatically scales to exploit all available CPU cores. Searching time is linearly related to the number of reference genomes (chunks). Scalable searching . Searching results against multiple databases can be fast merged . This brings many benefits: There's no need to re-built the database with newly added reference genomes . The searching step can be parallelized with a computer cluster in which each computation node searches against a small database. Computers with limited main memory can also utilize an extensive collection of reference genomes by building and searching against small databases.. Accurate taxonomic profiling Some k-mer based taxonomic profilers suffer from high false positive rates, while KMCP adopts multiple strategies to improve specificity and keeps high sensitivity at the same time . In addition to archaea and bacteria, KMCP performed well on viruses/phages . KMCP also provides confident infectious pathogen detection . Preset six modes for multiple scenarios . Supports CAMI and MetaPhlAn profiling format . Flexible support of taxonomy database Taxonomy data, in the format of NCBI taxdump files, are only needed in the profiling step. Therefore, it is easy to utilize an updated version of taxonomy data. GTDB, ICTV and custom taxonomy database are supported by creating taxdump files with taxonkit create-taxdump . You can even merge the GTDB taxonomy (for prokaryotic genomes from GTDB) and NCBI taxonomy (for genomes from NCBI) . Profiling without taxonomy data is also supported by setting --level strain in kmcp profile . Installation Download executable binaries , or install using conda: conda install -c bioconda kmcp SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters. ARM architecture is supported, but kmcp search would be slower. Commands Subcommand Function compute Generate k-mers (sketch) from FASTA/Q sequences index Construct a database from k-mer files search Search sequences against a database merge Merge search results from multiple databases profile Generate the taxonomic profile from search results utils split-genomes Split genomes into chunks utils unik-info Print information of .unik files utils index-info Print information of index files utils index-density Plot the element density of bloom filters for an index file utils ref-info Print information of reference chunks in a database utils cov2simi Convert k-mer coverage to sequence similarity utils query-fpr Compute the false positive rate of a query utils filter Filter search results and find species/assembly-specific queries utils merge-regions Merge species/assembly-specific regions Quickstart # compute k-mers kmcp compute -k 21 --split-number 10 --split-overlap 150 \\ --in-dir genomes/ --out-dir genomes-k21-n10 # index k-mers kmcp index --false-positive-rate 0.1 --num-hash 1 \\ --in-dir genomes-k21-n10/ --out-dir genomes.kmcp # delete temporary files # rm -rf genomes-k21-n10/ # search kmcp search --db-dir genomes.kmcp/ test.fa.gz --out-file search.kmcp@db1.kmcp.tsv.gz # merge search results against multiple databases kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz # profile and binning kmcp profile search.kmcp.tsv.gz \\ --taxid-map taxid.map \\ --taxdump taxdump/ \\ --out-file search.tsv.gz.k.profile \\ --metaphlan-report search.tsv.gz.m.profile \\ --cami-report search.tsv.gz.c.profile \\ --binning-result search.tsv.gz.binning.gz Next: Demo of taxonomic profiling Tutorial of taxonomic profiling KMCP vs COBS We reimplemented and modified the Compact Bit-Sliced Signature index ( COBS ) algorithm, bringing a smaller index size and much faster searching speed (2x for genome search and 10x for short reads) faster than COBS . The differences between KMCP and COBS Category Item COBS KMCP Comment Algorithm K-mer hashing xxhash ntHash1 xxHash is a general-purpose hashing function while ntHash is a recursive hash function for DNA/RNA Bloom filter hashing xxhash Using k-mer hash values Avoid hash computation Multiple-hash functions xxhash with different seeds Generating multiple values from a single one Avoid hash computation Single-hash function Same as multiple-hash functions Separated workflow Reducing loops AND step Serial bitwise AND Vectorised bitwise AND Bitwise AND for >1 hash functions PLUS step Serial bit-unpacking Vectorised positional popcount with pospop Counting from bit-packed data Index structure Size of blocks / Using extra thresholds to split the last block with the most k-mers Uneven genome size distribution would make bloom filters of the last block extremely huge Index files Concatenated Independent Index loading mmap, loading complete index into RAM mmap, loading complete index into RAM, seek Index loading modes Input/output Input files FASTA/Q, McCortex, text FASTA/Q Output Target and matched k-mers Target, matched k-mers, query FPR, etc. Support Please open an issue to report bugs, propose new functions, or ask for help. License MIT License Acknowledgments Zhi-Luo Deng (Helmholtz Centre for Infection Research, Germany) gave a lot of valuable advice on metagenomic profiling and benchmarking. Robert Clausecker (Zuse Institute Berlin, Germany) wrote the high-performance vectorized positional popcount package ( pospop ) during my development of KMCP , which greatly accelerated the bit-matrix searching.","title":"Home"},{"location":"#kmcp-accurate-metagenomic-profiling-of-both-prokaryotic-and-viral-populations-by-pseudo-mapping","text":"","title":"KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping"},{"location":"#citation","text":"KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. Bioinformatics, btac845, https://doi.org/10.1093/bioinformatics/btac845","title":"Citation"},{"location":"#table-of-contents","text":"Documents What can we do? 1. Accurate metagenomic profiling 2. Fast sequence search against large scales of genomic datasets 3. Fast genome similarity estimation Features Installation Commands Quickstart KMCP vs COBS Support License Acknowledgments","title":"Table of contents"},{"location":"#documents","text":"https://bioinf.shenwei.me/kmcp Installation Databases Tutorials Taxonomic profiling Sequence and genome searching Usage Benchmarks FAQs","title":"Documents"},{"location":"#what-can-we-do","text":"","title":"What can we do?"},{"location":"#1-accurate-metagenomic-profiling","text":"KMCP utilizes genome coverage information by splitting the reference genomes into chunks and stores k-mers in a modified and optimized COBS index for fast alignment-free sequence searching. KMCP combines k-mer similarity and genome coverage information to reduce the false positive rate of k-mer-based taxonomic classification and profiling methods. The read mapping process in KMCP is referred to as pseudo-mapping , which is similar to but different from the lightweight algorithm in Sailfish (Patro et al., 2014), pseudoalignment in Kallisto (Bray et al., 2016), quasi-mapping in RapMap (Srivastava et al., 2016), and lightweight mapping in Salmon (Patro et al., 2017). All of these methods seek to elide the computation of base-to-base alignment using distinct strategies (Srivastava et al., 2016). In KMCP, each reference genome is pre-split into chunks of equal size, and the k-mers of a query, as a whole, are compared to each genome chunk to find all possible ones sharing a predefined proportion of k-mers with the query . Like quasi-mapping in RapMap, KMCP tracks the target and position for each query. However, the read position in KMCP is approximate and in a predefined resolution (the number of genome chunks). Benchmarking results based on simulated and real data demonstrate that KMCP, despite a longer running time than some other methods, not only allows the accurate taxonomic profiling of prokaryotic and viral populations but also provides more confident pathogen detection in clinical samples of low depth . Genome collections with custom taxonomy , e.g., GTDB uses its own taxonomy and MGV uses ICTV taxonomy , are also supported by generating NCBI-style taxdump files with taxonkit create-taxdump . You can even merge the GTDB taxonomy (for prokaryotic genomes from GTDB) and NCBI taxonomy (for genomes from NCBI) .","title":"1. Accurate metagenomic profiling"},{"location":"#2-fast-sequence-search-against-large-scales-of-genomic-datasets","text":"KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. We reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a smaller index size and much faster searching speed (2x for genome search and 10x for short reads) faster than COBS (check the tutorial and benchmark ). Also check the algorithm and data structure differences between KMCP and COBS .","title":"2. Fast sequence search against large scales of genomic datasets"},{"location":"#3-fast-genome-similarity-estimation","text":"KMCP can also be used for fast similarity estimation of assemblies/genomes against known reference genomes. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Sourmash). KMCP supports multiple k-mer sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ), and Closed Syncmers ) for genome similarity estimation. And KMCP is 5x-7x faster than Mash/Sourmash (check the tutorial and benchmark ).","title":"3. Fast genome similarity estimation"},{"location":"#features","text":"Easy to install Statically linked executable binaries for multiple platforms (Linux/Windows/macOS, AMD64/ARM64). No dependencies, no configurations. conda install -c bioconda kmcp Easy to use Supporting shell autocompletion . Detailed usage , database , tutorials , and FAQs . Building database is easy and fast ~25 min for 47894 genomes from GTDB-r202 on a sever with 40 CPU threads and solid disk drive. Fast searching speed (for sequence/genome search) The index structure is modified from COBS, while KMCP is 2x-10x faster in sequence searching. Automatically scales to exploit all available CPU cores. Searching time is linearly related to the number of reference genomes (chunks). Scalable searching . Searching results against multiple databases can be fast merged . This brings many benefits: There's no need to re-built the database with newly added reference genomes . The searching step can be parallelized with a computer cluster in which each computation node searches against a small database. Computers with limited main memory can also utilize an extensive collection of reference genomes by building and searching against small databases.. Accurate taxonomic profiling Some k-mer based taxonomic profilers suffer from high false positive rates, while KMCP adopts multiple strategies to improve specificity and keeps high sensitivity at the same time . In addition to archaea and bacteria, KMCP performed well on viruses/phages . KMCP also provides confident infectious pathogen detection . Preset six modes for multiple scenarios . Supports CAMI and MetaPhlAn profiling format . Flexible support of taxonomy database Taxonomy data, in the format of NCBI taxdump files, are only needed in the profiling step. Therefore, it is easy to utilize an updated version of taxonomy data. GTDB, ICTV and custom taxonomy database are supported by creating taxdump files with taxonkit create-taxdump . You can even merge the GTDB taxonomy (for prokaryotic genomes from GTDB) and NCBI taxonomy (for genomes from NCBI) . Profiling without taxonomy data is also supported by setting --level strain in kmcp profile .","title":"Features"},{"location":"#installation","text":"Download executable binaries , or install using conda: conda install -c bioconda kmcp SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters. ARM architecture is supported, but kmcp search would be slower.","title":"Installation"},{"location":"#commands","text":"Subcommand Function compute Generate k-mers (sketch) from FASTA/Q sequences index Construct a database from k-mer files search Search sequences against a database merge Merge search results from multiple databases profile Generate the taxonomic profile from search results utils split-genomes Split genomes into chunks utils unik-info Print information of .unik files utils index-info Print information of index files utils index-density Plot the element density of bloom filters for an index file utils ref-info Print information of reference chunks in a database utils cov2simi Convert k-mer coverage to sequence similarity utils query-fpr Compute the false positive rate of a query utils filter Filter search results and find species/assembly-specific queries utils merge-regions Merge species/assembly-specific regions","title":"Commands"},{"location":"#quickstart","text":"# compute k-mers kmcp compute -k 21 --split-number 10 --split-overlap 150 \\ --in-dir genomes/ --out-dir genomes-k21-n10 # index k-mers kmcp index --false-positive-rate 0.1 --num-hash 1 \\ --in-dir genomes-k21-n10/ --out-dir genomes.kmcp # delete temporary files # rm -rf genomes-k21-n10/ # search kmcp search --db-dir genomes.kmcp/ test.fa.gz --out-file search.kmcp@db1.kmcp.tsv.gz # merge search results against multiple databases kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz # profile and binning kmcp profile search.kmcp.tsv.gz \\ --taxid-map taxid.map \\ --taxdump taxdump/ \\ --out-file search.tsv.gz.k.profile \\ --metaphlan-report search.tsv.gz.m.profile \\ --cami-report search.tsv.gz.c.profile \\ --binning-result search.tsv.gz.binning.gz Next: Demo of taxonomic profiling Tutorial of taxonomic profiling","title":"Quickstart"},{"location":"#kmcp-vs-cobs","text":"We reimplemented and modified the Compact Bit-Sliced Signature index ( COBS ) algorithm, bringing a smaller index size and much faster searching speed (2x for genome search and 10x for short reads) faster than COBS . The differences between KMCP and COBS Category Item COBS KMCP Comment Algorithm K-mer hashing xxhash ntHash1 xxHash is a general-purpose hashing function while ntHash is a recursive hash function for DNA/RNA Bloom filter hashing xxhash Using k-mer hash values Avoid hash computation Multiple-hash functions xxhash with different seeds Generating multiple values from a single one Avoid hash computation Single-hash function Same as multiple-hash functions Separated workflow Reducing loops AND step Serial bitwise AND Vectorised bitwise AND Bitwise AND for >1 hash functions PLUS step Serial bit-unpacking Vectorised positional popcount with pospop Counting from bit-packed data Index structure Size of blocks / Using extra thresholds to split the last block with the most k-mers Uneven genome size distribution would make bloom filters of the last block extremely huge Index files Concatenated Independent Index loading mmap, loading complete index into RAM mmap, loading complete index into RAM, seek Index loading modes Input/output Input files FASTA/Q, McCortex, text FASTA/Q Output Target and matched k-mers Target, matched k-mers, query FPR, etc.","title":"KMCP vs COBS"},{"location":"#support","text":"Please open an issue to report bugs, propose new functions, or ask for help.","title":"Support"},{"location":"#license","text":"MIT License","title":"License"},{"location":"#acknowledgments","text":"Zhi-Luo Deng (Helmholtz Centre for Infection Research, Germany) gave a lot of valuable advice on metagenomic profiling and benchmarking. Robert Clausecker (Zuse Institute Berlin, Germany) wrote the high-performance vectorized positional popcount package ( pospop ) during my development of KMCP , which greatly accelerated the bit-matrix searching.","title":"Acknowledgments"},{"location":"PREBUILT_DB_README/","text":"KMCP databases v2023.05 Source code: https://github.com/shenwei356/kmcp Documents : https://bioinf.shenwei.me/kmcp/database/ Data source Access date: 2023-05-03 DB source #NCBI-species #assemblies db-parameters size Bacteria and Archaea GTDB r214 34395+ 85205 k=21, chunks=10; fpr=0.3, hashes=1 50+49 GB Fungi Refseq r217 491 496 k=21, chunks=10; fpr=0.3, hashes=1 5.5 GB Viruses GenBank r255 26680 33479 k=21, chunks=5; fpr=0.05, hashes=1 5.9 GB Taxdump files: NCBI file: taxdump.tar.gz version: taxdmp_2023-05-01 https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2023-05-01.zip GTDB+NCBI file: taxdump.gtdb+ncbi.tar.gz version: GTDB r214 + NCBI taxdmp_2023-05-01 Files Please download files according to your purposes: For metagenomic profiling: metagenomic-profiling For genome similarity estimation: genome-search For developers: genomes File tree: v2023.05/ \u251c\u2500\u2500 genomes Genomes used to build the databases \u2502 \u251c\u2500\u2500 genbank-viral.tar \u2502 \u251c\u2500\u2500 genbank-viral.tar.md5.txt \u2502 \u251c\u2500\u2500 genbank-viral.taxid.map.stats.tsv Complete lineages (NCBI Taxonomy) of all TaxIds and the number of genomes \u2502 \u251c\u2500\u2500 gtdb.taxid.map.stats.tsv \u2502 \u251c\u2500\u2500 gtdb.txt \u2502 \u251c\u2500\u2500 refseq-fungi.tar \u2502 \u251c\u2500\u2500 refseq-fungi.tar.md5.txt \u2502 \u2514\u2500\u2500 refseq-fungi.taxid.map.stats.tsv \u251c\u2500\u2500 genome-search KMCP databases for genome similarity estimation \u2502 \u251c\u2500\u2500 genbank-viral.minhash.kmcp.tar.gz \u2502 \u251c\u2500\u2500 genbank-viral.minhash.kmcp.tar.gz.md5.txt \u2502 \u251c\u2500\u2500 gtdb.minhash.kmcp.tar.gz \u2502 \u251c\u2500\u2500 gtdb.minhash.kmcp.tar.gz.md5.txt \u2502 \u251c\u2500\u2500 name.map Mapping assembly accessions to genome names (may be the name of the larget contig) \u2502 \u251c\u2500\u2500 refseq-fungi.minhash.kmcp.tar.gz \u2502 \u2514\u2500\u2500 refseq-fungi.minhash.kmcp.tar.gz.md5.txt \u2514\u2500\u2500 metagenomic-profiling KMCP databases for metagenomic profiling \u251c\u2500\u2500 genbank-viral.kmcp.tar.gz \u251c\u2500\u2500 genbank-viral.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 gtdb.part_1.kmcp.tar.gz GTDB representative genomes are split into 2 parts before building the database \u251c\u2500\u2500 gtdb.part_1.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 gtdb.part_2.kmcp.tar.gz \u251c\u2500\u2500 gtdb.part_2.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 refseq-fungi.kmcp.tar.gz \u251c\u2500\u2500 refseq-fungi.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 taxdump.gtdb+ncbi.tar.gz Taxdump files (Combining GTDB and NCBI Taxonomy) \u251c\u2500\u2500 taxdump.gtdb+ncbi.tar.gz.md5.txt \u251c\u2500\u2500 taxdump.tar.gz Taxdump files (NCBI Taxonomy, 2023-05-01) \u251c\u2500\u2500 taxdump.tar.gz.md5.txt \u251c\u2500\u2500 taxid.gtdb+ncbi.map Mapping assembly accessions to TaxId (Combining GTDB and NCBI Taxonomy) \u2514\u2500\u2500 taxid.map Mapping assembly accessions to TaxId (NCBI Taxonomy)","title":"KMCP databases v2023.05"},{"location":"PREBUILT_DB_README/#kmcp-databases-v202305","text":"Source code: https://github.com/shenwei356/kmcp Documents : https://bioinf.shenwei.me/kmcp/database/","title":"KMCP databases v2023.05"},{"location":"PREBUILT_DB_README/#data-source","text":"Access date: 2023-05-03 DB source #NCBI-species #assemblies db-parameters size Bacteria and Archaea GTDB r214 34395+ 85205 k=21, chunks=10; fpr=0.3, hashes=1 50+49 GB Fungi Refseq r217 491 496 k=21, chunks=10; fpr=0.3, hashes=1 5.5 GB Viruses GenBank r255 26680 33479 k=21, chunks=5; fpr=0.05, hashes=1 5.9 GB Taxdump files: NCBI file: taxdump.tar.gz version: taxdmp_2023-05-01 https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2023-05-01.zip GTDB+NCBI file: taxdump.gtdb+ncbi.tar.gz version: GTDB r214 + NCBI taxdmp_2023-05-01","title":"Data source"},{"location":"PREBUILT_DB_README/#files","text":"Please download files according to your purposes: For metagenomic profiling: metagenomic-profiling For genome similarity estimation: genome-search For developers: genomes File tree: v2023.05/ \u251c\u2500\u2500 genomes Genomes used to build the databases \u2502 \u251c\u2500\u2500 genbank-viral.tar \u2502 \u251c\u2500\u2500 genbank-viral.tar.md5.txt \u2502 \u251c\u2500\u2500 genbank-viral.taxid.map.stats.tsv Complete lineages (NCBI Taxonomy) of all TaxIds and the number of genomes \u2502 \u251c\u2500\u2500 gtdb.taxid.map.stats.tsv \u2502 \u251c\u2500\u2500 gtdb.txt \u2502 \u251c\u2500\u2500 refseq-fungi.tar \u2502 \u251c\u2500\u2500 refseq-fungi.tar.md5.txt \u2502 \u2514\u2500\u2500 refseq-fungi.taxid.map.stats.tsv \u251c\u2500\u2500 genome-search KMCP databases for genome similarity estimation \u2502 \u251c\u2500\u2500 genbank-viral.minhash.kmcp.tar.gz \u2502 \u251c\u2500\u2500 genbank-viral.minhash.kmcp.tar.gz.md5.txt \u2502 \u251c\u2500\u2500 gtdb.minhash.kmcp.tar.gz \u2502 \u251c\u2500\u2500 gtdb.minhash.kmcp.tar.gz.md5.txt \u2502 \u251c\u2500\u2500 name.map Mapping assembly accessions to genome names (may be the name of the larget contig) \u2502 \u251c\u2500\u2500 refseq-fungi.minhash.kmcp.tar.gz \u2502 \u2514\u2500\u2500 refseq-fungi.minhash.kmcp.tar.gz.md5.txt \u2514\u2500\u2500 metagenomic-profiling KMCP databases for metagenomic profiling \u251c\u2500\u2500 genbank-viral.kmcp.tar.gz \u251c\u2500\u2500 genbank-viral.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 gtdb.part_1.kmcp.tar.gz GTDB representative genomes are split into 2 parts before building the database \u251c\u2500\u2500 gtdb.part_1.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 gtdb.part_2.kmcp.tar.gz \u251c\u2500\u2500 gtdb.part_2.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 refseq-fungi.kmcp.tar.gz \u251c\u2500\u2500 refseq-fungi.kmcp.tar.gz.md5.txt \u251c\u2500\u2500 taxdump.gtdb+ncbi.tar.gz Taxdump files (Combining GTDB and NCBI Taxonomy) \u251c\u2500\u2500 taxdump.gtdb+ncbi.tar.gz.md5.txt \u251c\u2500\u2500 taxdump.tar.gz Taxdump files (NCBI Taxonomy, 2023-05-01) \u251c\u2500\u2500 taxdump.tar.gz.md5.txt \u251c\u2500\u2500 taxid.gtdb+ncbi.map Mapping assembly accessions to TaxId (Combining GTDB and NCBI Taxonomy) \u2514\u2500\u2500 taxid.map Mapping assembly accessions to TaxId (NCBI Taxonomy)","title":"Files"},{"location":"database-time-and-mem-v2021.12/","text":"Building KMCP database The steps below is same to theses in database.md , but memusg is used to GTDB input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # # elapsed time: 10m:34s # peak rss: 3.87 GB # file size: 978.37 GB # memusg -t -s \"kmcp compute -I $input -O gtdb-r202-k21-n10 -k 21 -n 10 -l 150 -B plasmid \\ --log gtdb-r202-k21-n10.log -j 32 --force\" > gtdb.kmcp.s1.log2 2>&1 # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 # # elapsed time: 11m:48s # peak rss: 13.95 GB # file size: 58.03 GB # memusg -t -s \"kmcp index -j 32 -I gtdb-r202-k21-n10 -O gtdb.kmcp -n 1 -f 0.3 \\ --log gtdb.kmcp.log --force\" > gtdb.kmcp.s2.log2 2>&1 # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.kmcp/ RefSeq fungi name=fungi input=files.renamed # elapsed time: 1m:02s # peak rss: 11.72 GB # file size: 70.52 GB # memusg -t -s \"kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log -j 32 --force\" > refseq-fungi.kmcp.s1.log2 2>&1 # elapsed time: 52.204s # peak rss: 1.19 GB # file size: 4.18 GB memusg -t -s \"kmcp index -I refseq-$name-k21-n10/ -O refseq-fungi.kmcp \\ -j 32 -f 0.3 -n 1 \\ --log refseq-fungi.kmcp.log --force\" > refseq-fungi.kmcp.s2.log2 2>&1 # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.kmcp/ Genbank viral name=viral input=files.renamed.slim # elapsed time: 21.051s # peak rss: 3.53 GB # file size: 9.16 GB # memusg -t -s \"kmcp compute -I $input -O genbank-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log genbank-$name-k21-n10.log -j 32 --force\" > genbank-viral.kmcp.s1.log2 2>&1 # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 # # elapsed time: 31.828s # peak rss: 3.36 GB # file size: 4.72 GB # memusg -t -s \"kmcp index -I genbank-$name-k21-n10/ -O genbank-viral.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log genbank-viral.kmcp.log --force\" > genbank-viral.kmcp.s2.log2 2>&1 # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.kmcp/","title":"Building KMCP database"},{"location":"database-time-and-mem-v2021.12/#building-kmcp-database","text":"The steps below is same to theses in database.md , but memusg is used to","title":"Building KMCP database"},{"location":"database-time-and-mem-v2021.12/#gtdb","text":"input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # # elapsed time: 10m:34s # peak rss: 3.87 GB # file size: 978.37 GB # memusg -t -s \"kmcp compute -I $input -O gtdb-r202-k21-n10 -k 21 -n 10 -l 150 -B plasmid \\ --log gtdb-r202-k21-n10.log -j 32 --force\" > gtdb.kmcp.s1.log2 2>&1 # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 # # elapsed time: 11m:48s # peak rss: 13.95 GB # file size: 58.03 GB # memusg -t -s \"kmcp index -j 32 -I gtdb-r202-k21-n10 -O gtdb.kmcp -n 1 -f 0.3 \\ --log gtdb.kmcp.log --force\" > gtdb.kmcp.s2.log2 2>&1 # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.kmcp/","title":"GTDB"},{"location":"database-time-and-mem-v2021.12/#refseq-fungi","text":"name=fungi input=files.renamed # elapsed time: 1m:02s # peak rss: 11.72 GB # file size: 70.52 GB # memusg -t -s \"kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log -j 32 --force\" > refseq-fungi.kmcp.s1.log2 2>&1 # elapsed time: 52.204s # peak rss: 1.19 GB # file size: 4.18 GB memusg -t -s \"kmcp index -I refseq-$name-k21-n10/ -O refseq-fungi.kmcp \\ -j 32 -f 0.3 -n 1 \\ --log refseq-fungi.kmcp.log --force\" > refseq-fungi.kmcp.s2.log2 2>&1 # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.kmcp/","title":"RefSeq fungi"},{"location":"database-time-and-mem-v2021.12/#genbank-viral","text":"name=viral input=files.renamed.slim # elapsed time: 21.051s # peak rss: 3.53 GB # file size: 9.16 GB # memusg -t -s \"kmcp compute -I $input -O genbank-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log genbank-$name-k21-n10.log -j 32 --force\" > genbank-viral.kmcp.s1.log2 2>&1 # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 # # elapsed time: 31.828s # peak rss: 3.36 GB # file size: 4.72 GB # memusg -t -s \"kmcp index -I genbank-$name-k21-n10/ -O genbank-viral.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log genbank-viral.kmcp.log --force\" > genbank-viral.kmcp.s2.log2 2>&1 # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.kmcp/","title":"Genbank viral"},{"location":"database/","text":"Database Prebuilt databases All prebuilt databases and the used reference genomes are available at: OneDrive for global users . CowTransfer for Chinese users and global users . Please click the \"kmcp\" icon on the left to browse directories of the current version, and choose each individual file to download. . Please check file integrity with `md5sum` after downloading the files: md5sum -c *.kmcp.tar.gz.md5.txt A). Databases for metagenomic profiling These databases are created following steps below . Users can also build custom databases , it's simple and fast. Current version : v2023.05 ( OneDrive , CowTransfer ) kingdoms source #NCBI-species #assemblies db-parameters size Bacteria and Archaea GTDB r214 34395+ 85205 k=21, chunks=10; fpr=0.3, hashes=1 50+49 GB* Fungi Refseq r217 491 496 k=21, chunks=10; fpr=0.3, hashes=1 5.5 GB Viruses GenBank r255 26680 33479 k=21, chunks=5; fpr=0.05, hashes=1 5.9 GB Notes : *GTDB representative genomes were split into 2 parts to build relatively small databases which can be filled into workstations with small RAM (around 64GB). Users need to search reads on all these small databases and merge the results before running kmcp profile . Taxonomy information : Either NCBI taxonomy or a combination of GTDB and NCBI are available: NCBI: taxdmp_2023-05-01 GTDB+NCBI: GTDB r214 + NCBI taxdmp_2023-05-01 Hardware requirements Prebuilt databases were built for computers with >= 32 CPU cores in consideration of better parallelization, and computers should have at least 64 GB. By default, kmcp search loads the whole database into main memory (via mmap by default) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. To reduce memory requirements on computers without enough memory, users can split the reference genomes into partitions and build a smaller database for each partition, so that the biggest database can be loaded into RAM . This can also accelerate searching on a computer cluster, where every node searches against a small database. After performing database searching, search results from all small databases can be merged with kmcp merge for downstream analysis . B). Databases for genome similarity estimation Check the tutorial . FracMinHash (Scaled MinHash), links: OneDrive , CowTransfer kingdoms source #NCBI-species #assemblies parameters size Bacteria and Archaea GTDB r214 34395+ 85205 k=31, scale=1000 2.7 GB Fungi Refseq r217 491 496 k=31, scale=1000 131 MB Viruses GenBank r255 26680 33479 k=31, scale=10 2.0 GB Building databases GTDB Tools: brename for batching renaming files. rush for executing jobs in parallel. seqkit for FASTA file processing. csvtk for tsv/csv data manipulations. taxonkit for NCBI taxonomy data manipulations. kmcp for metagenomic profiling. Files (https://data.ace.uq.edu.au/public/gtdb/data/releases/latest/) representative genomes: gtdb_genomes_reps.tar.gz metagdata: ar53_metadata.tar.gz metagdata: bac120_metadata.tar.gz taxdump file: https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump_archive/ Uncompressing and renaming: # uncompress mkdir -p gtdb tar -zxvf gtdb_genomes_reps.tar.gz -C gtdb # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' gtdb Mapping file: # assembly accession -> full head find gtdb/ -name \"*.fna.gz\" \\ | rush -k 'echo -ne \"{%@(.+).fna}\\t$(seqkit sort --quiet -lr {} | head -n 1 | seqkit seq -n)\\n\" ' \\ > name.map tar -zxvf ar53_metadata.tar.gz tar -zxvf bac120_metadata.tar.gz # assembly accession -> taxid cat *_metadata*.tsv \\ | csvtk cut -t -f accession,ncbi_taxid \\ | csvtk replace -t -p '^.._' \\ | csvtk grep -t -P <(cut -f 1 name.map) \\ | csvtk del-header \\ > taxid.map # stats (optional) # taxdump is a directory containing NCBI taxdump files, including names.dmp, nodes.dmp, delnodes.dmp and merged.dmp cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > gtdb.taxid.map.stats.tsv # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump/ -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 31165 strain 4073 subspecies 104 forma specialis 59 no rank 31 isolate 26 Building database (all k-mers, for profiling on short-reads): input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -I $input -O gtdb-k21-n10 -k 21 -n 10 -l 150 -B plasmid \\ --log gtdb-k21-n10.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-k21-n10 -O gtdb.kmcp -n 1 -f 0.3 \\ --log gtdb.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.kmcp/ # clean up rm -rf gtdb-k21-n10 Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): input=gtdb find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=2 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $input.n$n- # create database for every chunks for f in $input.n$n-*; do echo $f # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -i $f -O $f-k21-n10 -k 21 -n 10 -l 150 -B plasmid \\ --log $f-k21-n10.log --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I $f-k21-n10 -O $f.kmcp -n 1 -f 0.3 \\ --log $f.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done # rename small databases # for example: # [INFO] checking: [ ok ] 'gtdb.n2-00.kmcp' -> 'gtdb.part_1.kmcp' # [INFO] checking: [ ok ] 'gtdb.n2-01.kmcp' -> 'gtdb.part_2.kmcp' # [INFO] 2 path(s) to be renamed brename -D --only-dir -p \"n$n-\\d+\" -r \"part_{nr}\" -d # clean up for f in $input.n$n-*; do rm -rf $f-k21-n10 done RefSeq viral or fungi Tools genome_updater (0.5.1) for downloading genomes from NCBI. Downloading viral and fungi sequences: name=fungi # name=viral # -k for dry-run # -i for fix time genome_updater.sh \\ -d \"refseq\"\\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"refseq-$name\" \\ -t 12 \\ -m -a -p # cd to 2021-09-30_19-35-19 # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > refseq-$name.taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- # for viral, only splitting into 10 chunks name=viral input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n10/ -O refseq-viral.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.kmcp/ # ----------------------------------------------------------------- # for fungi name=fungi input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log --force kmcp index -I refseq-$name-k21-n10/ -O refseq-fungi.kmcp \\ -j 32 -f 0.3 -n 1 \\ --log refseq-fungi.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.kmcp/ Genbank viral Tools genome_updater for downloading genomes from NCBI. Downloading viral sequences: name=viral # -k for dry-run # -i for fix # keep at most 5 genomes for a taxid: # genome_updater v0.5.1: -A 5 -c \"\" -l \"\" # genome_updater v0.2.5: -j taxids:5 -c \"all\" -l \"all\" time genome_updater.sh \\ -d \"genbank\"\\ -A 5 \\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"genbank-$name\" \\ -t 12 \\ -m -a -p # cd genbank-viral/2021-12-06_15-27-37/ # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > genbank-viral.taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Keep at most 5 genomes for a taxid (optional) # ----------------------------------------------------------------- # keep at most 5 genomes for a taxid # backup cat taxid.map > taxid.all.map cat name.map > name.all.map # keep at most 5 ids for a taxid cat taxid.all.map \\ | csvtk sort -Ht -k 2:n -k 1:n \\ | csvtk uniq -Ht -f 2 -n 5 \\ > taxid.map cat name.all.map \\ | csvtk grep -Ht -P <(cut -f 1 taxid.map) \\ > name.map # organize files input=files.renamed output=files.renamed.slim mkdir -p $output cd $output find ../$input -name \"*.fna.gz\" \\ | csvtk mutate -Ht -p '/([^\\/]+).fna.gz' \\ | csvtk grep -Ht -f 2 -P <(cut -f 1 ../taxid.map) \\ | rush 'ln -s {1}' cd .. # check number of genomes ls $output | wc -l wc -l taxid.map Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- name=viral # input=files.renamed.slim input=files.renamed # ---------------- # all kmers kmcp compute -I $input -O genbank-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log genbank-$name-k21-n10.log --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -I genbank-$name-k21-n10/ -O genbank-viral.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log genbank-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.kmcp/ Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): name=genbank-viral input=files.renamed.slim find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=4 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $name.n$n- # create database for every chunks for f in $name.n$n-*; do echo $f kmcp compute -i $f -O $f-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log $f-k21-n10.log -j 24 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I $f-k21-n10/ -O $f.kmcp \\ -j 24 -f 0.05 -n 1 -x 100K -8 1M \\ --log $f.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done Human genome Downloading human genome file from CHM13 : # v1.1: wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.3_CHM13_T2T_v1.1/GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.4_T2T-CHM13v2.0/GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz Building database (all k-mers, < 6min): # v1.1: input=GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 input=GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz # splitting human genome into 1024 chunks. # The regular expression '^(\\w{3}_\\d{9}\\.\\d+).+' is for extracting 'GCA_009914755.4' from the file name. kmcp compute $input -O human-chm13-k21-n1024 \\ --ref-name-regexp '^(\\w{3}_\\d{9}\\.\\d+).+' \\ -k 21 \\ --split-number 1024 --split-overlap 150 \\ --log human-chm13-k21-n1024.log -j 32 --force # using small false positive rate: 0.3 # using more hash functions: 1 kmcp index -I human-chm13-k21-n1024/ -O human-chm13.kmcp \\ -j 8 -f 0.3 -n 1 \\ --log human-chm13.kmcp.log --force # taxid.map echo -ne \"GCA_009914755.4\\t9606\\n\" > taxid.map # name.mapp echo -ne \"GCA_009914755.4\\tHomo sapiens isolate CHM13\\n\" > name.map # cp name mapping file to database directory cp taxid.map name.map human-chm13.kmcp/ Refseq plasmid Downloading plasmid sequences: wget https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ cat index.html \\ | perl -ne 'next unless /\"(plasmid.*genomic.fna.gz)\"/; print \"$1\\n\"' > files.txt baseurl=https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ outdir=archives; mkdir -p $outdir cat files.txt \\ | rush -j 12 -v b=$baseurl -v out=$outdir \\ 'wget -c {b}/{} -o /dev/null -O {out}/{}' -c -C download.rush # name mapping seqkit seq -n archives/*.fna.gz \\ | sed -E 's/\\s+/\\t/' \\ > name.map # length stats seqkit stats -b -a -j 12 -T archives/*.fna.gz > archives.stats.tsv # split to individual files for f in archives/*.fna.gz; do \\ seqkit split2 -s 1 --quiet $f -O refseq-plasmid; \\ done # rename files with sequence ID find refseq-plasmid -name \"*.fna.gz\" \\ | rush 'mv {} {/}/$(seqkit seq -ni {}).fna.gz' Building database (all k-mers): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --circular \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n10/ -O refseq-$name.kmcp \\ -j 32 -f 0.01 -n 3 -x 200K -X 1024 \\ --log refseq-$name.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.kmcp/ Building database (FracMinHash/Scaled MinHash): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-D10 \\ -k 31 --circular --scale 10 \\ --log refseq-$name-k31-D10.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-D10/ -O refseq-$name.minhash.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.minhash.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.minhash.kmcp/ Building database (Closed Syncmer): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-S21 \\ -k 31 --circular --syncmer-s 21 \\ --log refseq-$name-k31-S21.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-S21/ -O refseq-$name.syncmer.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.syncmer.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.syncmer.kmcp/ Building databases (prokaryotic genome collections) HumGut (30,691 clusters) HumGut is a comprehensive Human Gut prokaryotic genomes collection filtered by metagenome data. In this work, we aimed to create a collection of the most prevalent healthy human gut prokaryotic genomes, to be used as a reference database, including both MAGs from the human gut and ordinary RefSeq genomes. Dataset Genomes: HumGut.tar.gz Metadata: HumGut.tsv Taxdump files: NCBI taxonomy: ncbi_names.dump and ncbi_nodes.dump GTDB taxomomy: gtdb_names.dump and gtdb_nodes.dump Uncompressing and renaming: # uncompress tar -zxvf HumGut.tar.gz # taxdump files mkdir taxdump mv ncbi_names.dmp taxdump/names.dmp mv ncbi_nodes.dmp taxdump/nodes.dmp Mapping file: # assembly accession -> taxid cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid.map # assembly accession -> name cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_organism_name \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > name.map Stats: # ------------------------------------------------------------- # NCBI taxonomy # number of species/strains cat taxid.map \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.stats species 23604 strain 6816 subspecies 161 no rank 69 serotype 38 genus 3 # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats; species 1240 strain 458 subspecies 15 genus 1 no rank 1 serotype 1 # ------------------------------------------------------------- # GTDB taxonomy cat HumGut.tsv \\ | csvtk cut -t -f genome_file,gtdbtk_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid-gtdb.map cat taxid-gtdb.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-gtdb -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid-gtdb.map.uniq.stats species 2810 genus 434 family 52 order 12 class 2 Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I fna/ -k 21 -n 10 -B plasmid -O humgut-k21-n10 -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I humgut-k21-n10 -O humgut.kmcp -n 1 -f 0.3 cp taxid.map taxid-gtdb.map humgut.kmcp/ proGenomes proGenomes v2.1 provides 84,096 consistently annotated bacterial and archaeal genomes from over 12000 species. Dataset: Representative genomes: contigs.representatives.fasta.gz NCBI taxonomy: proGenomes2.1_specI_lineageNCBI.tab Organize sequences: # download wget https://progenomes.embl.de/data/repGenomes/freeze12.contigs.representatives.fasta.gz # unzip for splitting by genome ID time seqkit seq freeze12.contigs.representatives.fasta.gz -o freeze12.contigs.representatives.fasta # split by genome ID seqkit split --by-id --two-pass --id-regexp '(^\\d+\\.\\w+)\\.' freeze12.contigs.representatives.fasta --out-dir genomes # batch rename brename -p '^.+id_' genomes # compress genomes for saving space find genomes/ -name \"*.fasta\" | rush 'gzip {}' Mapping file: # download wget https://progenomes.embl.de/data/proGenomes2.1_specI_lineageNCBI.tab ls genomes/ | sed 's/.fasta.gz//' > id.txt # id -> taxid cut -f 1 proGenomes2.1_specI_lineageNCBI.tab \\ | awk -F . '{print $0\"\\t\"$1}' \\ | csvtk grep -Ht -P id.txt \\ > taxid.map cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 8088 strain 3262 subspecies 37 isolate 25 no rank 16 forma specialis 14 biotype 1 # use the taxonomy verion of the refseq-virus: 2021-10-01 # id -> name cat taxid.map \\ | taxonkit lineage --data-dir taxdump -i 2 -n -L \\ | cut -f 1,3 \\ > name.map Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I genomes/ -k 21 -n 10 -B plasmid -O progenomes-k21-n10 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I progenomes-k21-n10 -O progenomes.kmcp -n 1 -f 0.3 cp taxid.map name.map progenomes.kmcp/ Building databases (viral genome collections) MGV Bacteriophages have important roles in the ecology of the human gut microbiome but are under-represented in reference data- bases. To address this problem, we assembled the Metagenomic Gut Virus catalogue that comprises 189,680 viral genomes from 11,810 publicly available human stool metagenomes. Over 75% of genomes represent double-stranded DNA phages that infect members of the Bacteroidia and Clostridia classes. Based on sequence clustering we identified 54,118 candidate viral spe- cies, 92% of which were not found in existing databases. The Metagenomic Gut Virus catalogue improves detection of viruses in stool metagenomes and accounts for nearly 40% of CRISPR spacers found in human gut Bacteria and Archaea. We also pro- duced a catalogue of 459,375 viral protein clusters to explore the functional potential of the gut virome. This revealed tens of thousands of diversity-generating retroelements, which use error-prone reverse transcription to mutate target genes and may be involved in the molecular arms race between phages and their bacterial hosts. https://doi.org/10.1038/s41564-021-00928-6 https://portal.nersc.gov/MGV/ Basic information (optional) $ seqkit stats mgv_contigs.fna file format type num_seqs sum_len min_len avg_len max_len mgv_contigs.fna FASTA DNA 189,680 8,803,222,510 1,244 46,410.9 553,716 # Genome completeness Complete: n=26,030 >90% complete: n=53,220 50-90% complete: n=110,430 $ seqkit seq -n mgv_contigs.fna | head -n 3 MGV-GENOME-0364295 MGV-GENOME-0364296 MGV-GENOME-0364303 Stats (optional) cat mgv_contig_info.tsv \\ | csvtk cut -t -f completeness \\ | csvtk plot hist -o completeness.hist.png # Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness == 100' \\ | csvtk nrows 32577 # >90% complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90' \\ | csvtk nrows 78814 # < 79250 # checkv_quality $ cat mgv_contig_info.tsv \\ | csvtk cut -t -f checkv_quality \\ | csvtk freq -t -nr | more checkv_quality frequency Medium-quality 110430 High-quality 53220 Complete 26030 # >90% complete && checkv_quality == High-quality/Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk nrows 78813 Stats of high-quality genomes (optional) # high-quality genomes cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ > mgv.hq.tsv # number of families cat mgv.hq.tsv \\ | csvtk freq -t -f ictv_family -nr \\ | csvtk nrow -t 19 # number of species cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id -nr \\ | csvtk nrow -t 26779 # baltimore cat mgv.hq.tsv \\ | csvtk freq -t -f baltimore -nr \\ | csvtk pretty -t baltimore frequency --------- --------- dsDNA 76527 ssDNA 2215 NULL 62 DNA 7 dsDNA-RT 1 ssRNA-RT 1 # prophage? cat mgv.hq.tsv \\ | csvtk freq -t -f prophage -nr \\ | csvtk pretty -t prophage frequency -------- --------- No 58366 Yes 20447 # species with both prophage and lytic phages cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id,prophage \\ | csvtk freq -t -f votu_id \\ | csvtk filter2 -t -f '$frequency > 1' \\ | csvtk nrow -t 2745 Prepare genomes: # ID cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk cut -t -f contig_id \\ | csvtk del-header \\ > mgv.hq.tsv.id # extract sequences seqkit grep -f mgv.hq.tsv.id mgv_contigs.fna -o mgv.hq.fasta.gz # split into files with one genome seqkit split2 -s 1 mgv.hq.fasta.gz -O mgv # rename with find mgv/ -name \"*.fasta.gz\" \\ | rush -j 20 'mv {} {/}/$(seqkit seq -ni {}).fa.gz' Create taxdump files and taxid.map with taxonkit (version >= v0.12.0): cat mgv_contig_info.tsv \\ | csvtk cut -t -f ictv_order,ictv_family,ictv_genus,votu_id,contig_id \\ | csvtk del-header \\ > mgv.taxonomy.tsv taxonkit create-taxdump mgv.taxonomy.tsv --out-dir mgv-taxdump \\ --force -A 5 -R order,family,genus,species cp mgv-taxdump/taxid.map . # name.map cat mgv-taxdump/taxid.map \\ | taxonkit lineage --data-dir mgv-taxdump/ -i 2 \\ | cut -f 1,3 \\ > name.map head -n 5 name.map MGV-GENOME-0364295 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364296 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364303 Caudovirales;crAss-phage;OTU-05782 MGV-GENOME-0364311 Caudovirales;crAss-phage;OTU-01114 MGV-GENOME-0364312 Caudovirales;crAss-phage;OTU-23935 Building database: # compute k-mers # reference genomes are split into 10 chunks # k = 21 kmcp compute -I mgv/ -k 21 -n 10 -l 150 -O mgv-k21-n10 --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -j 32 -I mgv-k21-n10 -O mgv.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log mgv.kmcp.log cp taxid.map name.map mgv.kmcp/ Building custom databases Files: Genome files (Gzip-compressed) FASTA/Q format. One genome per file with the reference identifier in the file name . TaxId mapping file (for metagenomic profiling) Two-column (reference identifier and TaxId) tab-delimited. NCBI taxonomy dump files (for metagenomic profiling). You can use taxonkit create-taxdump to create NCBI-style taxdump files for custom genome collections, which also generates a TaxId mapping file. names.dmp nodes.dmp merged.dmp (optional) delnodes.dmp (optional) Tools: kmcp for metagenomic profiling. rush for executing jobs in parallel. brename for batching renaming files (optional) Memory notes : By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database. Step 1. Computing k-mers Input: Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, Or a directory containing sequence files via the flag -I/--in-dir , with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp . The default pattern matches files with extension of fa , fasta , fna , fq , fastq , fa.gz , fasta.gz , fna.gz , fq.gz or fastq.gz . You may rename the sequence files for convenience using brename . because the sequence/genome identifier in the index and search results would be: For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. For splitting sequence mode (see details below): same to 1). For computing k-mers for each sequence: the sequence identifier. Unwanted sequences like plasmid sequences can be filtered out by the name via regular expression(s) ( -B/--seq-name-filter ). How to compute k-mers : By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. It also supports splitting sequences into chunks, this could increase the specificity in profiling results at the cost of a slower searching speed . Splitting sequences : Sequences can be split into chunks by a chunk size ( -s/--split-size ) or number of chunks ( -n/--split-number ) with overlap ( -l/--split-overlap ). In this mode, the sequences of each genome should be saved in an individual file . When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter ) in a sequence file are concatenated with k-1 Ns before splitting . Both sequence/reference IDs and chunks indices are saved for later use, in form of meta/description data in .unik files, and will be reported in kmcp search results. Metadata : Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. When parsing whole sequence files or splitting by number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp , e.g., ^(\\w{3}_\\d{9}\\.\\d+) for RefSeq assembly accessions . Multiple sizes of k-mers are supported, but a single k-mer size is good enough. Supported k-mer (sketches) types : K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): Scaled MinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Output : All outputted .unik files are saved in ${outdir} , with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree /xxx/yyy/zzz/ is built for > 1000 output files. For splitting sequence mode ( --split-size > 0 or --split-number > 0 ), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik A summary file ( ${outdir}/_info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . Performance tips : Decrease value of -j/--threads for data in hard disk drives (HDD) to reduce I/O pressure. Commands ( usage ): # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks, # k = 21 kmcp compute --in-dir refs/ \\ --kmer 21 \\ --split-number 10 \\ --split-overlap 150 \\ --seq-name-filter plasmid \\ --ref-name-regexp '(.+).fasta.gz' \\ --out-dir refs-k21-n10 # demo output 13:11:13.397 [INFO] kmcp v0.9.0 13:11:13.397 [INFO] https://github.com/shenwei356/kmcp 13:11:13.397 [INFO] 13:11:13.397 [INFO] checking input files ... 13:11:13.398 [INFO] 9 input file(s) given 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] input and output: 13:11:13.398 [INFO] input directory: refs/ 13:11:13.398 [INFO] regular expression of input files: (?i)\\.(f[aq](st[aq])?|fna)(.gz)?$ 13:11:13.398 [INFO] *regular expression for extracting reference name from file name: (?i)(.+).fasta.gz 13:11:13.398 [INFO] *regular expressions for filtering out sequences: [plasmid] 13:11:13.398 [INFO] output directory: refs-k21-n10 13:11:13.398 [INFO] 13:11:13.398 [INFO] sequences splitting: true 13:11:13.398 [INFO] split parts: 10, overlap: 100 bp 13:11:13.398 [INFO] 13:11:13.398 [INFO] k-mer (sketches) computing: 13:11:13.398 [INFO] k-mer size(s): 21 13:11:13.398 [INFO] circular genome: false 13:11:13.398 [INFO] saving exact number of k-mers: true 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] 13:11:13.398 [INFO] computing ... processed files: 9 / 9 [======================================] ETA: 0s. done 13:11:13.845 [INFO] 13:11:13.845 [INFO] elapsed time: 453.870411ms 13:11:13.845 [INFO] A summary file ( _info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . #path name chunkIdx idxNum genomeSize kmers refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_0.unik NC_010655.1 0 10 2664102 264247 refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_1.unik NC_010655.1 1 10 2664102 266237 refs-k21-n10/595/162/595/NC_012971.2.fasta.gz/NC_012971.2-chunk_0.unik NC_012971.2 0 10 4558953 450494 refs-k21-n10/292/039/292/NC_000913.3.fasta.gz/NC_000913.3-chunk_0.unik NC_000913.3 0 10 4641652 459277 refs-k21-n10/934/859/934/NC_013654.1.fasta.gz/NC_013654.1-chunk_0.unik NC_013654.1 0 10 4717338 470575 Meta data in the .unik file can be showed using kmcp utils unik-info : kmcp utils unik-info refs-k21-n10/072/380/072/NZ_CP028116.1.fasta.gz/NZ_CP028116.1-chunk_0.unik -a Step 2. Building databases KMCP builds index for k-mers (sketches) with a modified Compact Bit-sliced Signature Index ( COBS ). We completely rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed (check benchmark ). Input : The output directory generated by kmcp compute . Database size and searching accuracy : Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size . -f/--false-positive-rate : the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. -n/--num-hash : large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. The value of block size -b/--block-size is better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 Use flag -x/--block-sizeX-kmers-t , -8/--block-size8-kmers-t , and -1/--block-size1-kmers-t to separately create indexes for inputs with huge number of k-mers, for precise control of database size. Taxonomy data : No taxonomy data are included in the database. Taxonomy information are only needed in kmcp profile . Performance tips : The umber of blocks ( .uniki files) is better to be smaller than or equal to the number of CPU cores for faster searching speed. We can set -j/--threads to control the blocks number . When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. #threads files are simultaneously opened, and the max number of opened files is limited by the flag -F/--max-open-files . You may use a small value of -F/--max-open-files for hard disk drive storages. When the database is used in a new computer with more CPU cores, kmcp search could automatically scale to utilize as many cores as possible. Commands ( usage ): # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -I refs-k21-n10 \\ --threads 32 \\ --num-hash 1 \\ --false-positive-rate 0.3 \\ --out-dir refs.kmcp # demo output 22:44:17.710 [INFO] kmcp v0.9.0 22:44:17.710 [INFO] https://github.com/shenwei356/kmcp 22:44:17.710 [INFO] 22:44:17.710 [INFO] loading .unik file infos from file: refs-k21-n10/_info.txt 22:44:17.716 [INFO] 90 cached file infos loaded 22:44:17.716 [INFO] 22:44:17.716 [INFO] -------------------- [main parameters] -------------------- 22:44:17.716 [INFO] number of hashes: 1 22:44:17.716 [INFO] false positive rate: 0.300000 22:44:17.717 [INFO] k-mer size(s): 21 22:44:17.717 [INFO] split seqequence size: 0, overlap: 0 22:44:17.717 [INFO] block-sizeX-kmers-t: 10.00 M 22:44:17.717 [INFO] block-sizeX : 256.00 22:44:17.717 [INFO] block-size8-kmers-t: 20.00 M 22:44:17.717 [INFO] block-size1-kmers-t: 200.00 M 22:44:17.717 [INFO] -------------------- [main parameters] -------------------- 22:44:17.717 [INFO] 22:44:17.717 [INFO] building index ... 22:44:17.726 [WARN] ignore -X/--block-size (256) which is >= -b/--block-size (8) 22:44:17.726 [INFO] 22:44:17.726 [INFO] block size: 8 22:44:17.726 [INFO] number of index files: 32 (may be more) 22:44:17.726 [INFO] 22:44:17.726 [block #001] 1 / 1 100 % 22:44:17.726 [block #002] 1 / 1 100 % 22:44:17.726 [block #003] 1 / 1 100 % 22:44:17.726 [block #004] 1 / 1 100 % 22:44:17.726 [block #005] 1 / 1 100 % 22:44:17.726 [block #006] 1 / 1 100 % 22:44:17.726 [block #007] 1 / 1 100 % 22:44:17.726 [block #008] 1 / 1 100 % 22:44:17.726 [block #009] 1 / 1 100 % 22:44:17.727 [block #010] 1 / 1 100 % 22:44:17.727 [block #011] 1 / 1 100 % 22:44:17.727 [block #012] 1 / 1 100 % [saved index files] 12 / 12 ETA: 0s. done 22:44:17.933 [INFO] 22:44:17.933 [INFO] kmcp database with 42713316 k-mers saved to refs.kmcp 22:44:17.933 [INFO] total file size: 15.66 MB 22:44:17.933 [INFO] total index files: 12 22:44:17.933 [INFO] 22:44:17.933 [INFO] elapsed time: 223.524128ms 22:44:17.933 [INFO] Output: refs.kmcp/ \u2514\u2500\u2500 R001 \u251c\u2500\u2500 _block001.uniki \u251c\u2500\u2500 _block002.uniki \u251c\u2500\u2500 _block003.uniki \u251c\u2500\u2500 _block004.uniki \u251c\u2500\u2500 _block005.uniki \u251c\u2500\u2500 _block006.uniki \u251c\u2500\u2500 _block007.uniki \u251c\u2500\u2500 _block008.uniki \u251c\u2500\u2500 _block009.uniki \u251c\u2500\u2500 _block010.uniki \u251c\u2500\u2500 _block011.uniki \u251c\u2500\u2500 _block012.uniki \u251c\u2500\u2500 __db.yml \u2514\u2500\u2500 __name_mapping.tsv __db.yml contains configuration of the database, and _blockXXX.uniki are index files. kmcp utils index-info could show the basic information of index files: kmcp utils index-info refs.kmcp/R001/_block001.uniki file k canonical num-hashes num-sigs num-names refs.kmcp/R001/_block001.uniki 21 true 1 746442 8 Where num-sigs is the size of the bloom filters, and num-names is the number of genome (chunks). What's next? Check the tutorials . Merging GTDB and NCBI taxonomy By default, we use NCBI taxonomy database for reference genomes from GTDB and Refseq. Alternately, we can also use the GTDB taxonomy. The idea is to export lineages from both GTDB and NCBI using taxonkit reformat , and then create taxdump files from them with taxonkit create-taxdump . Mapping genome assembly accessions to GTDB lineages using existing GTDB-taxdump , which provids GTDB taxonomy taxdump files for each GTDB version and a taxid.map file mapping accessions to TaxIds of taxa at the subspecies rank. Here, we only output lineages down to the species rank and leave name of taxa at the subspecies/strain rank empty. taxonkit reformat \\ --data-dir gtdb-taxdump/R214/ \\ --taxid-field 2 \\ --format \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\\t\" \\ gtdb-taxdump/R214/taxid.map \\ | csvtk cut -H -t -f -2 -o gtdb-r214.tsv Exporting taxonomic lineages of viral and fungal taxa with rank equal to or lower than species from NCBI taxdump. For taxa whose rank is \"no rank\" below the species, we treat them as tax of strain rank ( --pseudo-strain , taxonkit v0.14.1 or later versionsneeded). # for viral data taxonkit reformat \\ --data-dir taxdump/ \\ --taxid-field 2 \\ --pseudo-strain \\ --format \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\\t{t}\" \\ taxid.map \\ | csvtk cut -H -t -f -2 -o genbank-viral.tsv # for fungal data taxonkit reformat \\ --data-dir taxdump/ \\ --taxid-field 2 \\ --pseudo-strain \\ --format \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\\t{t}\" \\ taxid.map \\ | csvtk cut -H -t -f -2 -o refseq-fungi.tsv Creating taxdump from lineages above. # move the lineage files to current directory # cp gtdb/gtdb-r214.tsv genbank-viral/genbank-viral.tsv refseq-fungi/refseq-fungi.tsv . cat gtdb-r214.tsv genbank-viral.tsv refseq-fungi.tsv \\ | taxonkit create-taxdump \\ --field-accession 1 \\ -R \"superkingdom,phylum,class,order,family,genus,species,strain\" \\ -O taxdump.gtdb+ncbi NCBI taxonomy of Viruses changed rapidly, of which some TaxIds might be deleted. You may added them to the the taxid.map file for compatibility. cat genbank-viral.tsv refseq-fungi.tsv gtdb-r214.tsv \\ | csvtk grep -Ht -f 2 -r -p \"^$\" \\ | cut -f 1 \\ | awk '{print $1\"\\t0\"}' \\ >> taxdump.gtdb+ncbi/taxid.map Some tests: # SARS-COV-2 in NCBI taxonomy $ echo 2697049 \\ | taxonkit lineage -t --data-dir taxdump/ \\ | csvtk cut -Ht -f 3 \\ | csvtk unfold -Ht -f 1 -s \";\" \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | csvtk cut -Ht -f 1,3,2 \\ | csvtk pretty -Ht 10239 superkingdom Viruses 2559587 clade Riboviria 2732396 kingdom Orthornavirae 2732408 phylum Pisuviricota 2732506 class Pisoniviricetes 76804 order Nidovirales 2499399 suborder Cornidovirineae 11118 family Coronaviridae 2501931 subfamily Orthocoronavirinae 694002 genus Betacoronavirus 2509511 subgenus Sarbecovirus 694009 species Severe acute respiratory syndrome-related coronavirus 2697049 no rank Severe acute respiratory syndrome coronavirus 2 $ echo \"Severe acute respiratory syndrome coronavirus 2\" | taxonkit name2taxid --data-dir taxdump.gtdb+ncbi/ Severe acute respiratory syndrome coronavirus 2 216305222 $ echo 216305222 \\ | taxonkit lineage -t --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 3 \\ | csvtk unfold -Ht -f 1 -s \";\" \\ | taxonkit lineage -r -n -L --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 1,3,2 \\ | csvtk pretty -Ht 1287770734 superkingdom Viruses 1506901452 phylum Pisuviricota 1091693597 class Pisoniviricetes 37745009 order Nidovirales 738421640 family Coronaviridae 906833049 genus Betacoronavirus 1015862491 species Severe acute respiratory syndrome-related coronavirus 216305222 strain Severe acute respiratory syndrome coronavirus 2 $ echo \"Escherichia coli\" | taxonkit name2taxid --data-dir taxdump.gtdb+ncbi/ Escherichia coli 1945799576 $ echo 1945799576 \\ | taxonkit lineage -t --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 3 \\ | csvtk unfold -Ht -f 1 -s \";\" \\ | taxonkit lineage -r -n -L --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 1,3,2 \\ | csvtk pretty -Ht 609216830 superkingdom Bacteria 1641076285 phylum Proteobacteria 329474883 class Gammaproteobacteria 1012954932 order Enterobacterales 87250111 family Enterobacteriaceae 1187493883 genus Escherichia 1945799576 species Escherichia coli","title":"Databases"},{"location":"database/#database","text":"","title":"Database"},{"location":"database/#prebuilt-databases","text":"All prebuilt databases and the used reference genomes are available at: OneDrive for global users . CowTransfer for Chinese users and global users . Please click the \"kmcp\" icon on the left to browse directories of the current version, and choose each individual file to download. . Please check file integrity with `md5sum` after downloading the files: md5sum -c *.kmcp.tar.gz.md5.txt","title":"Prebuilt databases"},{"location":"database/#a-databases-for-metagenomic-profiling","text":"These databases are created following steps below . Users can also build custom databases , it's simple and fast. Current version : v2023.05 ( OneDrive , CowTransfer ) kingdoms source #NCBI-species #assemblies db-parameters size Bacteria and Archaea GTDB r214 34395+ 85205 k=21, chunks=10; fpr=0.3, hashes=1 50+49 GB* Fungi Refseq r217 491 496 k=21, chunks=10; fpr=0.3, hashes=1 5.5 GB Viruses GenBank r255 26680 33479 k=21, chunks=5; fpr=0.05, hashes=1 5.9 GB Notes : *GTDB representative genomes were split into 2 parts to build relatively small databases which can be filled into workstations with small RAM (around 64GB). Users need to search reads on all these small databases and merge the results before running kmcp profile . Taxonomy information : Either NCBI taxonomy or a combination of GTDB and NCBI are available: NCBI: taxdmp_2023-05-01 GTDB+NCBI: GTDB r214 + NCBI taxdmp_2023-05-01 Hardware requirements Prebuilt databases were built for computers with >= 32 CPU cores in consideration of better parallelization, and computers should have at least 64 GB. By default, kmcp search loads the whole database into main memory (via mmap by default) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. To reduce memory requirements on computers without enough memory, users can split the reference genomes into partitions and build a smaller database for each partition, so that the biggest database can be loaded into RAM . This can also accelerate searching on a computer cluster, where every node searches against a small database. After performing database searching, search results from all small databases can be merged with kmcp merge for downstream analysis .","title":"A). Databases for metagenomic profiling"},{"location":"database/#b-databases-for-genome-similarity-estimation","text":"Check the tutorial . FracMinHash (Scaled MinHash), links: OneDrive , CowTransfer kingdoms source #NCBI-species #assemblies parameters size Bacteria and Archaea GTDB r214 34395+ 85205 k=31, scale=1000 2.7 GB Fungi Refseq r217 491 496 k=31, scale=1000 131 MB Viruses GenBank r255 26680 33479 k=31, scale=10 2.0 GB","title":"B). Databases for genome similarity estimation"},{"location":"database/#building-databases","text":"","title":"Building databases"},{"location":"database/#gtdb","text":"Tools: brename for batching renaming files. rush for executing jobs in parallel. seqkit for FASTA file processing. csvtk for tsv/csv data manipulations. taxonkit for NCBI taxonomy data manipulations. kmcp for metagenomic profiling. Files (https://data.ace.uq.edu.au/public/gtdb/data/releases/latest/) representative genomes: gtdb_genomes_reps.tar.gz metagdata: ar53_metadata.tar.gz metagdata: bac120_metadata.tar.gz taxdump file: https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump_archive/ Uncompressing and renaming: # uncompress mkdir -p gtdb tar -zxvf gtdb_genomes_reps.tar.gz -C gtdb # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' gtdb Mapping file: # assembly accession -> full head find gtdb/ -name \"*.fna.gz\" \\ | rush -k 'echo -ne \"{%@(.+).fna}\\t$(seqkit sort --quiet -lr {} | head -n 1 | seqkit seq -n)\\n\" ' \\ > name.map tar -zxvf ar53_metadata.tar.gz tar -zxvf bac120_metadata.tar.gz # assembly accession -> taxid cat *_metadata*.tsv \\ | csvtk cut -t -f accession,ncbi_taxid \\ | csvtk replace -t -p '^.._' \\ | csvtk grep -t -P <(cut -f 1 name.map) \\ | csvtk del-header \\ > taxid.map # stats (optional) # taxdump is a directory containing NCBI taxdump files, including names.dmp, nodes.dmp, delnodes.dmp and merged.dmp cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > gtdb.taxid.map.stats.tsv # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump/ -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 31165 strain 4073 subspecies 104 forma specialis 59 no rank 31 isolate 26 Building database (all k-mers, for profiling on short-reads): input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -I $input -O gtdb-k21-n10 -k 21 -n 10 -l 150 -B plasmid \\ --log gtdb-k21-n10.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-k21-n10 -O gtdb.kmcp -n 1 -f 0.3 \\ --log gtdb.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.kmcp/ # clean up rm -rf gtdb-k21-n10 Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): input=gtdb find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=2 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $input.n$n- # create database for every chunks for f in $input.n$n-*; do echo $f # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -i $f -O $f-k21-n10 -k 21 -n 10 -l 150 -B plasmid \\ --log $f-k21-n10.log --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I $f-k21-n10 -O $f.kmcp -n 1 -f 0.3 \\ --log $f.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done # rename small databases # for example: # [INFO] checking: [ ok ] 'gtdb.n2-00.kmcp' -> 'gtdb.part_1.kmcp' # [INFO] checking: [ ok ] 'gtdb.n2-01.kmcp' -> 'gtdb.part_2.kmcp' # [INFO] 2 path(s) to be renamed brename -D --only-dir -p \"n$n-\\d+\" -r \"part_{nr}\" -d # clean up for f in $input.n$n-*; do rm -rf $f-k21-n10 done","title":"GTDB"},{"location":"database/#refseq-viral-or-fungi","text":"Tools genome_updater (0.5.1) for downloading genomes from NCBI. Downloading viral and fungi sequences: name=fungi # name=viral # -k for dry-run # -i for fix time genome_updater.sh \\ -d \"refseq\"\\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"refseq-$name\" \\ -t 12 \\ -m -a -p # cd to 2021-09-30_19-35-19 # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > refseq-$name.taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- # for viral, only splitting into 10 chunks name=viral input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n10/ -O refseq-viral.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.kmcp/ # ----------------------------------------------------------------- # for fungi name=fungi input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log --force kmcp index -I refseq-$name-k21-n10/ -O refseq-fungi.kmcp \\ -j 32 -f 0.3 -n 1 \\ --log refseq-fungi.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.kmcp/","title":"RefSeq viral or fungi"},{"location":"database/#genbank-viral","text":"Tools genome_updater for downloading genomes from NCBI. Downloading viral sequences: name=viral # -k for dry-run # -i for fix # keep at most 5 genomes for a taxid: # genome_updater v0.5.1: -A 5 -c \"\" -l \"\" # genome_updater v0.2.5: -j taxids:5 -c \"all\" -l \"all\" time genome_updater.sh \\ -d \"genbank\"\\ -A 5 \\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"genbank-$name\" \\ -t 12 \\ -m -a -p # cd genbank-viral/2021-12-06_15-27-37/ # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > genbank-viral.taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Keep at most 5 genomes for a taxid (optional) # ----------------------------------------------------------------- # keep at most 5 genomes for a taxid # backup cat taxid.map > taxid.all.map cat name.map > name.all.map # keep at most 5 ids for a taxid cat taxid.all.map \\ | csvtk sort -Ht -k 2:n -k 1:n \\ | csvtk uniq -Ht -f 2 -n 5 \\ > taxid.map cat name.all.map \\ | csvtk grep -Ht -P <(cut -f 1 taxid.map) \\ > name.map # organize files input=files.renamed output=files.renamed.slim mkdir -p $output cd $output find ../$input -name \"*.fna.gz\" \\ | csvtk mutate -Ht -p '/([^\\/]+).fna.gz' \\ | csvtk grep -Ht -f 2 -P <(cut -f 1 ../taxid.map) \\ | rush 'ln -s {1}' cd .. # check number of genomes ls $output | wc -l wc -l taxid.map Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- name=viral # input=files.renamed.slim input=files.renamed # ---------------- # all kmers kmcp compute -I $input -O genbank-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log genbank-$name-k21-n10.log --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -I genbank-$name-k21-n10/ -O genbank-viral.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log genbank-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.kmcp/ Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): name=genbank-viral input=files.renamed.slim find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=4 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $name.n$n- # create database for every chunks for f in $name.n$n-*; do echo $f kmcp compute -i $f -O $f-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 150 \\ --log $f-k21-n10.log -j 24 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I $f-k21-n10/ -O $f.kmcp \\ -j 24 -f 0.05 -n 1 -x 100K -8 1M \\ --log $f.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done","title":"Genbank viral"},{"location":"database/#human-genome","text":"Downloading human genome file from CHM13 : # v1.1: wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.3_CHM13_T2T_v1.1/GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.4_T2T-CHM13v2.0/GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz Building database (all k-mers, < 6min): # v1.1: input=GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 input=GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz # splitting human genome into 1024 chunks. # The regular expression '^(\\w{3}_\\d{9}\\.\\d+).+' is for extracting 'GCA_009914755.4' from the file name. kmcp compute $input -O human-chm13-k21-n1024 \\ --ref-name-regexp '^(\\w{3}_\\d{9}\\.\\d+).+' \\ -k 21 \\ --split-number 1024 --split-overlap 150 \\ --log human-chm13-k21-n1024.log -j 32 --force # using small false positive rate: 0.3 # using more hash functions: 1 kmcp index -I human-chm13-k21-n1024/ -O human-chm13.kmcp \\ -j 8 -f 0.3 -n 1 \\ --log human-chm13.kmcp.log --force # taxid.map echo -ne \"GCA_009914755.4\\t9606\\n\" > taxid.map # name.mapp echo -ne \"GCA_009914755.4\\tHomo sapiens isolate CHM13\\n\" > name.map # cp name mapping file to database directory cp taxid.map name.map human-chm13.kmcp/","title":"Human genome"},{"location":"database/#refseq-plasmid","text":"Downloading plasmid sequences: wget https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ cat index.html \\ | perl -ne 'next unless /\"(plasmid.*genomic.fna.gz)\"/; print \"$1\\n\"' > files.txt baseurl=https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ outdir=archives; mkdir -p $outdir cat files.txt \\ | rush -j 12 -v b=$baseurl -v out=$outdir \\ 'wget -c {b}/{} -o /dev/null -O {out}/{}' -c -C download.rush # name mapping seqkit seq -n archives/*.fna.gz \\ | sed -E 's/\\s+/\\t/' \\ > name.map # length stats seqkit stats -b -a -j 12 -T archives/*.fna.gz > archives.stats.tsv # split to individual files for f in archives/*.fna.gz; do \\ seqkit split2 -s 1 --quiet $f -O refseq-plasmid; \\ done # rename files with sequence ID find refseq-plasmid -name \"*.fna.gz\" \\ | rush 'mv {} {/}/$(seqkit seq -ni {}).fna.gz' Building database (all k-mers): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --circular \\ --split-number 10 --split-overlap 150 \\ --log refseq-$name-k21-n10.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n10/ -O refseq-$name.kmcp \\ -j 32 -f 0.01 -n 3 -x 200K -X 1024 \\ --log refseq-$name.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.kmcp/ Building database (FracMinHash/Scaled MinHash): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-D10 \\ -k 31 --circular --scale 10 \\ --log refseq-$name-k31-D10.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-D10/ -O refseq-$name.minhash.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.minhash.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.minhash.kmcp/ Building database (Closed Syncmer): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-S21 \\ -k 31 --circular --syncmer-s 21 \\ --log refseq-$name-k31-S21.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-S21/ -O refseq-$name.syncmer.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.syncmer.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.syncmer.kmcp/","title":"Refseq plasmid"},{"location":"database/#building-databases-prokaryotic-genome-collections","text":"","title":"Building databases (prokaryotic genome collections)"},{"location":"database/#humgut-30691-clusters","text":"HumGut is a comprehensive Human Gut prokaryotic genomes collection filtered by metagenome data. In this work, we aimed to create a collection of the most prevalent healthy human gut prokaryotic genomes, to be used as a reference database, including both MAGs from the human gut and ordinary RefSeq genomes. Dataset Genomes: HumGut.tar.gz Metadata: HumGut.tsv Taxdump files: NCBI taxonomy: ncbi_names.dump and ncbi_nodes.dump GTDB taxomomy: gtdb_names.dump and gtdb_nodes.dump Uncompressing and renaming: # uncompress tar -zxvf HumGut.tar.gz # taxdump files mkdir taxdump mv ncbi_names.dmp taxdump/names.dmp mv ncbi_nodes.dmp taxdump/nodes.dmp Mapping file: # assembly accession -> taxid cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid.map # assembly accession -> name cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_organism_name \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > name.map Stats: # ------------------------------------------------------------- # NCBI taxonomy # number of species/strains cat taxid.map \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.stats species 23604 strain 6816 subspecies 161 no rank 69 serotype 38 genus 3 # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats; species 1240 strain 458 subspecies 15 genus 1 no rank 1 serotype 1 # ------------------------------------------------------------- # GTDB taxonomy cat HumGut.tsv \\ | csvtk cut -t -f genome_file,gtdbtk_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid-gtdb.map cat taxid-gtdb.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-gtdb -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid-gtdb.map.uniq.stats species 2810 genus 434 family 52 order 12 class 2 Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I fna/ -k 21 -n 10 -B plasmid -O humgut-k21-n10 -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I humgut-k21-n10 -O humgut.kmcp -n 1 -f 0.3 cp taxid.map taxid-gtdb.map humgut.kmcp/","title":"HumGut (30,691 clusters)"},{"location":"database/#progenomes","text":"proGenomes v2.1 provides 84,096 consistently annotated bacterial and archaeal genomes from over 12000 species. Dataset: Representative genomes: contigs.representatives.fasta.gz NCBI taxonomy: proGenomes2.1_specI_lineageNCBI.tab Organize sequences: # download wget https://progenomes.embl.de/data/repGenomes/freeze12.contigs.representatives.fasta.gz # unzip for splitting by genome ID time seqkit seq freeze12.contigs.representatives.fasta.gz -o freeze12.contigs.representatives.fasta # split by genome ID seqkit split --by-id --two-pass --id-regexp '(^\\d+\\.\\w+)\\.' freeze12.contigs.representatives.fasta --out-dir genomes # batch rename brename -p '^.+id_' genomes # compress genomes for saving space find genomes/ -name \"*.fasta\" | rush 'gzip {}' Mapping file: # download wget https://progenomes.embl.de/data/proGenomes2.1_specI_lineageNCBI.tab ls genomes/ | sed 's/.fasta.gz//' > id.txt # id -> taxid cut -f 1 proGenomes2.1_specI_lineageNCBI.tab \\ | awk -F . '{print $0\"\\t\"$1}' \\ | csvtk grep -Ht -P id.txt \\ > taxid.map cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 8088 strain 3262 subspecies 37 isolate 25 no rank 16 forma specialis 14 biotype 1 # use the taxonomy verion of the refseq-virus: 2021-10-01 # id -> name cat taxid.map \\ | taxonkit lineage --data-dir taxdump -i 2 -n -L \\ | cut -f 1,3 \\ > name.map Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I genomes/ -k 21 -n 10 -B plasmid -O progenomes-k21-n10 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I progenomes-k21-n10 -O progenomes.kmcp -n 1 -f 0.3 cp taxid.map name.map progenomes.kmcp/","title":"proGenomes"},{"location":"database/#building-databases-viral-genome-collections","text":"","title":"Building databases (viral genome collections)"},{"location":"database/#mgv","text":"Bacteriophages have important roles in the ecology of the human gut microbiome but are under-represented in reference data- bases. To address this problem, we assembled the Metagenomic Gut Virus catalogue that comprises 189,680 viral genomes from 11,810 publicly available human stool metagenomes. Over 75% of genomes represent double-stranded DNA phages that infect members of the Bacteroidia and Clostridia classes. Based on sequence clustering we identified 54,118 candidate viral spe- cies, 92% of which were not found in existing databases. The Metagenomic Gut Virus catalogue improves detection of viruses in stool metagenomes and accounts for nearly 40% of CRISPR spacers found in human gut Bacteria and Archaea. We also pro- duced a catalogue of 459,375 viral protein clusters to explore the functional potential of the gut virome. This revealed tens of thousands of diversity-generating retroelements, which use error-prone reverse transcription to mutate target genes and may be involved in the molecular arms race between phages and their bacterial hosts. https://doi.org/10.1038/s41564-021-00928-6 https://portal.nersc.gov/MGV/ Basic information (optional) $ seqkit stats mgv_contigs.fna file format type num_seqs sum_len min_len avg_len max_len mgv_contigs.fna FASTA DNA 189,680 8,803,222,510 1,244 46,410.9 553,716 # Genome completeness Complete: n=26,030 >90% complete: n=53,220 50-90% complete: n=110,430 $ seqkit seq -n mgv_contigs.fna | head -n 3 MGV-GENOME-0364295 MGV-GENOME-0364296 MGV-GENOME-0364303 Stats (optional) cat mgv_contig_info.tsv \\ | csvtk cut -t -f completeness \\ | csvtk plot hist -o completeness.hist.png # Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness == 100' \\ | csvtk nrows 32577 # >90% complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90' \\ | csvtk nrows 78814 # < 79250 # checkv_quality $ cat mgv_contig_info.tsv \\ | csvtk cut -t -f checkv_quality \\ | csvtk freq -t -nr | more checkv_quality frequency Medium-quality 110430 High-quality 53220 Complete 26030 # >90% complete && checkv_quality == High-quality/Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk nrows 78813 Stats of high-quality genomes (optional) # high-quality genomes cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ > mgv.hq.tsv # number of families cat mgv.hq.tsv \\ | csvtk freq -t -f ictv_family -nr \\ | csvtk nrow -t 19 # number of species cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id -nr \\ | csvtk nrow -t 26779 # baltimore cat mgv.hq.tsv \\ | csvtk freq -t -f baltimore -nr \\ | csvtk pretty -t baltimore frequency --------- --------- dsDNA 76527 ssDNA 2215 NULL 62 DNA 7 dsDNA-RT 1 ssRNA-RT 1 # prophage? cat mgv.hq.tsv \\ | csvtk freq -t -f prophage -nr \\ | csvtk pretty -t prophage frequency -------- --------- No 58366 Yes 20447 # species with both prophage and lytic phages cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id,prophage \\ | csvtk freq -t -f votu_id \\ | csvtk filter2 -t -f '$frequency > 1' \\ | csvtk nrow -t 2745 Prepare genomes: # ID cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk cut -t -f contig_id \\ | csvtk del-header \\ > mgv.hq.tsv.id # extract sequences seqkit grep -f mgv.hq.tsv.id mgv_contigs.fna -o mgv.hq.fasta.gz # split into files with one genome seqkit split2 -s 1 mgv.hq.fasta.gz -O mgv # rename with find mgv/ -name \"*.fasta.gz\" \\ | rush -j 20 'mv {} {/}/$(seqkit seq -ni {}).fa.gz' Create taxdump files and taxid.map with taxonkit (version >= v0.12.0): cat mgv_contig_info.tsv \\ | csvtk cut -t -f ictv_order,ictv_family,ictv_genus,votu_id,contig_id \\ | csvtk del-header \\ > mgv.taxonomy.tsv taxonkit create-taxdump mgv.taxonomy.tsv --out-dir mgv-taxdump \\ --force -A 5 -R order,family,genus,species cp mgv-taxdump/taxid.map . # name.map cat mgv-taxdump/taxid.map \\ | taxonkit lineage --data-dir mgv-taxdump/ -i 2 \\ | cut -f 1,3 \\ > name.map head -n 5 name.map MGV-GENOME-0364295 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364296 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364303 Caudovirales;crAss-phage;OTU-05782 MGV-GENOME-0364311 Caudovirales;crAss-phage;OTU-01114 MGV-GENOME-0364312 Caudovirales;crAss-phage;OTU-23935 Building database: # compute k-mers # reference genomes are split into 10 chunks # k = 21 kmcp compute -I mgv/ -k 21 -n 10 -l 150 -O mgv-k21-n10 --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -j 32 -I mgv-k21-n10 -O mgv.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log mgv.kmcp.log cp taxid.map name.map mgv.kmcp/","title":"MGV"},{"location":"database/#building-custom-databases","text":"Files: Genome files (Gzip-compressed) FASTA/Q format. One genome per file with the reference identifier in the file name . TaxId mapping file (for metagenomic profiling) Two-column (reference identifier and TaxId) tab-delimited. NCBI taxonomy dump files (for metagenomic profiling). You can use taxonkit create-taxdump to create NCBI-style taxdump files for custom genome collections, which also generates a TaxId mapping file. names.dmp nodes.dmp merged.dmp (optional) delnodes.dmp (optional) Tools: kmcp for metagenomic profiling. rush for executing jobs in parallel. brename for batching renaming files (optional) Memory notes : By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database.","title":"Building custom databases"},{"location":"database/#step-1-computing-k-mers","text":"Input: Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, Or a directory containing sequence files via the flag -I/--in-dir , with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp . The default pattern matches files with extension of fa , fasta , fna , fq , fastq , fa.gz , fasta.gz , fna.gz , fq.gz or fastq.gz . You may rename the sequence files for convenience using brename . because the sequence/genome identifier in the index and search results would be: For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. For splitting sequence mode (see details below): same to 1). For computing k-mers for each sequence: the sequence identifier. Unwanted sequences like plasmid sequences can be filtered out by the name via regular expression(s) ( -B/--seq-name-filter ). How to compute k-mers : By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. It also supports splitting sequences into chunks, this could increase the specificity in profiling results at the cost of a slower searching speed . Splitting sequences : Sequences can be split into chunks by a chunk size ( -s/--split-size ) or number of chunks ( -n/--split-number ) with overlap ( -l/--split-overlap ). In this mode, the sequences of each genome should be saved in an individual file . When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter ) in a sequence file are concatenated with k-1 Ns before splitting . Both sequence/reference IDs and chunks indices are saved for later use, in form of meta/description data in .unik files, and will be reported in kmcp search results. Metadata : Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. When parsing whole sequence files or splitting by number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp , e.g., ^(\\w{3}_\\d{9}\\.\\d+) for RefSeq assembly accessions . Multiple sizes of k-mers are supported, but a single k-mer size is good enough. Supported k-mer (sketches) types : K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): Scaled MinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Output : All outputted .unik files are saved in ${outdir} , with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree /xxx/yyy/zzz/ is built for > 1000 output files. For splitting sequence mode ( --split-size > 0 or --split-number > 0 ), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik A summary file ( ${outdir}/_info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . Performance tips : Decrease value of -j/--threads for data in hard disk drives (HDD) to reduce I/O pressure. Commands ( usage ): # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks, # k = 21 kmcp compute --in-dir refs/ \\ --kmer 21 \\ --split-number 10 \\ --split-overlap 150 \\ --seq-name-filter plasmid \\ --ref-name-regexp '(.+).fasta.gz' \\ --out-dir refs-k21-n10 # demo output 13:11:13.397 [INFO] kmcp v0.9.0 13:11:13.397 [INFO] https://github.com/shenwei356/kmcp 13:11:13.397 [INFO] 13:11:13.397 [INFO] checking input files ... 13:11:13.398 [INFO] 9 input file(s) given 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] input and output: 13:11:13.398 [INFO] input directory: refs/ 13:11:13.398 [INFO] regular expression of input files: (?i)\\.(f[aq](st[aq])?|fna)(.gz)?$ 13:11:13.398 [INFO] *regular expression for extracting reference name from file name: (?i)(.+).fasta.gz 13:11:13.398 [INFO] *regular expressions for filtering out sequences: [plasmid] 13:11:13.398 [INFO] output directory: refs-k21-n10 13:11:13.398 [INFO] 13:11:13.398 [INFO] sequences splitting: true 13:11:13.398 [INFO] split parts: 10, overlap: 100 bp 13:11:13.398 [INFO] 13:11:13.398 [INFO] k-mer (sketches) computing: 13:11:13.398 [INFO] k-mer size(s): 21 13:11:13.398 [INFO] circular genome: false 13:11:13.398 [INFO] saving exact number of k-mers: true 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] 13:11:13.398 [INFO] computing ... processed files: 9 / 9 [======================================] ETA: 0s. done 13:11:13.845 [INFO] 13:11:13.845 [INFO] elapsed time: 453.870411ms 13:11:13.845 [INFO] A summary file ( _info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . #path name chunkIdx idxNum genomeSize kmers refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_0.unik NC_010655.1 0 10 2664102 264247 refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_1.unik NC_010655.1 1 10 2664102 266237 refs-k21-n10/595/162/595/NC_012971.2.fasta.gz/NC_012971.2-chunk_0.unik NC_012971.2 0 10 4558953 450494 refs-k21-n10/292/039/292/NC_000913.3.fasta.gz/NC_000913.3-chunk_0.unik NC_000913.3 0 10 4641652 459277 refs-k21-n10/934/859/934/NC_013654.1.fasta.gz/NC_013654.1-chunk_0.unik NC_013654.1 0 10 4717338 470575 Meta data in the .unik file can be showed using kmcp utils unik-info : kmcp utils unik-info refs-k21-n10/072/380/072/NZ_CP028116.1.fasta.gz/NZ_CP028116.1-chunk_0.unik -a","title":"Step 1. Computing k-mers"},{"location":"database/#step-2-building-databases","text":"KMCP builds index for k-mers (sketches) with a modified Compact Bit-sliced Signature Index ( COBS ). We completely rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed (check benchmark ). Input : The output directory generated by kmcp compute . Database size and searching accuracy : Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size . -f/--false-positive-rate : the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. -n/--num-hash : large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. The value of block size -b/--block-size is better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 Use flag -x/--block-sizeX-kmers-t , -8/--block-size8-kmers-t , and -1/--block-size1-kmers-t to separately create indexes for inputs with huge number of k-mers, for precise control of database size. Taxonomy data : No taxonomy data are included in the database. Taxonomy information are only needed in kmcp profile . Performance tips : The umber of blocks ( .uniki files) is better to be smaller than or equal to the number of CPU cores for faster searching speed. We can set -j/--threads to control the blocks number . When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. #threads files are simultaneously opened, and the max number of opened files is limited by the flag -F/--max-open-files . You may use a small value of -F/--max-open-files for hard disk drive storages. When the database is used in a new computer with more CPU cores, kmcp search could automatically scale to utilize as many cores as possible. Commands ( usage ): # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -I refs-k21-n10 \\ --threads 32 \\ --num-hash 1 \\ --false-positive-rate 0.3 \\ --out-dir refs.kmcp # demo output 22:44:17.710 [INFO] kmcp v0.9.0 22:44:17.710 [INFO] https://github.com/shenwei356/kmcp 22:44:17.710 [INFO] 22:44:17.710 [INFO] loading .unik file infos from file: refs-k21-n10/_info.txt 22:44:17.716 [INFO] 90 cached file infos loaded 22:44:17.716 [INFO] 22:44:17.716 [INFO] -------------------- [main parameters] -------------------- 22:44:17.716 [INFO] number of hashes: 1 22:44:17.716 [INFO] false positive rate: 0.300000 22:44:17.717 [INFO] k-mer size(s): 21 22:44:17.717 [INFO] split seqequence size: 0, overlap: 0 22:44:17.717 [INFO] block-sizeX-kmers-t: 10.00 M 22:44:17.717 [INFO] block-sizeX : 256.00 22:44:17.717 [INFO] block-size8-kmers-t: 20.00 M 22:44:17.717 [INFO] block-size1-kmers-t: 200.00 M 22:44:17.717 [INFO] -------------------- [main parameters] -------------------- 22:44:17.717 [INFO] 22:44:17.717 [INFO] building index ... 22:44:17.726 [WARN] ignore -X/--block-size (256) which is >= -b/--block-size (8) 22:44:17.726 [INFO] 22:44:17.726 [INFO] block size: 8 22:44:17.726 [INFO] number of index files: 32 (may be more) 22:44:17.726 [INFO] 22:44:17.726 [block #001] 1 / 1 100 % 22:44:17.726 [block #002] 1 / 1 100 % 22:44:17.726 [block #003] 1 / 1 100 % 22:44:17.726 [block #004] 1 / 1 100 % 22:44:17.726 [block #005] 1 / 1 100 % 22:44:17.726 [block #006] 1 / 1 100 % 22:44:17.726 [block #007] 1 / 1 100 % 22:44:17.726 [block #008] 1 / 1 100 % 22:44:17.726 [block #009] 1 / 1 100 % 22:44:17.727 [block #010] 1 / 1 100 % 22:44:17.727 [block #011] 1 / 1 100 % 22:44:17.727 [block #012] 1 / 1 100 % [saved index files] 12 / 12 ETA: 0s. done 22:44:17.933 [INFO] 22:44:17.933 [INFO] kmcp database with 42713316 k-mers saved to refs.kmcp 22:44:17.933 [INFO] total file size: 15.66 MB 22:44:17.933 [INFO] total index files: 12 22:44:17.933 [INFO] 22:44:17.933 [INFO] elapsed time: 223.524128ms 22:44:17.933 [INFO] Output: refs.kmcp/ \u2514\u2500\u2500 R001 \u251c\u2500\u2500 _block001.uniki \u251c\u2500\u2500 _block002.uniki \u251c\u2500\u2500 _block003.uniki \u251c\u2500\u2500 _block004.uniki \u251c\u2500\u2500 _block005.uniki \u251c\u2500\u2500 _block006.uniki \u251c\u2500\u2500 _block007.uniki \u251c\u2500\u2500 _block008.uniki \u251c\u2500\u2500 _block009.uniki \u251c\u2500\u2500 _block010.uniki \u251c\u2500\u2500 _block011.uniki \u251c\u2500\u2500 _block012.uniki \u251c\u2500\u2500 __db.yml \u2514\u2500\u2500 __name_mapping.tsv __db.yml contains configuration of the database, and _blockXXX.uniki are index files. kmcp utils index-info could show the basic information of index files: kmcp utils index-info refs.kmcp/R001/_block001.uniki file k canonical num-hashes num-sigs num-names refs.kmcp/R001/_block001.uniki 21 true 1 746442 8 Where num-sigs is the size of the bloom filters, and num-names is the number of genome (chunks). What's next? Check the tutorials .","title":"Step 2. Building databases"},{"location":"database/#merging-gtdb-and-ncbi-taxonomy","text":"By default, we use NCBI taxonomy database for reference genomes from GTDB and Refseq. Alternately, we can also use the GTDB taxonomy. The idea is to export lineages from both GTDB and NCBI using taxonkit reformat , and then create taxdump files from them with taxonkit create-taxdump . Mapping genome assembly accessions to GTDB lineages using existing GTDB-taxdump , which provids GTDB taxonomy taxdump files for each GTDB version and a taxid.map file mapping accessions to TaxIds of taxa at the subspecies rank. Here, we only output lineages down to the species rank and leave name of taxa at the subspecies/strain rank empty. taxonkit reformat \\ --data-dir gtdb-taxdump/R214/ \\ --taxid-field 2 \\ --format \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\\t\" \\ gtdb-taxdump/R214/taxid.map \\ | csvtk cut -H -t -f -2 -o gtdb-r214.tsv Exporting taxonomic lineages of viral and fungal taxa with rank equal to or lower than species from NCBI taxdump. For taxa whose rank is \"no rank\" below the species, we treat them as tax of strain rank ( --pseudo-strain , taxonkit v0.14.1 or later versionsneeded). # for viral data taxonkit reformat \\ --data-dir taxdump/ \\ --taxid-field 2 \\ --pseudo-strain \\ --format \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\\t{t}\" \\ taxid.map \\ | csvtk cut -H -t -f -2 -o genbank-viral.tsv # for fungal data taxonkit reformat \\ --data-dir taxdump/ \\ --taxid-field 2 \\ --pseudo-strain \\ --format \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\\t{t}\" \\ taxid.map \\ | csvtk cut -H -t -f -2 -o refseq-fungi.tsv Creating taxdump from lineages above. # move the lineage files to current directory # cp gtdb/gtdb-r214.tsv genbank-viral/genbank-viral.tsv refseq-fungi/refseq-fungi.tsv . cat gtdb-r214.tsv genbank-viral.tsv refseq-fungi.tsv \\ | taxonkit create-taxdump \\ --field-accession 1 \\ -R \"superkingdom,phylum,class,order,family,genus,species,strain\" \\ -O taxdump.gtdb+ncbi NCBI taxonomy of Viruses changed rapidly, of which some TaxIds might be deleted. You may added them to the the taxid.map file for compatibility. cat genbank-viral.tsv refseq-fungi.tsv gtdb-r214.tsv \\ | csvtk grep -Ht -f 2 -r -p \"^$\" \\ | cut -f 1 \\ | awk '{print $1\"\\t0\"}' \\ >> taxdump.gtdb+ncbi/taxid.map Some tests: # SARS-COV-2 in NCBI taxonomy $ echo 2697049 \\ | taxonkit lineage -t --data-dir taxdump/ \\ | csvtk cut -Ht -f 3 \\ | csvtk unfold -Ht -f 1 -s \";\" \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | csvtk cut -Ht -f 1,3,2 \\ | csvtk pretty -Ht 10239 superkingdom Viruses 2559587 clade Riboviria 2732396 kingdom Orthornavirae 2732408 phylum Pisuviricota 2732506 class Pisoniviricetes 76804 order Nidovirales 2499399 suborder Cornidovirineae 11118 family Coronaviridae 2501931 subfamily Orthocoronavirinae 694002 genus Betacoronavirus 2509511 subgenus Sarbecovirus 694009 species Severe acute respiratory syndrome-related coronavirus 2697049 no rank Severe acute respiratory syndrome coronavirus 2 $ echo \"Severe acute respiratory syndrome coronavirus 2\" | taxonkit name2taxid --data-dir taxdump.gtdb+ncbi/ Severe acute respiratory syndrome coronavirus 2 216305222 $ echo 216305222 \\ | taxonkit lineage -t --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 3 \\ | csvtk unfold -Ht -f 1 -s \";\" \\ | taxonkit lineage -r -n -L --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 1,3,2 \\ | csvtk pretty -Ht 1287770734 superkingdom Viruses 1506901452 phylum Pisuviricota 1091693597 class Pisoniviricetes 37745009 order Nidovirales 738421640 family Coronaviridae 906833049 genus Betacoronavirus 1015862491 species Severe acute respiratory syndrome-related coronavirus 216305222 strain Severe acute respiratory syndrome coronavirus 2 $ echo \"Escherichia coli\" | taxonkit name2taxid --data-dir taxdump.gtdb+ncbi/ Escherichia coli 1945799576 $ echo 1945799576 \\ | taxonkit lineage -t --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 3 \\ | csvtk unfold -Ht -f 1 -s \";\" \\ | taxonkit lineage -r -n -L --data-dir taxdump.gtdb+ncbi/ \\ | csvtk cut -Ht -f 1,3,2 \\ | csvtk pretty -Ht 609216830 superkingdom Bacteria 1641076285 phylum Proteobacteria 329474883 class Gammaproteobacteria 1012954932 order Enterobacterales 87250111 family Enterobacteriaceae 1187493883 genus Escherichia 1945799576 species Escherichia coli","title":"Merging GTDB and NCBI taxonomy"},{"location":"download/","text":"Download KMCP is implemented in Go programming language, statically-linked executable binary files are freely available . SIMD instructions support SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters. ARM architecture is supported, but kmcp search would be slower. Current Version v0.9.2 - 2023-05-16 kmcp profile/cos2simi/filter/index-info/merge-regions/query-fpr : rename/unify the long flag --out-prefix to --out-file . kmcp profile : fix the number of reads belonging to references in the profile when no matches are found, which should be 0 instead of 1. new command: kmcp utils index-density : plotting the element density of bloom filters for an index file. An audience was concerned about it, but the results showed the elements (1s) are uniformly distributed in all BFs. Links OS Arch File, \u4e2d\u56fd\u955c\u50cf Download Count Linux 64-bit kmcp_linux_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf Linux arm64 kmcp_linux_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS 64-bit kmcp_darwin_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS arm64 kmcp_darwin_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf Windows 64-bit kmcp_windows_amd64.exe.tar.gz , \u4e2d\u56fd\u955c\u50cf Notes: please open an issue to request binaries for other platforms or compile from the source . run kmcp version to check update !!! run kmcp autocompletion to update shell autocompletion script !!! Installation Method 1: Install using conda conda install -c bioconda kmcp Method 2: Download binaries Download the compressed executable file of your operating system, and decompress it with tar -zxvf *.tar.gz command or other tools. And then: For Linux-like systems If you have root privilege, simply copy it to /usr/local/bin : sudo cp kmcp /usr/local/bin/ Or copy to anywhere in the environment variable PATH : mkdir -p $HOME/bin/; cp kmcp $HOME/bin/ For Windows , just copy kmcp.exe to C:\\WINDOWS\\system32 . Method 3: Compile from source Install go wget https://go.dev/dl/go1.17.13.linux-amd64.tar.gz tar -zxf go1.17.13.linux-amd64.tar.gz -C $HOME/ # or # echo \"export PATH=$PATH:$HOME/go/bin\" >> ~/.bashrc # source ~/.bashrc export PATH=$PATH:$HOME/go/bin Compile KMCP # ------------- the latest stable version ------------- go get -v -u github.com/shenwei356/kmcp/kmcp # The executable binary file is located in: # ~/go/bin/kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ~/go/bin/kmcp $HOME/bin/ # --------------- the development version -------------- git clone https://github.com/shenwei356/kmcp cd kmcp/kmcp/ go build # The executable binary file is located in: # ./kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ./kmcp $HOME/bin/ Shell-completion Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish Release History v0.9.1 - 2022-12-26 kmcp search faster speed for ARM architectures. fix compilation for ARM architectures. v0.9.0 - 2022-09-28 compute : smaller output files and faster speed. more even genome splitting. index : faster speed due to smaller input files. search : more accurate and smaller query FPR following Theorem 2 in SBT paper, instead of the Chernoff bound . change the default value of -f/--max-fpr from 0.05 to 0.01. 10-20% speedup . profile : more accurate abundance estimation using EM algorithm . change the default value of -f/--max-fpr from 0.05 to 0.01. mode 0: change the default value of -H/--min-hic-ureads-qcov from 0.55 to 0.7. increase float width of reference coverage in KMCP profile format from 2 to 6. util query-fpr : compute query FPR following Theorem 2 in SBT paper, instead of the Chernoff bound. new commands: utils split-genomes for splitting genomes into chunks. utils ref-info for printing information of reference (chunks), including the number of k-mers and the actual false-positive rate. v0.8.3 - 2022-08-15 kmcp : fix compiling from source for ARM architectures. #17 search : fix searching with paired-end reads where the read2 is shorter than the value of --min-query-len . #10 fix the log. #8 a new flag -f/--max-fpr : maximum false positive rate of a query (default 0.05). It reduces the unnecessary output when searching with a low minimum query coverage ( -t/--min-query-cov ). profile : recommend using the flag --no-amb-corr to disable ambiguous reads correction when >= 1000 candidates are detected. fix logging when using --level strain and no taxonomy given. v0.8.2 - 2022-03-26 search : flag -g/--query-whole-file : fix panic for invalid input. add gaps of k-1 bp before concatatenating seqs. add warning for invalid input. profile : allow modifying parts of parameters in preset profiling modes . #5 decrease thresholds of minimum reads and unique reads in preset profiling modes 1 and 2 for low coverage sequence data. the profiling results generated with mode 3 in the manuscript are not affected . v0.8.1 - 2022-03-07 update help message, show common usages, add examples, add notes to important options. v0.8.0 - 2022-02-24 commands: new command utils cov2simi : Convert k-mer coverage to sequence similarity. new command utils query-fpr : Compute the maximum false positive rate of a query. compute : update doc. add flags compatibility check. search : output the false positive rate of each match, rather than the FPR upper bound of the query . this could save some short queries with high similarity. change default values of reads filter, because clinical data contain many short reads . -c/--min-uniq-reads : 30 -> 10 . -m/--min-query-len : 70 -> 30 . update doc. profile : rename flags: --keep-main-matches -> --keep-main-matches . --keep-perfect-match -> --keep-perfect-matches . change default values: --max-qcov-gap : 0.2 -> 0.4 . mode 0 (pathogen detection): switch on flag --keep-main-matches use --max-qcov-gap 0.4 update doc. v0.7.1 - 2022-02-08 profile : new flag --metaphlan-report-version and the default value is 3 . #4 column name renamed: from fragsFrac , fragsRelDepth , fragsRelDepthStd to chunksFrac , chunksRelDepth , chunksRelDepthStd . fix computation of chunksRelDepth . slightly improve sensitivity for -m 0 . v0.7.0 - 2022-01-24 commands: new command utils filter : Filter search results and find species-specific queries. new command utils merge-regions : Merge species/assembly-specific regions. rename info to utils index-info . compute : skip k-mer containing Ns. when splitting genome into fragments, sequences are concatenated with k-1 'N's instead of directly concatenation. It eliminates fake k-mers at the concatenation position. set default value for flag -N/--ref-name-regexp : (?i)(.+)\\.(f[aq](st[aq])?|fna)(.gz)?$ . fix a rare bug when splitting FASTQ files. search : support searching with paired-end reads which has a higher specificity and a lower sensitivity. A flag --try-se is added for search read1/read2 when the paired end reads have no hits. fix matches order of a query. fix queries with many Ns. change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-scores from 5 to 0 , i.e., keep all matches by default. new flag -w/--load-whole-db : load all index files into memory. 10-25% faster. better log. merge : fix adding up hits . fix bug of incorrect order, reduce memory usage. support one input file. profile : change analysis workflow, using 4 stages. output format change: new column coverage , fragsRelDepth and fragsRelDepthStd . change default file extension of binning file. check if the taxid of a target is given by taxid mapping file. automatically switch to the new taxid for a merged one. change computation of score . new flag -d/--max-frags-depth-stdev . new option -m/--mode . change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-qcovs from 5 to 0 (keep all matches). change default value of falg -f/--max-fpr from 0.01 to 0.05 . change default value of flag -H/--min-hic-ureads-qcov from 0.8 to 0.75 (similarity ~98% ). faster search result parsing. v0.6.0 - 2021-08-13 new command: merge : merge search results from multiple databases. compute : fix splitting very short genomes. remove flag -e/--exact-number , making it default. index : do not roundup sizes of indexes. The searching speed is not affected and even faster due to optimization of search command. use three k-mers thresholds to control index file size. better control of cocurrency number and better progress bar. do not support RAMBO index anymore. search : 1.37X speedup, and faster for database with two or more hash functions. new flag -S/--do-not-sort . profile : fix a nil pointer bug when no taxid mapping data given. fix number of ureads. new flag -m/--keep-main-matches and --max-score-gap v0.5.0 - 2021-06-24 compute : support multiple sizes of k-mer. fix bug of --by-seq . more log. index : default block size is computed by -j/--threads instead of number of CPUs. search : show real-time processing speed. new flag -g/--query-whole-file . new flag -u/--kmer-dedup-threshold . new flag -m/--min-query-len . increase speed for database with mulitple hashes. profile : better decision of the existence of a reference. new flag -B/--binning-result for output reads binning result. new flag -m/--norm-abund . v0.4.0 - 2021-04-08 new command: profile for generating taxonomic profile from search result. compute : new flag -B/--seq-name-filter for filtering out unwanted sequences like plasmid. new flag -N/--ref-name-regexp for extracting reference name from sequence file. search : change default threshold value. new flag -n/--keep-top-scores for keeping matches with the top N score. v0.3.0 - 2021-03-16 use --quiet to replace --verbose , making printing log info default. search : fix computing intersetion between repeats. fix closing mmap on Windows. change output format and add Jaccard Index. speedup by parallelizing name mapping and database closing. flush result immediately. keep the output order by default compute : change default file regexp for matching .fna files. autocompletion : support bash, zsh, fish, powershell. v0.2.1 - 2020-12-31 index : reduce memory occupation. v0.2.0 - 2020-12-30 Add support of RAMBO like indexing. Limit to only one input database. Change output format. v0.1.0 - 2020-xx-xx First release with basic function.","title":"Download"},{"location":"download/#download","text":"KMCP is implemented in Go programming language, statically-linked executable binary files are freely available .","title":"Download"},{"location":"download/#simd-instructions-support","text":"SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters. ARM architecture is supported, but kmcp search would be slower.","title":"SIMD instructions support"},{"location":"download/#current-version","text":"","title":"Current Version"},{"location":"download/#v092-2023-05-16","text":"kmcp profile/cos2simi/filter/index-info/merge-regions/query-fpr : rename/unify the long flag --out-prefix to --out-file . kmcp profile : fix the number of reads belonging to references in the profile when no matches are found, which should be 0 instead of 1. new command: kmcp utils index-density : plotting the element density of bloom filters for an index file. An audience was concerned about it, but the results showed the elements (1s) are uniformly distributed in all BFs.","title":"v0.9.2 - 2023-05-16"},{"location":"download/#links","text":"OS Arch File, \u4e2d\u56fd\u955c\u50cf Download Count Linux 64-bit kmcp_linux_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf Linux arm64 kmcp_linux_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS 64-bit kmcp_darwin_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS arm64 kmcp_darwin_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf Windows 64-bit kmcp_windows_amd64.exe.tar.gz , \u4e2d\u56fd\u955c\u50cf Notes: please open an issue to request binaries for other platforms or compile from the source . run kmcp version to check update !!! run kmcp autocompletion to update shell autocompletion script !!!","title":"Links"},{"location":"download/#installation","text":"","title":"Installation"},{"location":"download/#method-1-install-using-conda","text":"conda install -c bioconda kmcp","title":"Method 1: Install using conda"},{"location":"download/#method-2-download-binaries","text":"Download the compressed executable file of your operating system, and decompress it with tar -zxvf *.tar.gz command or other tools. And then: For Linux-like systems If you have root privilege, simply copy it to /usr/local/bin : sudo cp kmcp /usr/local/bin/ Or copy to anywhere in the environment variable PATH : mkdir -p $HOME/bin/; cp kmcp $HOME/bin/ For Windows , just copy kmcp.exe to C:\\WINDOWS\\system32 .","title":"Method 2: Download binaries"},{"location":"download/#method-3-compile-from-source","text":"Install go wget https://go.dev/dl/go1.17.13.linux-amd64.tar.gz tar -zxf go1.17.13.linux-amd64.tar.gz -C $HOME/ # or # echo \"export PATH=$PATH:$HOME/go/bin\" >> ~/.bashrc # source ~/.bashrc export PATH=$PATH:$HOME/go/bin Compile KMCP # ------------- the latest stable version ------------- go get -v -u github.com/shenwei356/kmcp/kmcp # The executable binary file is located in: # ~/go/bin/kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ~/go/bin/kmcp $HOME/bin/ # --------------- the development version -------------- git clone https://github.com/shenwei356/kmcp cd kmcp/kmcp/ go build # The executable binary file is located in: # ./kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ./kmcp $HOME/bin/","title":"Method 3: Compile from source"},{"location":"download/#shell-completion","text":"Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish","title":"Shell-completion"},{"location":"download/#release-history","text":"","title":"Release History"},{"location":"download/#v091-2022-12-26","text":"kmcp search faster speed for ARM architectures. fix compilation for ARM architectures.","title":"v0.9.1 - 2022-12-26"},{"location":"download/#v090-2022-09-28","text":"compute : smaller output files and faster speed. more even genome splitting. index : faster speed due to smaller input files. search : more accurate and smaller query FPR following Theorem 2 in SBT paper, instead of the Chernoff bound . change the default value of -f/--max-fpr from 0.05 to 0.01. 10-20% speedup . profile : more accurate abundance estimation using EM algorithm . change the default value of -f/--max-fpr from 0.05 to 0.01. mode 0: change the default value of -H/--min-hic-ureads-qcov from 0.55 to 0.7. increase float width of reference coverage in KMCP profile format from 2 to 6. util query-fpr : compute query FPR following Theorem 2 in SBT paper, instead of the Chernoff bound. new commands: utils split-genomes for splitting genomes into chunks. utils ref-info for printing information of reference (chunks), including the number of k-mers and the actual false-positive rate.","title":"v0.9.0 - 2022-09-28"},{"location":"download/#v083-2022-08-15","text":"kmcp : fix compiling from source for ARM architectures. #17 search : fix searching with paired-end reads where the read2 is shorter than the value of --min-query-len . #10 fix the log. #8 a new flag -f/--max-fpr : maximum false positive rate of a query (default 0.05). It reduces the unnecessary output when searching with a low minimum query coverage ( -t/--min-query-cov ). profile : recommend using the flag --no-amb-corr to disable ambiguous reads correction when >= 1000 candidates are detected. fix logging when using --level strain and no taxonomy given.","title":"v0.8.3 - 2022-08-15"},{"location":"download/#v082-2022-03-26","text":"search : flag -g/--query-whole-file : fix panic for invalid input. add gaps of k-1 bp before concatatenating seqs. add warning for invalid input. profile : allow modifying parts of parameters in preset profiling modes . #5 decrease thresholds of minimum reads and unique reads in preset profiling modes 1 and 2 for low coverage sequence data. the profiling results generated with mode 3 in the manuscript are not affected .","title":"v0.8.2 - 2022-03-26"},{"location":"download/#v081-2022-03-07","text":"update help message, show common usages, add examples, add notes to important options.","title":"v0.8.1 - 2022-03-07"},{"location":"download/#v080-2022-02-24","text":"commands: new command utils cov2simi : Convert k-mer coverage to sequence similarity. new command utils query-fpr : Compute the maximum false positive rate of a query. compute : update doc. add flags compatibility check. search : output the false positive rate of each match, rather than the FPR upper bound of the query . this could save some short queries with high similarity. change default values of reads filter, because clinical data contain many short reads . -c/--min-uniq-reads : 30 -> 10 . -m/--min-query-len : 70 -> 30 . update doc. profile : rename flags: --keep-main-matches -> --keep-main-matches . --keep-perfect-match -> --keep-perfect-matches . change default values: --max-qcov-gap : 0.2 -> 0.4 . mode 0 (pathogen detection): switch on flag --keep-main-matches use --max-qcov-gap 0.4 update doc.","title":"v0.8.0 - 2022-02-24"},{"location":"download/#v071-2022-02-08","text":"profile : new flag --metaphlan-report-version and the default value is 3 . #4 column name renamed: from fragsFrac , fragsRelDepth , fragsRelDepthStd to chunksFrac , chunksRelDepth , chunksRelDepthStd . fix computation of chunksRelDepth . slightly improve sensitivity for -m 0 .","title":"v0.7.1 - 2022-02-08"},{"location":"download/#v070-2022-01-24","text":"commands: new command utils filter : Filter search results and find species-specific queries. new command utils merge-regions : Merge species/assembly-specific regions. rename info to utils index-info . compute : skip k-mer containing Ns. when splitting genome into fragments, sequences are concatenated with k-1 'N's instead of directly concatenation. It eliminates fake k-mers at the concatenation position. set default value for flag -N/--ref-name-regexp : (?i)(.+)\\.(f[aq](st[aq])?|fna)(.gz)?$ . fix a rare bug when splitting FASTQ files. search : support searching with paired-end reads which has a higher specificity and a lower sensitivity. A flag --try-se is added for search read1/read2 when the paired end reads have no hits. fix matches order of a query. fix queries with many Ns. change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-scores from 5 to 0 , i.e., keep all matches by default. new flag -w/--load-whole-db : load all index files into memory. 10-25% faster. better log. merge : fix adding up hits . fix bug of incorrect order, reduce memory usage. support one input file. profile : change analysis workflow, using 4 stages. output format change: new column coverage , fragsRelDepth and fragsRelDepthStd . change default file extension of binning file. check if the taxid of a target is given by taxid mapping file. automatically switch to the new taxid for a merged one. change computation of score . new flag -d/--max-frags-depth-stdev . new option -m/--mode . change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-qcovs from 5 to 0 (keep all matches). change default value of falg -f/--max-fpr from 0.01 to 0.05 . change default value of flag -H/--min-hic-ureads-qcov from 0.8 to 0.75 (similarity ~98% ). faster search result parsing.","title":"v0.7.0 - 2022-01-24"},{"location":"download/#v060-2021-08-13","text":"new command: merge : merge search results from multiple databases. compute : fix splitting very short genomes. remove flag -e/--exact-number , making it default. index : do not roundup sizes of indexes. The searching speed is not affected and even faster due to optimization of search command. use three k-mers thresholds to control index file size. better control of cocurrency number and better progress bar. do not support RAMBO index anymore. search : 1.37X speedup, and faster for database with two or more hash functions. new flag -S/--do-not-sort . profile : fix a nil pointer bug when no taxid mapping data given. fix number of ureads. new flag -m/--keep-main-matches and --max-score-gap","title":"v0.6.0 - 2021-08-13"},{"location":"download/#v050-2021-06-24","text":"compute : support multiple sizes of k-mer. fix bug of --by-seq . more log. index : default block size is computed by -j/--threads instead of number of CPUs. search : show real-time processing speed. new flag -g/--query-whole-file . new flag -u/--kmer-dedup-threshold . new flag -m/--min-query-len . increase speed for database with mulitple hashes. profile : better decision of the existence of a reference. new flag -B/--binning-result for output reads binning result. new flag -m/--norm-abund .","title":"v0.5.0 - 2021-06-24"},{"location":"download/#v040-2021-04-08","text":"new command: profile for generating taxonomic profile from search result. compute : new flag -B/--seq-name-filter for filtering out unwanted sequences like plasmid. new flag -N/--ref-name-regexp for extracting reference name from sequence file. search : change default threshold value. new flag -n/--keep-top-scores for keeping matches with the top N score.","title":"v0.4.0 - 2021-04-08"},{"location":"download/#v030-2021-03-16","text":"use --quiet to replace --verbose , making printing log info default. search : fix computing intersetion between repeats. fix closing mmap on Windows. change output format and add Jaccard Index. speedup by parallelizing name mapping and database closing. flush result immediately. keep the output order by default compute : change default file regexp for matching .fna files. autocompletion : support bash, zsh, fish, powershell.","title":"v0.3.0 - 2021-03-16"},{"location":"download/#v021-2020-12-31","text":"index : reduce memory occupation.","title":"v0.2.1 - 2020-12-31"},{"location":"download/#v020-2020-12-30","text":"Add support of RAMBO like indexing. Limit to only one input database. Change output format.","title":"v0.2.0 - 2020-12-30"},{"location":"download/#v010-2020-xx-xx","text":"First release with basic function.","title":"v0.1.0 - 2020-xx-xx"},{"location":"faq/","text":"Frequently Asked Questions General How can I run KMCP on a computer without enough main memory? By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database. Database building Can I create a database with custom genome collections for profiling? Yes, you can use taxonkit create-taxdump to create NCBI-style taxdump files for profiling, which also generates a taxid.map file. What k-mer size should I use to build the database? Multiple k-mer sizes are supported, but one value is good enough. Bigger k-mer sizes bring high specificity at the cost of decrease of sensitivity. k = 21 is recommended for metagenomic profiling. How to add new genomes to the database? KMCP builds database very fast, you can either rebuilt the database after adding new genomes, or create a separate database with the new genomes, search against these databases, and merge the results. Unexpected EOF error Some files could corrupt during downloading, we recommend checking sequence file integrity using seqkit ( gzip -t failed for some files in my tests). List corrupted files # corrupted files find $genomes -name \"*.gz\" \\ | rush 'seqkit seq -w 0 {} > /dev/null; if [ $? -ne 0 ]; then echo {}; fi' \\ > failed.txt # empty files find $genomes -name \"*.gz\" -size 0 >> failed.txt Delete these files: cat failed.txt | rush '/bin/rm {}' Redownload these files. For example, from rush cache file download.rush (list of succeed commands), where the commands are in one line. grep -f failed.txt download.rush \\ | sed 's/__CMD__//g' \\ | rush '{}' For genome_updater , URLs of genomes can be found in files like 2021-09-30_13-32-30_url_downloaded.txt , you can extract URLs using grep -f failed.txt -v *url_downloaded.txt or some other ways, and batch redownload them using parallel . Are the elements in the bloom filters uniformly distributed? Someone asked me this when I was giving a talk on KMCP. The answer is yes. I created a new command kmcp utils index-density to plot the density of a index file. In the image, X: bins. The width is the number of bins Y: bloom filters, with each representing a genome (chunk). The height is the number of names (genome or genome chunks) greyscale/darkness of a pixel: the density of a bin, calculated as: 255 - 255 * ${the number of 1s in the bin} / ${bin-size} Here's are some example outputs. v2023.05-genbank-viral-_block001.uniki (FPR of bloom filter: 0.3), only a part of image is shown. Note that the genome sizes (base pairs) of this genome chunk vary widely, so the densities of the preceding bloom files are relatively spare. Information of k-mers: kmcp utils ref-info -d genbank-viral.kmcp \\ | csvtk grep -t -p _block001.uniki \\ | csvtk cut -t -f -2 \\ | csvtk head -n 10 \\ | csvtk csv2md -t file target chunkIdx chunks kmers fpr _block001.uniki GCA_008766775.1 0 1 145 0.022419 _block001.uniki GCA_003330005.1 8 10 150 0.023183 _block001.uniki GCA_002814895.1 0 1 154 0.023794 _block001.uniki GCA_001967255.1 9 10 165 0.025471 _block001.uniki GCA_002219745.1 4 10 197 0.030336 _block001.uniki GCA_000839085.1 0 1 200 0.030790 _block001.uniki GCA_000919055.1 9 10 207 0.031851 _block001.uniki GCA_000848385.1 6 10 207 0.031851 _block001.uniki GCA_002820565.1 9 10 208 0.032002 _block001.uniki GCA_002820505.1 9 10 208 0.032002 Information of bloom filters: kmcp utils index-info -b genbank-viral.kmcp/R001/_block001.uniki | csvtk csv2md -t file k canonical num-hashes num-sigs num-names _block001.uniki 21 true 1 6395 10400 v2023.05-refseq-fungi-_block001.uniki (FPR of bloom filter: 0.3) Information of k-mers: kmcp utils ref-info -d refseq-fungi.kmcp \\ | csvtk grep -t -p _block001.uniki \\ | csvtk cut -t -f -2 \\ | csvtk head -n 10 \\ | csvtk csv2md -t file target chunkIdx chunks kmers fpr _block001.uniki GCF_000277815.2 0 10 207084 0.094334 _block001.uniki GCF_000146465.1 0 10 209422 0.095346 _block001.uniki GCF_000091225.2 0 10 209787 0.095504 _block001.uniki GCF_000280035.1 8 10 216972 0.098608 _block001.uniki GCF_000280035.1 9 10 217529 0.098849 _block001.uniki GCF_000280035.1 3 10 217631 0.098892 _block001.uniki GCF_000280035.1 6 10 218560 0.099293 _block001.uniki GCF_000280035.1 7 10 218763 0.099380 _block001.uniki GCF_000280035.1 4 10 218830 0.099409 _block001.uniki GCF_000280035.1 0 10 218860 0.099422 Information of bloom filters: kmcp utils index-info -b refseq-fungi.kmcp/R001/_block001.uniki | csvtk csv2md -t file k canonical num-hashes num-sigs num-names _block001.uniki 21 true 1 2089979 160 v2023.05-refseq-fungi-_block002.uniki (FPR of bloom filter: 0.3) file target chunkIdx chunks kmers fpr _block002.uniki GCF_000349005.2 5 10 745638 0.249355 _block002.uniki GCF_000349005.2 2 10 746237 0.249528 _block002.uniki GCF_000349005.2 9 10 747586 0.249917 _block002.uniki GCF_000349005.2 0 10 748331 0.250132 _block002.uniki GCF_000349005.2 7 10 749497 0.250469 _block002.uniki GCF_001477545.1 5 10 751685 0.251099 _block002.uniki GCF_002251995.1 3 10 753439 0.251604 _block002.uniki GCF_001477545.1 4 10 755317 0.252145 _block002.uniki GCF_001477545.1 0 10 758418 0.253036 _block002.uniki GCF_001477545.1 8 10 760460 0.253623 file k canonical num-hashes num-sigs num-names _block002.uniki 21 true 1 2599648 160 Searching Why are the CPU usages are very low, not 100%? Please check the log (in terminal not the log file via the option --log ), it should report current searching speed, like: 21:07:42.949 [INFO] reading sequence file: t_t3.fa.gz processed queries: 1253376, speed: 8.127 million queries per minute If the speed is very slow. Are you running KMCP in a computer cluster (HPC)? If yes, please switch on -w/--load-whole-db . Because the default database loading mode would be very slow for network-attached storage (NAS). Are the reference genomes are highly similar? E.g., tens of thousands of genomes of a same species? If yes, check the search result to see if there are thousands of matches for a read. You may choose another graph-based sequence searching tool. Are you searching with metatranscriptomics data? If yes, the search results would show that a huge number of reads from 16 rRNA genes have thousands of matches, therefore, writing results slow down the search. So these reads should be filtered out before the search using tools like https://github.com/hzi-bifo/RiboDetector. Can I run multiple KMCP processes in a machine? Yes you can. But note that KMCP search is CPU- and RAM-intense. So please to limit the number of CPUs cores to use for each process with the falg -j/--threads , of which the default value is the available CPUs cores of the machine. Profiling Where is the taxid.map? Each prebuilt database contains a taxid.map file in its directory. You can concatenate them into a big one: $ cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map $ head -n 5 taxid.map GCA_004025655.1 10243 GCA_004025395.1 10243 GCA_004025355.1 10243 GCA_003971405.1 10243 GCA_003971385.1 10243 Or set the options -T/--taxid-map multiple times: kmcp profile -T gtdb.kmcp/taxid.map -T refseq-viral.kmcp/taxid.map -T refseq-fungi.kmcp/taxid.map ... For other custom genome collections, you can use taxonkit create-taxdump to create NCBI-style taxdump files for custom taxonomy, e.g., GTDB and ICTV , which also generates a taxid.map file. Unknown taxid? 19:54:54.632 [ERRO] unknown taxid for NZ_CP028116.1, please check taxid mapping file(s) If the kmcp profile reports this, you may need to check if the taxid mapping file contain all the reference IDs. And make sure the reference IDs match these in the database, the later ones are listed in: $ head -n 5 $kmcp_db_dir/R001/__name_mapping.tsv NC_013654.1 NC_013654.1 NC_000913.3 NC_000913.3 NC_010655.1 NC_010655.1 NC_012971.2 NC_012971.2 NC_011750.1 NC_011750.1 There's another case: you used --name-map in kmcp search . Please don't do this if you will use the search result for metagenomic profiling which needs the original reference IDs. We add a note now: -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs. How to tune parts of options when using preset profiling modes? Sorry it's not supported due to the limitation of the command-line argument parsers. You need explicitly set all relevant options of the mode. It's available since v0.8.2.","title":"FAQs"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#general","text":"","title":"General"},{"location":"faq/#how-can-i-run-kmcp-on-a-computer-without-enough-main-memory","text":"By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database.","title":"How can I run KMCP on a computer without enough main memory?"},{"location":"faq/#database-building","text":"","title":"Database building"},{"location":"faq/#can-i-create-a-database-with-custom-genome-collections-for-profiling","text":"Yes, you can use taxonkit create-taxdump to create NCBI-style taxdump files for profiling, which also generates a taxid.map file.","title":"Can I create a database with custom genome collections for profiling?"},{"location":"faq/#what-k-mer-size-should-i-use-to-build-the-database","text":"Multiple k-mer sizes are supported, but one value is good enough. Bigger k-mer sizes bring high specificity at the cost of decrease of sensitivity. k = 21 is recommended for metagenomic profiling.","title":"What k-mer size should I use to build the database?"},{"location":"faq/#how-to-add-new-genomes-to-the-database","text":"KMCP builds database very fast, you can either rebuilt the database after adding new genomes, or create a separate database with the new genomes, search against these databases, and merge the results.","title":"How to add new genomes to the database?"},{"location":"faq/#unexpected-eof-error","text":"Some files could corrupt during downloading, we recommend checking sequence file integrity using seqkit ( gzip -t failed for some files in my tests). List corrupted files # corrupted files find $genomes -name \"*.gz\" \\ | rush 'seqkit seq -w 0 {} > /dev/null; if [ $? -ne 0 ]; then echo {}; fi' \\ > failed.txt # empty files find $genomes -name \"*.gz\" -size 0 >> failed.txt Delete these files: cat failed.txt | rush '/bin/rm {}' Redownload these files. For example, from rush cache file download.rush (list of succeed commands), where the commands are in one line. grep -f failed.txt download.rush \\ | sed 's/__CMD__//g' \\ | rush '{}' For genome_updater , URLs of genomes can be found in files like 2021-09-30_13-32-30_url_downloaded.txt , you can extract URLs using grep -f failed.txt -v *url_downloaded.txt or some other ways, and batch redownload them using parallel .","title":"Unexpected EOF error"},{"location":"faq/#are-the-elements-in-the-bloom-filters-uniformly-distributed","text":"Someone asked me this when I was giving a talk on KMCP. The answer is yes. I created a new command kmcp utils index-density to plot the density of a index file. In the image, X: bins. The width is the number of bins Y: bloom filters, with each representing a genome (chunk). The height is the number of names (genome or genome chunks) greyscale/darkness of a pixel: the density of a bin, calculated as: 255 - 255 * ${the number of 1s in the bin} / ${bin-size} Here's are some example outputs. v2023.05-genbank-viral-_block001.uniki (FPR of bloom filter: 0.3), only a part of image is shown. Note that the genome sizes (base pairs) of this genome chunk vary widely, so the densities of the preceding bloom files are relatively spare. Information of k-mers: kmcp utils ref-info -d genbank-viral.kmcp \\ | csvtk grep -t -p _block001.uniki \\ | csvtk cut -t -f -2 \\ | csvtk head -n 10 \\ | csvtk csv2md -t file target chunkIdx chunks kmers fpr _block001.uniki GCA_008766775.1 0 1 145 0.022419 _block001.uniki GCA_003330005.1 8 10 150 0.023183 _block001.uniki GCA_002814895.1 0 1 154 0.023794 _block001.uniki GCA_001967255.1 9 10 165 0.025471 _block001.uniki GCA_002219745.1 4 10 197 0.030336 _block001.uniki GCA_000839085.1 0 1 200 0.030790 _block001.uniki GCA_000919055.1 9 10 207 0.031851 _block001.uniki GCA_000848385.1 6 10 207 0.031851 _block001.uniki GCA_002820565.1 9 10 208 0.032002 _block001.uniki GCA_002820505.1 9 10 208 0.032002 Information of bloom filters: kmcp utils index-info -b genbank-viral.kmcp/R001/_block001.uniki | csvtk csv2md -t file k canonical num-hashes num-sigs num-names _block001.uniki 21 true 1 6395 10400 v2023.05-refseq-fungi-_block001.uniki (FPR of bloom filter: 0.3) Information of k-mers: kmcp utils ref-info -d refseq-fungi.kmcp \\ | csvtk grep -t -p _block001.uniki \\ | csvtk cut -t -f -2 \\ | csvtk head -n 10 \\ | csvtk csv2md -t file target chunkIdx chunks kmers fpr _block001.uniki GCF_000277815.2 0 10 207084 0.094334 _block001.uniki GCF_000146465.1 0 10 209422 0.095346 _block001.uniki GCF_000091225.2 0 10 209787 0.095504 _block001.uniki GCF_000280035.1 8 10 216972 0.098608 _block001.uniki GCF_000280035.1 9 10 217529 0.098849 _block001.uniki GCF_000280035.1 3 10 217631 0.098892 _block001.uniki GCF_000280035.1 6 10 218560 0.099293 _block001.uniki GCF_000280035.1 7 10 218763 0.099380 _block001.uniki GCF_000280035.1 4 10 218830 0.099409 _block001.uniki GCF_000280035.1 0 10 218860 0.099422 Information of bloom filters: kmcp utils index-info -b refseq-fungi.kmcp/R001/_block001.uniki | csvtk csv2md -t file k canonical num-hashes num-sigs num-names _block001.uniki 21 true 1 2089979 160 v2023.05-refseq-fungi-_block002.uniki (FPR of bloom filter: 0.3) file target chunkIdx chunks kmers fpr _block002.uniki GCF_000349005.2 5 10 745638 0.249355 _block002.uniki GCF_000349005.2 2 10 746237 0.249528 _block002.uniki GCF_000349005.2 9 10 747586 0.249917 _block002.uniki GCF_000349005.2 0 10 748331 0.250132 _block002.uniki GCF_000349005.2 7 10 749497 0.250469 _block002.uniki GCF_001477545.1 5 10 751685 0.251099 _block002.uniki GCF_002251995.1 3 10 753439 0.251604 _block002.uniki GCF_001477545.1 4 10 755317 0.252145 _block002.uniki GCF_001477545.1 0 10 758418 0.253036 _block002.uniki GCF_001477545.1 8 10 760460 0.253623 file k canonical num-hashes num-sigs num-names _block002.uniki 21 true 1 2599648 160","title":"Are the elements in the bloom filters uniformly distributed?"},{"location":"faq/#searching","text":"","title":"Searching"},{"location":"faq/#why-are-the-cpu-usages-are-very-low-not-100","text":"Please check the log (in terminal not the log file via the option --log ), it should report current searching speed, like: 21:07:42.949 [INFO] reading sequence file: t_t3.fa.gz processed queries: 1253376, speed: 8.127 million queries per minute If the speed is very slow. Are you running KMCP in a computer cluster (HPC)? If yes, please switch on -w/--load-whole-db . Because the default database loading mode would be very slow for network-attached storage (NAS). Are the reference genomes are highly similar? E.g., tens of thousands of genomes of a same species? If yes, check the search result to see if there are thousands of matches for a read. You may choose another graph-based sequence searching tool. Are you searching with metatranscriptomics data? If yes, the search results would show that a huge number of reads from 16 rRNA genes have thousands of matches, therefore, writing results slow down the search. So these reads should be filtered out before the search using tools like https://github.com/hzi-bifo/RiboDetector.","title":"Why are the CPU usages are very low, not 100%?"},{"location":"faq/#can-i-run-multiple-kmcp-processes-in-a-machine","text":"Yes you can. But note that KMCP search is CPU- and RAM-intense. So please to limit the number of CPUs cores to use for each process with the falg -j/--threads , of which the default value is the available CPUs cores of the machine.","title":"Can I run multiple KMCP processes in a machine?"},{"location":"faq/#profiling","text":"","title":"Profiling"},{"location":"faq/#where-is-the-taxidmap","text":"Each prebuilt database contains a taxid.map file in its directory. You can concatenate them into a big one: $ cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map $ head -n 5 taxid.map GCA_004025655.1 10243 GCA_004025395.1 10243 GCA_004025355.1 10243 GCA_003971405.1 10243 GCA_003971385.1 10243 Or set the options -T/--taxid-map multiple times: kmcp profile -T gtdb.kmcp/taxid.map -T refseq-viral.kmcp/taxid.map -T refseq-fungi.kmcp/taxid.map ... For other custom genome collections, you can use taxonkit create-taxdump to create NCBI-style taxdump files for custom taxonomy, e.g., GTDB and ICTV , which also generates a taxid.map file.","title":"Where is the taxid.map?"},{"location":"faq/#unknown-taxid","text":"19:54:54.632 [ERRO] unknown taxid for NZ_CP028116.1, please check taxid mapping file(s) If the kmcp profile reports this, you may need to check if the taxid mapping file contain all the reference IDs. And make sure the reference IDs match these in the database, the later ones are listed in: $ head -n 5 $kmcp_db_dir/R001/__name_mapping.tsv NC_013654.1 NC_013654.1 NC_000913.3 NC_000913.3 NC_010655.1 NC_010655.1 NC_012971.2 NC_012971.2 NC_011750.1 NC_011750.1 There's another case: you used --name-map in kmcp search . Please don't do this if you will use the search result for metagenomic profiling which needs the original reference IDs. We add a note now: -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs.","title":"Unknown taxid?"},{"location":"faq/#how-to-tune-parts-of-options-when-using-preset-profiling-modes","text":"Sorry it's not supported due to the limitation of the command-line argument parsers. You need explicitly set all relevant options of the mode. It's available since v0.8.2.","title":"How to tune parts of options when using preset profiling modes?"},{"location":"usage/","text":"Usage KMCP is a command-line tool consisting of several subcommands. Subcommand Function compute Generate k-mers (sketch) from FASTA/Q sequences index Construct adatabase from k-mer files search Search sequences against a database merge Merge search results from multiple databases profile Generate the taxonomic profile from search results utils split-genomes Split genomes into chunks utils unik-info Print information of .unik files utils index-info Print information of index files utils index-density Plot the element density of bloom filters for an index file utils ref-info Print information of reference chunks in a database utils cov2simi Convert k-mer coverage to sequence similarity utils query-fpr Compute the false positive rate of a query utils filter Filter search results and find species/assembly-specific queries utils merge-regions Merge species/assembly-specific regions kmcp Program: kmcp (K-mer-based Metagenomic Classification and Profiling) Version: v0.9.2 Documents: https://bioinf.shenwei.me/kmcp Source code: https://github.com/shenwei356/kmcp KMCP is a tool for metagenomic classification and profiling. KMCP can also be used for: 1. Fast sequence search against large scales of genomic datasets as BIGSI and COBS do. 2. Fast assembly/genome similarity estimation as Mash and sourmash do, by utilizing Minimizer, FracMinHash (Scaled MinHash), or Closed Syncmers. Usage: kmcp [command] Available Commands: autocompletion Generate shell autocompletion script compute Generate k-mers (sketches) from FASTA/Q sequences index Construct a database from k-mer files merge Merge search results from multiple databases profile Generate the taxonomic profile from search results search Search sequences against a database utils Some utilities version Print version information and check for update Flags: -h, --help help for kmcp -i, --infile-list string \u25ba File of input files list (one file per line). If given, they are appended to files from CLI arguments. --log string \u25ba Log file. -q, --quiet \u25ba Do not print any verbose information. But you can write them to file with --log. -j, --threads int \u25ba Number of CPUs cores to use. (default 16) Use \"kmcp [command] --help\" for more information about a command. compute Generate k-mers (sketches) from FASTA/Q sequences Input: 1. Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, 2. Or a directory containing sequence files via the flag -I/--in-dir, with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp. *3. For taxonomic profiling, the sequences of each reference genome should be saved in a separate file, with the reference identifier in the file name. Attention: You may rename the sequence files for convenience because the sequence/genome identifier in the index and search results would be: 1). For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. 2). For splitting sequence mode (see details below): same to 1). 3). For computing k-mers for each sequence: the sequence identifier. Attentions: 1. Unwanted sequences like plasmid can be filtered out by the name via regular expressions (-B/--seq-name-filter). 2. By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. 3. It also supports splitting sequences into chunks, this could increase the specificity in search results at the cost of a slower searching speed. 4. Multiple sizes of k-mers are supported. Supported k-mer (sketches) types: 1. K-mer: 1). ntHash of k-mer (-k) 2. K-mer sketchs (all using ntHash): 1). FracMinHash (-k -D), previously named Scaled MinHash 2). Minimizer (-k -W), optionally scaling/down-sampling (-D) 3). Closed Syncmer (-k -S), optionally scaling/down-sampling (-D) Splitting sequences: 1. Sequences can be splitted into chunks by a chunk size (-s/--split-size) or number of chunks (-n/--split-number) with overlap (-l/--split-overlap). In this mode, the sequences of each genome should be saved in an individual file. 2. When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter) in a sequence file are concatenated with k-1 N's before splitting. 3. Both sequence IDs and chunks indices are saved for later use, in form of meta/description data in .unik files. Metadata: 1. Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. 2. When parsing whole sequence files or splitting by the number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp, e.g., \"^(\\w{3}_\\d{9}\\.\\d+)\" for RefSeq records. Output: 1. All outputted .unik files are saved in ${outdir}, with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree '/xxx/yyy/zzz/' is built for > 1000 output files. 2. For splitting sequence mode (--split-size > 0 or --split-number > 0), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik 3. A summary file (\"${outdir}/_info.txt\") is generated for later use. Users need to check if the reference IDs (column \"name\") are what supposed to be. Performance tips: 1. Decrease the value of -j/--threads for data in hard disk drives to reduce I/O pressure. Next step: 1. Check the summary file (${outdir}/_info.txt) to see if the reference IDs (column \"name\") are what supposed to be. 2. Run \"kmcp index\" with the output directory. Examples: 1. From few sequence files: kmcp compute -k 21 -n 10 -l 150 -O tmp-k21-n10-l150 NC_045512.2.fna.gz 2. From a list file: kmcp compute -k 21 -n 10 -l 150 -O tmp-k21-210-l150 -i list.txt 3. From a directory containing many sequence files: kmcp compute -k 21 -n 10 -l 150 -B plasmid \\ -O gtdb-k21-n10-l150 -I gtdb-genomes/ Usage: kmcp compute [flags] [-k <k>] [-n <chunks>] [-l <overlap>] {[-I <seqs dir>] | <seq files>} -O <out dir> Flags: --by-seq \u25ba Compute k-mers (sketches) for each sequence, instead of the whole file. --circular \u25ba Input sequences are circular. -c, --compress \u25ba Output gzipped .unik files, it's slower and can save little space. -r, --file-regexp string \u25ba Regular expression for matching sequence files in -I/--in-dir, case ignored. (default \"\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") --force \u25ba Overwrite existed output directory. -h, --help help for compute -I, --in-dir string \u25ba Directory containing FASTA/Q files. Directory symlinks are followed. -k, --kmer ints \u25ba K-mer size(s). (default [21]) -W, --minimizer-w int \u25ba Minimizer window size. -O, --out-dir string \u25ba Output directory. -N, --ref-name-regexp string \u25ba Regular expression (must contains \"(\" and \")\") for extracting reference name from filename. (default \"(?i)(.+)\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") -D, --scale int \u25ba Scale of the FracMinHash (Scaled MinHash), or down-sample factor for Syncmers and Minimizer. (default 1) -B, --seq-name-filter strings \u25ba List of regular expressions for filtering out sequences by header/name, case ignored. -m, --split-min-ref int \u25ba Only splitting sequences >= X bp. (default 1000) -n, --split-number int \u25ba Chunk number for splitting sequences, incompatible with -s/--split-size. -l, --split-overlap int \u25ba Chunk overlap for splitting sequences. The default value will be set to k-1 unless you change it. -s, --split-size int \u25ba Chunk size for splitting sequences, incompatible with -n/--split-number. -S, --syncmer-s int \u25ba Length of the s-mer in Closed Syncmers. index Construct a database from k-mer files We build the index for k-mers (sketches) with a modified compact bit-sliced signature index (COBS). We totally rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed. Input: The output directory generated by \"kmcp compute\". Database size and searching accuracy: 0. Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size. 1. -f/--false-positive-rate: the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. 2. -n/--num-hash: large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. 3. The value of block size -b/--block-size better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 4. Use flag -x/--block-sizeX-kmers-t, -8/--block-size8-kmers-t, and -1/--block-size1-kmers-t to separately create indexes for inputs with a huge number of k-mers, for precise control of database size. References: 1. COBS: https://arxiv.org/abs/1905.09624 Taxonomy data: 1. No taxonomy data are included in the database. 2. Taxonomy information are only needed in \"profile\" command. Performance tips: 1. The number of blocks (.uniki files) is better be smaller than or equal to the number of CPU cores for faster searching speed. We can set the flag -j/--threads to control the blocks number. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. 2. #threads files are simultaneously opened, and the max number of opened files is limited by the flag -F/--max-open-files. You may use a small value of -F/--max-open-files for hard disk drive storages. 3. When the database is used in a new computer with more CPU cores, 'kmcp search' could automatically scale to utilize as many cores as possible. Next step: 1. Use 'kmcp search' for searching. 2. Use 'kmcp utils ref-info' to check the number of k-mers and FPR of each genome chunk. Examples: 1. For bacterial genomes: kmcp index -f 0.3 -n 1 -j 32 -I gtdb-k21-n10-l150/ -O gtdb.kmcp 2. For viruses, use -x and -8 to control index size of the largest chunks: kmcp index -f 0.05 -n 1 -j 32 -x 100K -8 1M \\ -I genbank-viral-k21-n10-l150/ -O genbank-viral.kmcp Usage: kmcp index [flags] [-f <fpr>] [-n <hashes>] [-j <blocks>] -I <compute output> -O <kmcp db> Flags: -a, --alias string \u25ba Database alias/name. (default: basename of --out-dir). You can also manually edit it in info file: ${outdir}/__db.yml. -b, --block-size int \u25ba Block size, better be multiple of 64 for large number of input files. (default: min(#.files/#theads, 8)) -1, --block-size1-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, an individual index is created for this file. Supported units: K, M, G. (default \"200M\") -8, --block-size8-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to 8. Supported units: K, M, G. (default \"20M\") -X, --block-sizeX int \u25ba If k-mers of single .unik file exceeds --block-sizeX-kmers-t, block size is changed to this value. (default 256) -x, --block-sizeX-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to --block-sizeX. Supported units: K, M, G. (default \"10M\") --dry-run \u25ba Dry run, useful for adjusting parameters (highly recommended). -f, --false-positive-rate float \u25ba False positive rate of the bloom filters, range: (0, 1). (default 0.3) --file-regexp string \u25ba Regular expression for matching files in -I/--in-dir, case ignored. (default \".unik$\") --force \u25ba Overwrite existed output directory. -h, --help help for index -I, --in-dir string \u25ba Directory containing .unik files. Directory symlinks are followed. -F, --max-open-files int \u25ba Maximum number of opened files, please use a small value for hard disk drive storage. (default 256) -n, --num-hash int \u25ba Number of hash functions in bloom filters. (default 1) -O, --out-dir string \u25ba Output directory. (default: ${indir}.kmcp-db) search Search sequences against a database Attentions: 1. Input format should be (gzipped) FASTA or FASTQ from files or stdin. - Paired-end files should be given via -1/--read1 and -2/--read2. kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz - Single-end can be given as positional arguments or -1/-2. kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz **Single-end mode is recommended for paired-end reads, for higher sensitivity**. 2. A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold to remove duplicates. 3. For long reads or contigs, you should split them into short reads using \"seqkit sliding\", e.g., seqkit sliding -s 100 -W 300 Shared flags between \"search\" and \"profile\": 1. -t/--min-query-cov 2. -f/--max-fpr Index files loading modes: 1. Using memory-mapped index files with mmap (default): - Faster startup speed when index files are buffered in memory. - Multiple KMCP processes can share the memory. 2. Loading the whole index files into memory (-w/--load-whole-db): - This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. - It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases. - **Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS)**. 3. Low memory mode (--low-mem): - Do not load all index files into memory nor use mmap, using file seeking. - It's much slower, >4X slower on SSD and would be much slower on HDD disks. - Only use this mode for small number of queries or a huge database that can't be loaded into memory. Output format: Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging The values of tCov and jacc in results only apply to databases built with a single size of k-mer. Performance tips: 1. Increase the value of -j/--threads for acceleratation, but values larger than the number of CPU cores won't bring extra speedup. 2. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. Examples: 1. Single-end mode (recommended) kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz sample_1_unpaired.fq.gz sample_2_unpaired.fq.gz 2. Paired-end mode kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ -1 sample_1.fq.gz -2 sample_2.fq.gz 3. In computer clusters, where databases are saved in NAS storages. kmcp search -w -d gtdb.n16-00.kmcp -o sample.kmcp@gtdb.n16-00.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz Usage: kmcp search [flags] [-w] -d <kmcp db> [-t <min-query-cov>] [read1.fq.gz] [read2.fq.gz] [unpaired.fq.gz] [-o read.tsv.gz] Flags: -d, --db-dir string \u25ba Database directory created by \"kmcp index\". Please add -w/--load-whole-db for databases on network-attached storages (NAS), e.g., a computer cluster environment. -D, --default-name-map \u25ba Load ${db}/__name_mapping.tsv for mapping name first. -S, --do-not-sort \u25ba Do not sort matches of a query. -h, --help help for search -n, --keep-top-scores int \u25ba Keep matches with the top N scores for a query, 0 for all. -K, --keep-unmatched \u25ba Keep unmatched query sequence information. -u, --kmer-dedup-threshold int \u25ba Remove duplicated kmers for a query with >= X k-mers. (default 256) -w, --load-whole-db \u25ba Load all index files into memory, it's faster for small databases but needs more memory. Use this for databases on network-attached storages (NAS). Please read \"Index files loading modes\" in \"kmcp search -h\". --low-mem \u25ba Do not load all index files into memory nor use mmap, the searching would be very very slow for a large number of queries. Please read \"Index files loading modes\" in \"kmcp search -h\". -f, --max-fpr float \u25ba Maximum false positive rate of a query. (default 0.01) -c, --min-kmers int \u25ba Minimum number of matched k-mers (sketches). (default 10) -t, --min-query-cov float \u25ba Minimum query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. (default 0.55) -m, --min-query-len int \u25ba Minimum query length. (default 30) -T, --min-target-cov float \u25ba Minimum target coverage, i.e., proportion of matched k-mers and unique k-mers of a target. -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs. -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") --query-id string \u25ba Custom query Id when using the whole file as a query. -g, --query-whole-file \u25ba Use the whole file as a query, e.g., for genome similarity estimation against k-mer sketch database. -1, --read1 string \u25ba (Gzipped) read1 file. -2, --read2 string \u25ba (Gzipped) read2 file. -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\") --try-se \u25ba If paired-end reads have no hits, re-search with read1, if still fails, try read2. -G, --use-filename \u25ba Use file name as query ID when using the whole file as a query. merge Merge search results from multiple databases Input: *. Searching results of the same reads in different databases. *. The order of multiple input reads files should be the same during searching. *. When only one input given, we just copy and write to the input file. This is friendly to workflows which assume multiple inputs are given. Example: kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz Usage: kmcp merge [flags] [-o read.tsv.gz] [<search results> ...] Flags: -n, --field-hits int \u25ba Field of hits. (default 5) -f, --field-queryIdx int \u25ba Field of queryIdx. (default 15) -h, --help help for merge -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\") profile Generate the taxonomic profile from search results Methods: 1. Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimum proportion of matched chunks (-p/--min-chunks-fraction). (***highly recommended***) Another flag -d/--max-chunks-depth-stdev further reduces false positives. 2. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. 3. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positives of ambiguous matches. You can also disable this step by the flag --no-amb-corr. If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. 4. Abundance are estimated using an Expectation-Maximization (EM) algorithm. 5. Input files are parsed for multiple times, therefore STDIN is not supported. Reference: 1. MegaPath: https://doi.org/10.1186/s12864-020-06875-6 Accuracy notes: *. Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate (-f/--max-fpr) of a query. *. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 *. -U/--min-hic-ureads, minimum number, >= 1 *. -H/--min-hic-ureads-qcov, minimum query coverage, >= -t/--min-qcov *. -P/--min-hic-ureads-prop, minimum proportion, higher values increase precision at the cost of sensitivity. *. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambiguous reads with the algorithm in MegaPath. *. --keep-perfect-matches is not recommended, which decreases sensitivity. *. --keep-main-matches is not recommended, which affects accuracy of abundance estimation. *. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation. Profiling modes: We preset six profiling modes, available with the flag -m/--mode: - 0 (for pathogen detection) - 1 (higher recall) - 2 (high recall) - 3 (default) - 4 (high precision) - 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.7 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use \"taxonkit create-taxdump\" (https://github.com/shenwei356/taxonkit) to create NCBI-style taxdump files, which also generates a TaxId mapping file. Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. *3. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results. Profiling output formats: 1. KMCP (-o/--out-file) Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please also output CAMI or MetaPhlAn format. 2. CAMI (-M/--metaphlan-report, --metaphlan-report-version, -s/--sample-id, --taxonomy-id) Related tools (https://github.com/shenwei356/taxonkit): - taxonkit profile2cami: convert any metagenomic profile table with TaxIds to CAMI format. Use this if you forget to output CAMI format. - taxonkit cami-filter: remove taxa of given TaxIds and their descendants in a CAMI metagenomic profile. 3. MetaPhlAn (-C/--cami-report, -s/--sample-id) KMCP format: Tab-delimited format with 17 columns: 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. coverage, Average coverage of the reference 4. score, The 90th percentile of qCov of uniquely matched reads 5. chunksFrac, Genome chunks fraction 6. chunksRelDepth, Relative depths of reference chunks 7. chunksRelDepthStd, The standard deviation of chunksRelDepth 8. reads, Total number of matched reads of this reference 9. ureads, Number of uniquely matched reads 10. hicureads, Number of uniquely matched reads with high-confidence 11. refsize, Reference size 12. refname, Reference name, optional via name mapping file 13. taxid, TaxId of the reference 14. rank, Taxonomic rank 15. taxname, Taxonomic name 16. taxpath, Complete lineage 17. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Taxonomic binning formats: 1. CAMI (-B/--binning-result) Examples: 1. Default mode: kmcp profile -X taxdump/ -T taxid.map -m 3 \\ sample.kmcp.tsv.gz -o sample.k.profile \\ -C sample.c.profile -s sample 2. For pathogen detection (you may create databases with lower FPRs, e.g., kmcp index -f 0.1 -n 2 for bacteria and fungi genomes, and search with a low k-mer coverage threshold -t 0.4): kmcp profile -X taxdump/ -T taxid.map -m 3 -t 0.4 \\ sample.kmcp.tsv.gz -o sample.k.profile Usage: kmcp profile [flags] [-X <taxdump dir>] [-T <taxid.map>] [-m <mode>] [-o <kmcp profile>] <search results> Flags: -I, --abund-max-iters int \u25ba Miximal iteration of abundance estimation. (default 10) --abund-pct-threshold float \u25ba If the percentage change of the predominant target is smaller than this threshold, stop the iteration. (default 0.01) -B, --binning-result string \u25ba Save extra binning result in CAMI report. -C, --cami-report string \u25ba Save extra CAMI-like report. --debug string \u25ba Debug output file. -F, --filter-low-pct float \u25ba Filter out predictions with the smallest relative abundances summing up X%. Range: [0,100). -h, --help help for profile --keep-main-matches \u25ba Only keep main matches, abandon matches with sharply decreased qcov (> --max-qcov-gap). --keep-perfect-matches \u25ba Only keep the perfect matches (qcov == 1) if there are. -n, --keep-top-qcovs int \u25ba Keep matches with the top N qcovs for a query, 0 for all. --level string \u25ba Level to estimate abundance at. Available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp profile -h\" for details. (default 5000) -d, --max-chunks-depth-stdev float \u25ba Maximum standard deviation of relative depths of all chunks. (default 2) -f, --max-fpr float \u25ba Maximum false positive rate of a read in search result. (default 0.01) -R, --max-mismatch-err float \u25ba Maximum error rate of a read being matched to a wrong reference, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) --max-qcov-gap float \u25ba Max qcov gap between adjacent matches. (default 0.4) -M, --metaphlan-report string \u25ba Save extra metaphlan-like report. --metaphlan-report-version string \u25ba Metaphlan report version (2 or 3) (default \"3\") -p, --min-chunks-fraction float \u25ba Minimum fraction of matched reference chunks with reads >= -r/--min-chunks-reads. (default 0.8) -r, --min-chunks-reads int \u25ba Minimum number of reads for a reference chunk. (default 50) -D, --min-dreads-prop float \u25ba Minimum proportion of distinct reads, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) -U, --min-hic-ureads int \u25ba Minimum number of high-confidence uniquely matched reads for a reference. (default 5) -P, --min-hic-ureads-prop float \u25ba Minimum proportion of high-confidence uniquely matched reads. (default 0.1) -H, --min-hic-ureads-qcov float \u25ba Minimum query coverage of high-confidence uniquely matched reads. (default 0.75) -t, --min-query-cov float \u25ba Minimum query coverage of a read in search result. (default 0.55) -u, --min-uniq-reads int \u25ba Minimum number of uniquely matched reads for a reference. (default 20) -m, --mode int \u25ba Profiling mode, type \"kmcp profile -h\" for details. available values: 0 (for pathogen detection), 1 (higherrecall), 2 (high recall), 3 (default), 4 (high precision), 5 (higher precision). (default 3) -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to reference names. --no-amb-corr \u25ba Do not correct ambiguous reads. Use this flag to reduce analysis time if the stage 1/4 produces thousands of candidates. --norm-abund string \u25ba Method for normalize abundance of a reference by the mean/min/max abundance in all chunks, available values: mean, min, max. (default \"mean\") -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") --rank-prefix strings \u25ba Prefixes of taxon name in certain ranks, used with --metaphlan-report. (default [k__,p__,c__,o__,f__,g__,s__,t__]) -s, --sample-id string \u25ba Sample ID in result file. -S, --separator string \u25ba Separator of TaxIds and taxonomy names. (default \";\") --show-rank strings \u25ba Only show TaxIds and names of these ranks. (default [superkingdom,phylum,class,order,family,genus,species,strain]) -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds. --taxonomy-id string \u25ba Taxonomy ID in result file. utils Some utilities Usage: kmcp utils [command] Available Commands: cov2simi Convert k-mer coverage to sequence similarity filter Filter search results and find species/assembly-specific queries index-density Plot the element density of bloom filters for an index file index-info Print information of index files merge-regions Merge species/assembly-specific regions query-fpr Compute the false positive rate of a query ref-info Print information of reference chunks in a database split-genomes Split genomes into chunks unik-info Print information of .unik files ref-info Print information of reference chunks in a database Columns: file, the base name of index file i, the idx of a reference chunk in the index file, 1-based target, reference name chunkIdx, the idx of the chunk, 0-based chunks, the number of chunks of the reference kmers, the number of k-mers of the chunk fpr, the actual false-positive rate of the chunk Usage: kmcp utils ref-info [flags] Flags: -d, --db-dir string \u25ba Database directory created by \"kmcp index\". -h, --help help for ref-info -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") index-info Print information of a index file Usage: kmcp utils index-info [flags] Flags: -a, --all \u25ba Show all information. -b, --basename \u25ba Only output basenames of files. -h, --help help for index-info -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") index-density Plot the element density of bloom filters for an index file Purposes: 1. Checking whether elements (Ones) in bloom filters are uniformly distributed via an intuitive grayscale image. Outputs: 1. default output (a TSV file), columns: 1) target: reference id 2) chunkIdx: the index of genome chunk 3) bins: the number of bins in bloom filters for counting 1s 4) binSize: the size/width of a bin 5) counts: comma-seperated counts in each bin 2. the density image (a grayscale JPEG image): - X: bins. The width is the number of bins - Y: bloom filters, with each representing a genome (chunk). The height is the number of names (genome or genome chunks) - greyscale/darkness of a pixel: the density of a bin, calculated as: 255 - 255 * ${the number of 1s in the bin} / ${bin-size} Examples: 1. common use: kmcp utils index-density gtdb.kmcp/R001/_block001.uniki \\ --bins 1024 --out-file t.tsv --out-img t.jpg 2. export every bit of each position, the image could fail to create: kmcp utils index-density gtdb.kmcp/R001/_block001.uniki \\ --bin-size 1 --out-file t.tsv Usage: kmcp utils index-density [flags] Flags: -s, --bin-size int \u25ba bin size/width -b, --bins int \u25ba number of bins for counting the number of 1s. (default 1024) -h, --help help for index-density -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") --out-img string \u25ba Out density image, in format of jpeg Example: https://bioinf.shenwei.me/kmcp/faq/#are-the-elements-in-the-bloom-filters-uniformly-distributed unik-info Print information of .unik file Tips: 1. For lots of small files (especially on SDD), use big value of '-j' to parallelize counting. Usage: kmcp utils unik-info [flags] Flags: -a, --all \u25ba All information, including the number of k-mers. -b, --basename \u25ba Only output basename of files. -h, --help help for unik-info -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") -e, --skip-err \u25ba Skip error, only show warning message. --symbol-false string \u25ba Smybol for false. (default \"\u2715\") --symbol-true string \u25ba Smybol for true. (default \"\u2713\") -T, --tabular \u25ba Output in machine-friendly tabular format. merge-regions Merge species/assembly-specific regions Steps: # 1. Simulating reads and searching on one or more databases. seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db1.kmcp -o ref.fna.gz.kmcp@db1.tsv.gz seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db2.kmcp -o ref.fna.gz.kmcp@db2.tsv.gz # 2. Merging and filtering searching results kmcp merge ref.fna.gz.kmcp@*.tsv.gz \\ | kmcp utils filter -X taxdump -T taxid.map \\ -o ref.fna.gz.kmcp.uniq.tsv.gz # 3. Merging regions. # Here the value of --min-overlap should be k-1. kmcp utils merge-regions --min-overlap 20 ref.fna.gz.kmcp.uniq.tsv.gz \\ -o ref.fna.gz.kmcp.uniq.tsv.gz.bed Output (BED6 format): 1. chrom - chromosome name 2. chromStart - starting position (0-based) 3. chromEnd - ending position (0-based) 4. name - \"species-specific\" or \"assembly-specific\" 5. score - 0-1000, 1000 for \"assembly-specific\", others for \"\"species-specific\" 6. strand - \".\" Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils merge-regions [flags] Flags: -h, --help help for merge-regions -I, --ignore-type \u25ba Merge species and assembly-specific regions. --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils merge-regions -h\" for details. (default 5000) -f, --max-fpr float \u25ba Maximum false positive rate of a read in search result. (default 0.05) -g, --max-gap int \u25ba Maximum distance of starting positions of two adjacent regions, 0 for no limitation, 1 for no merging. -l, --min-overlap int \u25ba Minimum overlap of two adjacent regions, recommend K-1. (default 1) -t, --min-query-cov float \u25ba Minimum query coverage of a read in search result. (default 0.55) -a, --name-assembly string \u25ba Name of assembly-specific regions. (default \"assembly-specific\") -s, --name-species string \u25ba Name of species-specific regions. (default \"species-specific\") -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -r, --regexp string \u25ba Regular expression for extract reference name and query locations. (default \"^(.+)_sliding:(\\\\d+)\\\\-(\\\\d+)$\") filter Filter search results and find species/assembly-specific queries Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils filter [flags] Flags: -h, --help help for filter --level string \u25ba Level to filter. available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils filter\" for details. (default 5000) -f, --max-fpr float \u25ba Maximum false positive rate of a read in search result. (default 0.05) -t, --min-query-cov float \u25ba Minimum query coverage of a read in search result. (default 0.55) -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds. split-genomes Split genomes into chunks This command acts like 'kmcp compute' with many same options/flags shared, but it only performs genome splitting and does not compute k-mers. Genome chunks will be saved into the output directory with one file for a chunk. One single input file or a directory with one single genome file is preferred. Warning (experimental feature): If more than one genome files are given, the \"reference genome\" with the least and longest sequence(s) will be chosen and split into chunks. Then other genomes are fragmented and each genome fragment is assigned to the most similar genome chunk of the reference genome. Usage: kmcp utils split-genomes [flags] [-k <k>] [-n <chunks>] [-l <overlap>] {[-I <seqs dir>] | <seq files>} -O <out dir> Flags: --circular \u25ba Input sequences are circular. -r, --file-regexp string \u25ba Regular expression for matching sequence files in -I/--in-dir, case ignored. (default \"\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") --force \u25ba Overwrite existed output directory. -f, --frag-size int \u25ba size of sequence fragments to be assigned to the reference genome chunks. (default 100) -h, --help help for split-genomes -I, --in-dir string \u25ba Directory containing FASTA files. Directory symlinks are followed. --info-file string \u25ba An extra output file to show which chunk(s) are assigned to for each genome fragment. -k, --kmer int \u25ba K-mer size. (default 21) -O, --out-dir string \u25ba Output directory. -B, --seq-name-filter strings \u25ba List of regular expressions for filtering out sequences by header/name, case ignored. -m, --split-min-ref int \u25ba Only splitting sequences >= X bp. (default 1000) -n, --split-number int \u25ba Chunk number for splitting sequences, incompatible with -s/--split-size. -l, --split-overlap int \u25ba Chunk overlap for splitting sequences. The default value will be set to k-1 unless you change it. cov2simi Convert k-mer coverage to sequence similarity similarity = 87.456 + 26.410*qcov - 22.008*qcov*qcov + 7.325*qcov*qcov*qcov Usage: kmcp utils cov2simi [flags] Flags: -h, --help help for cov2simi -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") -t, --query-cov float \u25ba K-mer query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. range: [0, 1] query-fpr Compute the false positive rate of a query When the flag '-a/--all' is given, the Chernoff bound (column 'cbound') is also output along with input parameters. > Given K \u2265 p, Solomon and Kingsford also apply a Chernoff bound and show that the false positive probability for a query to be detected in a document is \u2264 exp(\u2212l(K \u2212 p)^2 /(2(1 \u2212 p))) Reference: 1. Theorem 2 in https://doi.org/10.1038/nbt.3442 2. Theorem 1 in https://arxiv.org/abs/1905.09624v2 Usage: kmcp utils query-fpr [flags] Flags: -H, --add-header \u25ba Add header line (column names -a, --all \u25ba Also show the value of -f, -n, and -t -f, --false-positive-rate float \u25ba False positive rate of a single k-mer, i.e., FPR of the bloom filters in the database. range: (0, 1) (default 0.3) -h, --help help for query-fpr -m, --matched-kmers int \u25ba The number of matched k-mers of a query. (default 35) -n, --num-kmers int \u25ba Number of unique k-mers of the query. (default 70) -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") autocompletion Generate shell autocompletion script Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish Usage: kmcp autocompletion [flags] Flags: --file string autocompletion file (default \"/home/shenwei/.bash_completion.d/kmcp.sh\") -h, --help help for autocompletion --shell string autocompletion type (bash|zsh|fish|powershell) (default \"bash\")","title":"Usage"},{"location":"usage/#usage","text":"KMCP is a command-line tool consisting of several subcommands. Subcommand Function compute Generate k-mers (sketch) from FASTA/Q sequences index Construct adatabase from k-mer files search Search sequences against a database merge Merge search results from multiple databases profile Generate the taxonomic profile from search results utils split-genomes Split genomes into chunks utils unik-info Print information of .unik files utils index-info Print information of index files utils index-density Plot the element density of bloom filters for an index file utils ref-info Print information of reference chunks in a database utils cov2simi Convert k-mer coverage to sequence similarity utils query-fpr Compute the false positive rate of a query utils filter Filter search results and find species/assembly-specific queries utils merge-regions Merge species/assembly-specific regions","title":"Usage"},{"location":"usage/#kmcp","text":"Program: kmcp (K-mer-based Metagenomic Classification and Profiling) Version: v0.9.2 Documents: https://bioinf.shenwei.me/kmcp Source code: https://github.com/shenwei356/kmcp KMCP is a tool for metagenomic classification and profiling. KMCP can also be used for: 1. Fast sequence search against large scales of genomic datasets as BIGSI and COBS do. 2. Fast assembly/genome similarity estimation as Mash and sourmash do, by utilizing Minimizer, FracMinHash (Scaled MinHash), or Closed Syncmers. Usage: kmcp [command] Available Commands: autocompletion Generate shell autocompletion script compute Generate k-mers (sketches) from FASTA/Q sequences index Construct a database from k-mer files merge Merge search results from multiple databases profile Generate the taxonomic profile from search results search Search sequences against a database utils Some utilities version Print version information and check for update Flags: -h, --help help for kmcp -i, --infile-list string \u25ba File of input files list (one file per line). If given, they are appended to files from CLI arguments. --log string \u25ba Log file. -q, --quiet \u25ba Do not print any verbose information. But you can write them to file with --log. -j, --threads int \u25ba Number of CPUs cores to use. (default 16) Use \"kmcp [command] --help\" for more information about a command.","title":"kmcp"},{"location":"usage/#compute","text":"Generate k-mers (sketches) from FASTA/Q sequences Input: 1. Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, 2. Or a directory containing sequence files via the flag -I/--in-dir, with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp. *3. For taxonomic profiling, the sequences of each reference genome should be saved in a separate file, with the reference identifier in the file name. Attention: You may rename the sequence files for convenience because the sequence/genome identifier in the index and search results would be: 1). For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. 2). For splitting sequence mode (see details below): same to 1). 3). For computing k-mers for each sequence: the sequence identifier. Attentions: 1. Unwanted sequences like plasmid can be filtered out by the name via regular expressions (-B/--seq-name-filter). 2. By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. 3. It also supports splitting sequences into chunks, this could increase the specificity in search results at the cost of a slower searching speed. 4. Multiple sizes of k-mers are supported. Supported k-mer (sketches) types: 1. K-mer: 1). ntHash of k-mer (-k) 2. K-mer sketchs (all using ntHash): 1). FracMinHash (-k -D), previously named Scaled MinHash 2). Minimizer (-k -W), optionally scaling/down-sampling (-D) 3). Closed Syncmer (-k -S), optionally scaling/down-sampling (-D) Splitting sequences: 1. Sequences can be splitted into chunks by a chunk size (-s/--split-size) or number of chunks (-n/--split-number) with overlap (-l/--split-overlap). In this mode, the sequences of each genome should be saved in an individual file. 2. When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter) in a sequence file are concatenated with k-1 N's before splitting. 3. Both sequence IDs and chunks indices are saved for later use, in form of meta/description data in .unik files. Metadata: 1. Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. 2. When parsing whole sequence files or splitting by the number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp, e.g., \"^(\\w{3}_\\d{9}\\.\\d+)\" for RefSeq records. Output: 1. All outputted .unik files are saved in ${outdir}, with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree '/xxx/yyy/zzz/' is built for > 1000 output files. 2. For splitting sequence mode (--split-size > 0 or --split-number > 0), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik 3. A summary file (\"${outdir}/_info.txt\") is generated for later use. Users need to check if the reference IDs (column \"name\") are what supposed to be. Performance tips: 1. Decrease the value of -j/--threads for data in hard disk drives to reduce I/O pressure. Next step: 1. Check the summary file (${outdir}/_info.txt) to see if the reference IDs (column \"name\") are what supposed to be. 2. Run \"kmcp index\" with the output directory. Examples: 1. From few sequence files: kmcp compute -k 21 -n 10 -l 150 -O tmp-k21-n10-l150 NC_045512.2.fna.gz 2. From a list file: kmcp compute -k 21 -n 10 -l 150 -O tmp-k21-210-l150 -i list.txt 3. From a directory containing many sequence files: kmcp compute -k 21 -n 10 -l 150 -B plasmid \\ -O gtdb-k21-n10-l150 -I gtdb-genomes/ Usage: kmcp compute [flags] [-k <k>] [-n <chunks>] [-l <overlap>] {[-I <seqs dir>] | <seq files>} -O <out dir> Flags: --by-seq \u25ba Compute k-mers (sketches) for each sequence, instead of the whole file. --circular \u25ba Input sequences are circular. -c, --compress \u25ba Output gzipped .unik files, it's slower and can save little space. -r, --file-regexp string \u25ba Regular expression for matching sequence files in -I/--in-dir, case ignored. (default \"\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") --force \u25ba Overwrite existed output directory. -h, --help help for compute -I, --in-dir string \u25ba Directory containing FASTA/Q files. Directory symlinks are followed. -k, --kmer ints \u25ba K-mer size(s). (default [21]) -W, --minimizer-w int \u25ba Minimizer window size. -O, --out-dir string \u25ba Output directory. -N, --ref-name-regexp string \u25ba Regular expression (must contains \"(\" and \")\") for extracting reference name from filename. (default \"(?i)(.+)\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") -D, --scale int \u25ba Scale of the FracMinHash (Scaled MinHash), or down-sample factor for Syncmers and Minimizer. (default 1) -B, --seq-name-filter strings \u25ba List of regular expressions for filtering out sequences by header/name, case ignored. -m, --split-min-ref int \u25ba Only splitting sequences >= X bp. (default 1000) -n, --split-number int \u25ba Chunk number for splitting sequences, incompatible with -s/--split-size. -l, --split-overlap int \u25ba Chunk overlap for splitting sequences. The default value will be set to k-1 unless you change it. -s, --split-size int \u25ba Chunk size for splitting sequences, incompatible with -n/--split-number. -S, --syncmer-s int \u25ba Length of the s-mer in Closed Syncmers.","title":"compute"},{"location":"usage/#index","text":"Construct a database from k-mer files We build the index for k-mers (sketches) with a modified compact bit-sliced signature index (COBS). We totally rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed. Input: The output directory generated by \"kmcp compute\". Database size and searching accuracy: 0. Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size. 1. -f/--false-positive-rate: the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. 2. -n/--num-hash: large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. 3. The value of block size -b/--block-size better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 4. Use flag -x/--block-sizeX-kmers-t, -8/--block-size8-kmers-t, and -1/--block-size1-kmers-t to separately create indexes for inputs with a huge number of k-mers, for precise control of database size. References: 1. COBS: https://arxiv.org/abs/1905.09624 Taxonomy data: 1. No taxonomy data are included in the database. 2. Taxonomy information are only needed in \"profile\" command. Performance tips: 1. The number of blocks (.uniki files) is better be smaller than or equal to the number of CPU cores for faster searching speed. We can set the flag -j/--threads to control the blocks number. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. 2. #threads files are simultaneously opened, and the max number of opened files is limited by the flag -F/--max-open-files. You may use a small value of -F/--max-open-files for hard disk drive storages. 3. When the database is used in a new computer with more CPU cores, 'kmcp search' could automatically scale to utilize as many cores as possible. Next step: 1. Use 'kmcp search' for searching. 2. Use 'kmcp utils ref-info' to check the number of k-mers and FPR of each genome chunk. Examples: 1. For bacterial genomes: kmcp index -f 0.3 -n 1 -j 32 -I gtdb-k21-n10-l150/ -O gtdb.kmcp 2. For viruses, use -x and -8 to control index size of the largest chunks: kmcp index -f 0.05 -n 1 -j 32 -x 100K -8 1M \\ -I genbank-viral-k21-n10-l150/ -O genbank-viral.kmcp Usage: kmcp index [flags] [-f <fpr>] [-n <hashes>] [-j <blocks>] -I <compute output> -O <kmcp db> Flags: -a, --alias string \u25ba Database alias/name. (default: basename of --out-dir). You can also manually edit it in info file: ${outdir}/__db.yml. -b, --block-size int \u25ba Block size, better be multiple of 64 for large number of input files. (default: min(#.files/#theads, 8)) -1, --block-size1-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, an individual index is created for this file. Supported units: K, M, G. (default \"200M\") -8, --block-size8-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to 8. Supported units: K, M, G. (default \"20M\") -X, --block-sizeX int \u25ba If k-mers of single .unik file exceeds --block-sizeX-kmers-t, block size is changed to this value. (default 256) -x, --block-sizeX-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to --block-sizeX. Supported units: K, M, G. (default \"10M\") --dry-run \u25ba Dry run, useful for adjusting parameters (highly recommended). -f, --false-positive-rate float \u25ba False positive rate of the bloom filters, range: (0, 1). (default 0.3) --file-regexp string \u25ba Regular expression for matching files in -I/--in-dir, case ignored. (default \".unik$\") --force \u25ba Overwrite existed output directory. -h, --help help for index -I, --in-dir string \u25ba Directory containing .unik files. Directory symlinks are followed. -F, --max-open-files int \u25ba Maximum number of opened files, please use a small value for hard disk drive storage. (default 256) -n, --num-hash int \u25ba Number of hash functions in bloom filters. (default 1) -O, --out-dir string \u25ba Output directory. (default: ${indir}.kmcp-db)","title":"index"},{"location":"usage/#search","text":"Search sequences against a database Attentions: 1. Input format should be (gzipped) FASTA or FASTQ from files or stdin. - Paired-end files should be given via -1/--read1 and -2/--read2. kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz - Single-end can be given as positional arguments or -1/-2. kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz **Single-end mode is recommended for paired-end reads, for higher sensitivity**. 2. A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold to remove duplicates. 3. For long reads or contigs, you should split them into short reads using \"seqkit sliding\", e.g., seqkit sliding -s 100 -W 300 Shared flags between \"search\" and \"profile\": 1. -t/--min-query-cov 2. -f/--max-fpr Index files loading modes: 1. Using memory-mapped index files with mmap (default): - Faster startup speed when index files are buffered in memory. - Multiple KMCP processes can share the memory. 2. Loading the whole index files into memory (-w/--load-whole-db): - This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. - It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases. - **Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS)**. 3. Low memory mode (--low-mem): - Do not load all index files into memory nor use mmap, using file seeking. - It's much slower, >4X slower on SSD and would be much slower on HDD disks. - Only use this mode for small number of queries or a huge database that can't be loaded into memory. Output format: Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging The values of tCov and jacc in results only apply to databases built with a single size of k-mer. Performance tips: 1. Increase the value of -j/--threads for acceleratation, but values larger than the number of CPU cores won't bring extra speedup. 2. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. Examples: 1. Single-end mode (recommended) kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz sample_1_unpaired.fq.gz sample_2_unpaired.fq.gz 2. Paired-end mode kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ -1 sample_1.fq.gz -2 sample_2.fq.gz 3. In computer clusters, where databases are saved in NAS storages. kmcp search -w -d gtdb.n16-00.kmcp -o sample.kmcp@gtdb.n16-00.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz Usage: kmcp search [flags] [-w] -d <kmcp db> [-t <min-query-cov>] [read1.fq.gz] [read2.fq.gz] [unpaired.fq.gz] [-o read.tsv.gz] Flags: -d, --db-dir string \u25ba Database directory created by \"kmcp index\". Please add -w/--load-whole-db for databases on network-attached storages (NAS), e.g., a computer cluster environment. -D, --default-name-map \u25ba Load ${db}/__name_mapping.tsv for mapping name first. -S, --do-not-sort \u25ba Do not sort matches of a query. -h, --help help for search -n, --keep-top-scores int \u25ba Keep matches with the top N scores for a query, 0 for all. -K, --keep-unmatched \u25ba Keep unmatched query sequence information. -u, --kmer-dedup-threshold int \u25ba Remove duplicated kmers for a query with >= X k-mers. (default 256) -w, --load-whole-db \u25ba Load all index files into memory, it's faster for small databases but needs more memory. Use this for databases on network-attached storages (NAS). Please read \"Index files loading modes\" in \"kmcp search -h\". --low-mem \u25ba Do not load all index files into memory nor use mmap, the searching would be very very slow for a large number of queries. Please read \"Index files loading modes\" in \"kmcp search -h\". -f, --max-fpr float \u25ba Maximum false positive rate of a query. (default 0.01) -c, --min-kmers int \u25ba Minimum number of matched k-mers (sketches). (default 10) -t, --min-query-cov float \u25ba Minimum query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. (default 0.55) -m, --min-query-len int \u25ba Minimum query length. (default 30) -T, --min-target-cov float \u25ba Minimum target coverage, i.e., proportion of matched k-mers and unique k-mers of a target. -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs. -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") --query-id string \u25ba Custom query Id when using the whole file as a query. -g, --query-whole-file \u25ba Use the whole file as a query, e.g., for genome similarity estimation against k-mer sketch database. -1, --read1 string \u25ba (Gzipped) read1 file. -2, --read2 string \u25ba (Gzipped) read2 file. -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\") --try-se \u25ba If paired-end reads have no hits, re-search with read1, if still fails, try read2. -G, --use-filename \u25ba Use file name as query ID when using the whole file as a query.","title":"search"},{"location":"usage/#merge","text":"Merge search results from multiple databases Input: *. Searching results of the same reads in different databases. *. The order of multiple input reads files should be the same during searching. *. When only one input given, we just copy and write to the input file. This is friendly to workflows which assume multiple inputs are given. Example: kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz Usage: kmcp merge [flags] [-o read.tsv.gz] [<search results> ...] Flags: -n, --field-hits int \u25ba Field of hits. (default 5) -f, --field-queryIdx int \u25ba Field of queryIdx. (default 15) -h, --help help for merge -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\")","title":"merge"},{"location":"usage/#profile","text":"Generate the taxonomic profile from search results Methods: 1. Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimum proportion of matched chunks (-p/--min-chunks-fraction). (***highly recommended***) Another flag -d/--max-chunks-depth-stdev further reduces false positives. 2. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. 3. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positives of ambiguous matches. You can also disable this step by the flag --no-amb-corr. If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. 4. Abundance are estimated using an Expectation-Maximization (EM) algorithm. 5. Input files are parsed for multiple times, therefore STDIN is not supported. Reference: 1. MegaPath: https://doi.org/10.1186/s12864-020-06875-6 Accuracy notes: *. Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate (-f/--max-fpr) of a query. *. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 *. -U/--min-hic-ureads, minimum number, >= 1 *. -H/--min-hic-ureads-qcov, minimum query coverage, >= -t/--min-qcov *. -P/--min-hic-ureads-prop, minimum proportion, higher values increase precision at the cost of sensitivity. *. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambiguous reads with the algorithm in MegaPath. *. --keep-perfect-matches is not recommended, which decreases sensitivity. *. --keep-main-matches is not recommended, which affects accuracy of abundance estimation. *. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation. Profiling modes: We preset six profiling modes, available with the flag -m/--mode: - 0 (for pathogen detection) - 1 (higher recall) - 2 (high recall) - 3 (default) - 4 (high precision) - 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.7 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use \"taxonkit create-taxdump\" (https://github.com/shenwei356/taxonkit) to create NCBI-style taxdump files, which also generates a TaxId mapping file. Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. *3. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results. Profiling output formats: 1. KMCP (-o/--out-file) Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please also output CAMI or MetaPhlAn format. 2. CAMI (-M/--metaphlan-report, --metaphlan-report-version, -s/--sample-id, --taxonomy-id) Related tools (https://github.com/shenwei356/taxonkit): - taxonkit profile2cami: convert any metagenomic profile table with TaxIds to CAMI format. Use this if you forget to output CAMI format. - taxonkit cami-filter: remove taxa of given TaxIds and their descendants in a CAMI metagenomic profile. 3. MetaPhlAn (-C/--cami-report, -s/--sample-id) KMCP format: Tab-delimited format with 17 columns: 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. coverage, Average coverage of the reference 4. score, The 90th percentile of qCov of uniquely matched reads 5. chunksFrac, Genome chunks fraction 6. chunksRelDepth, Relative depths of reference chunks 7. chunksRelDepthStd, The standard deviation of chunksRelDepth 8. reads, Total number of matched reads of this reference 9. ureads, Number of uniquely matched reads 10. hicureads, Number of uniquely matched reads with high-confidence 11. refsize, Reference size 12. refname, Reference name, optional via name mapping file 13. taxid, TaxId of the reference 14. rank, Taxonomic rank 15. taxname, Taxonomic name 16. taxpath, Complete lineage 17. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Taxonomic binning formats: 1. CAMI (-B/--binning-result) Examples: 1. Default mode: kmcp profile -X taxdump/ -T taxid.map -m 3 \\ sample.kmcp.tsv.gz -o sample.k.profile \\ -C sample.c.profile -s sample 2. For pathogen detection (you may create databases with lower FPRs, e.g., kmcp index -f 0.1 -n 2 for bacteria and fungi genomes, and search with a low k-mer coverage threshold -t 0.4): kmcp profile -X taxdump/ -T taxid.map -m 3 -t 0.4 \\ sample.kmcp.tsv.gz -o sample.k.profile Usage: kmcp profile [flags] [-X <taxdump dir>] [-T <taxid.map>] [-m <mode>] [-o <kmcp profile>] <search results> Flags: -I, --abund-max-iters int \u25ba Miximal iteration of abundance estimation. (default 10) --abund-pct-threshold float \u25ba If the percentage change of the predominant target is smaller than this threshold, stop the iteration. (default 0.01) -B, --binning-result string \u25ba Save extra binning result in CAMI report. -C, --cami-report string \u25ba Save extra CAMI-like report. --debug string \u25ba Debug output file. -F, --filter-low-pct float \u25ba Filter out predictions with the smallest relative abundances summing up X%. Range: [0,100). -h, --help help for profile --keep-main-matches \u25ba Only keep main matches, abandon matches with sharply decreased qcov (> --max-qcov-gap). --keep-perfect-matches \u25ba Only keep the perfect matches (qcov == 1) if there are. -n, --keep-top-qcovs int \u25ba Keep matches with the top N qcovs for a query, 0 for all. --level string \u25ba Level to estimate abundance at. Available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp profile -h\" for details. (default 5000) -d, --max-chunks-depth-stdev float \u25ba Maximum standard deviation of relative depths of all chunks. (default 2) -f, --max-fpr float \u25ba Maximum false positive rate of a read in search result. (default 0.01) -R, --max-mismatch-err float \u25ba Maximum error rate of a read being matched to a wrong reference, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) --max-qcov-gap float \u25ba Max qcov gap between adjacent matches. (default 0.4) -M, --metaphlan-report string \u25ba Save extra metaphlan-like report. --metaphlan-report-version string \u25ba Metaphlan report version (2 or 3) (default \"3\") -p, --min-chunks-fraction float \u25ba Minimum fraction of matched reference chunks with reads >= -r/--min-chunks-reads. (default 0.8) -r, --min-chunks-reads int \u25ba Minimum number of reads for a reference chunk. (default 50) -D, --min-dreads-prop float \u25ba Minimum proportion of distinct reads, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) -U, --min-hic-ureads int \u25ba Minimum number of high-confidence uniquely matched reads for a reference. (default 5) -P, --min-hic-ureads-prop float \u25ba Minimum proportion of high-confidence uniquely matched reads. (default 0.1) -H, --min-hic-ureads-qcov float \u25ba Minimum query coverage of high-confidence uniquely matched reads. (default 0.75) -t, --min-query-cov float \u25ba Minimum query coverage of a read in search result. (default 0.55) -u, --min-uniq-reads int \u25ba Minimum number of uniquely matched reads for a reference. (default 20) -m, --mode int \u25ba Profiling mode, type \"kmcp profile -h\" for details. available values: 0 (for pathogen detection), 1 (higherrecall), 2 (high recall), 3 (default), 4 (high precision), 5 (higher precision). (default 3) -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to reference names. --no-amb-corr \u25ba Do not correct ambiguous reads. Use this flag to reduce analysis time if the stage 1/4 produces thousands of candidates. --norm-abund string \u25ba Method for normalize abundance of a reference by the mean/min/max abundance in all chunks, available values: mean, min, max. (default \"mean\") -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") --rank-prefix strings \u25ba Prefixes of taxon name in certain ranks, used with --metaphlan-report. (default [k__,p__,c__,o__,f__,g__,s__,t__]) -s, --sample-id string \u25ba Sample ID in result file. -S, --separator string \u25ba Separator of TaxIds and taxonomy names. (default \";\") --show-rank strings \u25ba Only show TaxIds and names of these ranks. (default [superkingdom,phylum,class,order,family,genus,species,strain]) -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds. --taxonomy-id string \u25ba Taxonomy ID in result file.","title":"profile"},{"location":"usage/#utils","text":"Some utilities Usage: kmcp utils [command] Available Commands: cov2simi Convert k-mer coverage to sequence similarity filter Filter search results and find species/assembly-specific queries index-density Plot the element density of bloom filters for an index file index-info Print information of index files merge-regions Merge species/assembly-specific regions query-fpr Compute the false positive rate of a query ref-info Print information of reference chunks in a database split-genomes Split genomes into chunks unik-info Print information of .unik files","title":"utils"},{"location":"usage/#ref-info","text":"Print information of reference chunks in a database Columns: file, the base name of index file i, the idx of a reference chunk in the index file, 1-based target, reference name chunkIdx, the idx of the chunk, 0-based chunks, the number of chunks of the reference kmers, the number of k-mers of the chunk fpr, the actual false-positive rate of the chunk Usage: kmcp utils ref-info [flags] Flags: -d, --db-dir string \u25ba Database directory created by \"kmcp index\". -h, --help help for ref-info -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\")","title":"ref-info"},{"location":"usage/#index-info","text":"Print information of a index file Usage: kmcp utils index-info [flags] Flags: -a, --all \u25ba Show all information. -b, --basename \u25ba Only output basenames of files. -h, --help help for index-info -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\")","title":"index-info"},{"location":"usage/#index-density","text":"Plot the element density of bloom filters for an index file Purposes: 1. Checking whether elements (Ones) in bloom filters are uniformly distributed via an intuitive grayscale image. Outputs: 1. default output (a TSV file), columns: 1) target: reference id 2) chunkIdx: the index of genome chunk 3) bins: the number of bins in bloom filters for counting 1s 4) binSize: the size/width of a bin 5) counts: comma-seperated counts in each bin 2. the density image (a grayscale JPEG image): - X: bins. The width is the number of bins - Y: bloom filters, with each representing a genome (chunk). The height is the number of names (genome or genome chunks) - greyscale/darkness of a pixel: the density of a bin, calculated as: 255 - 255 * ${the number of 1s in the bin} / ${bin-size} Examples: 1. common use: kmcp utils index-density gtdb.kmcp/R001/_block001.uniki \\ --bins 1024 --out-file t.tsv --out-img t.jpg 2. export every bit of each position, the image could fail to create: kmcp utils index-density gtdb.kmcp/R001/_block001.uniki \\ --bin-size 1 --out-file t.tsv Usage: kmcp utils index-density [flags] Flags: -s, --bin-size int \u25ba bin size/width -b, --bins int \u25ba number of bins for counting the number of 1s. (default 1024) -h, --help help for index-density -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") --out-img string \u25ba Out density image, in format of jpeg Example: https://bioinf.shenwei.me/kmcp/faq/#are-the-elements-in-the-bloom-filters-uniformly-distributed","title":"index-density"},{"location":"usage/#unik-info","text":"Print information of .unik file Tips: 1. For lots of small files (especially on SDD), use big value of '-j' to parallelize counting. Usage: kmcp utils unik-info [flags] Flags: -a, --all \u25ba All information, including the number of k-mers. -b, --basename \u25ba Only output basename of files. -h, --help help for unik-info -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") -e, --skip-err \u25ba Skip error, only show warning message. --symbol-false string \u25ba Smybol for false. (default \"\u2715\") --symbol-true string \u25ba Smybol for true. (default \"\u2713\") -T, --tabular \u25ba Output in machine-friendly tabular format.","title":"unik-info"},{"location":"usage/#merge-regions","text":"Merge species/assembly-specific regions Steps: # 1. Simulating reads and searching on one or more databases. seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db1.kmcp -o ref.fna.gz.kmcp@db1.tsv.gz seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db2.kmcp -o ref.fna.gz.kmcp@db2.tsv.gz # 2. Merging and filtering searching results kmcp merge ref.fna.gz.kmcp@*.tsv.gz \\ | kmcp utils filter -X taxdump -T taxid.map \\ -o ref.fna.gz.kmcp.uniq.tsv.gz # 3. Merging regions. # Here the value of --min-overlap should be k-1. kmcp utils merge-regions --min-overlap 20 ref.fna.gz.kmcp.uniq.tsv.gz \\ -o ref.fna.gz.kmcp.uniq.tsv.gz.bed Output (BED6 format): 1. chrom - chromosome name 2. chromStart - starting position (0-based) 3. chromEnd - ending position (0-based) 4. name - \"species-specific\" or \"assembly-specific\" 5. score - 0-1000, 1000 for \"assembly-specific\", others for \"\"species-specific\" 6. strand - \".\" Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils merge-regions [flags] Flags: -h, --help help for merge-regions -I, --ignore-type \u25ba Merge species and assembly-specific regions. --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils merge-regions -h\" for details. (default 5000) -f, --max-fpr float \u25ba Maximum false positive rate of a read in search result. (default 0.05) -g, --max-gap int \u25ba Maximum distance of starting positions of two adjacent regions, 0 for no limitation, 1 for no merging. -l, --min-overlap int \u25ba Minimum overlap of two adjacent regions, recommend K-1. (default 1) -t, --min-query-cov float \u25ba Minimum query coverage of a read in search result. (default 0.55) -a, --name-assembly string \u25ba Name of assembly-specific regions. (default \"assembly-specific\") -s, --name-species string \u25ba Name of species-specific regions. (default \"species-specific\") -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -r, --regexp string \u25ba Regular expression for extract reference name and query locations. (default \"^(.+)_sliding:(\\\\d+)\\\\-(\\\\d+)$\")","title":"merge-regions"},{"location":"usage/#filter","text":"Filter search results and find species/assembly-specific queries Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils filter [flags] Flags: -h, --help help for filter --level string \u25ba Level to filter. available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils filter\" for details. (default 5000) -f, --max-fpr float \u25ba Maximum false positive rate of a read in search result. (default 0.05) -t, --min-query-cov float \u25ba Minimum query coverage of a read in search result. (default 0.55) -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds.","title":"filter"},{"location":"usage/#split-genomes","text":"Split genomes into chunks This command acts like 'kmcp compute' with many same options/flags shared, but it only performs genome splitting and does not compute k-mers. Genome chunks will be saved into the output directory with one file for a chunk. One single input file or a directory with one single genome file is preferred. Warning (experimental feature): If more than one genome files are given, the \"reference genome\" with the least and longest sequence(s) will be chosen and split into chunks. Then other genomes are fragmented and each genome fragment is assigned to the most similar genome chunk of the reference genome. Usage: kmcp utils split-genomes [flags] [-k <k>] [-n <chunks>] [-l <overlap>] {[-I <seqs dir>] | <seq files>} -O <out dir> Flags: --circular \u25ba Input sequences are circular. -r, --file-regexp string \u25ba Regular expression for matching sequence files in -I/--in-dir, case ignored. (default \"\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") --force \u25ba Overwrite existed output directory. -f, --frag-size int \u25ba size of sequence fragments to be assigned to the reference genome chunks. (default 100) -h, --help help for split-genomes -I, --in-dir string \u25ba Directory containing FASTA files. Directory symlinks are followed. --info-file string \u25ba An extra output file to show which chunk(s) are assigned to for each genome fragment. -k, --kmer int \u25ba K-mer size. (default 21) -O, --out-dir string \u25ba Output directory. -B, --seq-name-filter strings \u25ba List of regular expressions for filtering out sequences by header/name, case ignored. -m, --split-min-ref int \u25ba Only splitting sequences >= X bp. (default 1000) -n, --split-number int \u25ba Chunk number for splitting sequences, incompatible with -s/--split-size. -l, --split-overlap int \u25ba Chunk overlap for splitting sequences. The default value will be set to k-1 unless you change it.","title":"split-genomes"},{"location":"usage/#cov2simi","text":"Convert k-mer coverage to sequence similarity similarity = 87.456 + 26.410*qcov - 22.008*qcov*qcov + 7.325*qcov*qcov*qcov Usage: kmcp utils cov2simi [flags] Flags: -h, --help help for cov2simi -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\") -t, --query-cov float \u25ba K-mer query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. range: [0, 1]","title":"cov2simi"},{"location":"usage/#query-fpr","text":"Compute the false positive rate of a query When the flag '-a/--all' is given, the Chernoff bound (column 'cbound') is also output along with input parameters. > Given K \u2265 p, Solomon and Kingsford also apply a Chernoff bound and show that the false positive probability for a query to be detected in a document is \u2264 exp(\u2212l(K \u2212 p)^2 /(2(1 \u2212 p))) Reference: 1. Theorem 2 in https://doi.org/10.1038/nbt.3442 2. Theorem 1 in https://arxiv.org/abs/1905.09624v2 Usage: kmcp utils query-fpr [flags] Flags: -H, --add-header \u25ba Add header line (column names -a, --all \u25ba Also show the value of -f, -n, and -t -f, --false-positive-rate float \u25ba False positive rate of a single k-mer, i.e., FPR of the bloom filters in the database. range: (0, 1) (default 0.3) -h, --help help for query-fpr -m, --matched-kmers int \u25ba The number of matched k-mers of a query. (default 35) -n, --num-kmers int \u25ba Number of unique k-mers of the query. (default 70) -o, --out-file string \u25ba Out file, supports a \".gz\" suffix (\"-\" for stdout). (default \"-\")","title":"query-fpr"},{"location":"usage/#autocompletion","text":"Generate shell autocompletion script Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish Usage: kmcp autocompletion [flags] Flags: --file string autocompletion file (default \"/home/shenwei/.bash_completion.d/kmcp.sh\") -h, --help help for autocompletion --shell string autocompletion type (bash|zsh|fish|powershell) (default \"bash\")","title":"autocompletion"},{"location":"benchmark/","text":"","title":"Index"},{"location":"benchmark/profiling/","text":"Benchmark Please read the paper: KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. Bioinformatics, btac845, https://doi.org/10.1093/bioinformatics/btac845","title":"Taxonomic profiling"},{"location":"benchmark/profiling/#benchmark","text":"Please read the paper: KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. Bioinformatics, btac845, https://doi.org/10.1093/bioinformatics/btac845","title":"Benchmark"},{"location":"benchmark/searching/","text":"Searching benchmarks Software, datasets and commands details . Softwares COBS ( 1915fc0 ) Sourmash (v4.5.0) KMCP ( v0.9.0 ) GTDB r202 representative genomes are used for tests: file size: 46.26 GB files: 47,894 bases: 151.94 Gb KMCP vs COBS All k-mers are indexed and searched. Database size and building time: cobs kmcp database size 86.96GB 55.15GB building time 29m55s 21min04s temporary files 160.76GB 935.11G Searching with bacterial genomes or short reads (~1M reads). KMCP vs Mash and Sourmash Only FracMinHash (Scaled MinHash) (scale=1000 for Sourmash and KMCP) or MinHash (scale=3400 for Mash) are indexed and searched. Database size and building time: mash sourmash kmcp database size 1.22GB 5.19GB 1.52GB buiding time 13m30s 40m39s 7min59s temporary files - - 1.85GB Searching with bacterial genomes. Result","title":"Sequence and genome searching"},{"location":"benchmark/searching/#searching-benchmarks","text":"Software, datasets and commands details . Softwares COBS ( 1915fc0 ) Sourmash (v4.5.0) KMCP ( v0.9.0 ) GTDB r202 representative genomes are used for tests: file size: 46.26 GB files: 47,894 bases: 151.94 Gb","title":"Searching benchmarks"},{"location":"benchmark/searching/#kmcp-vs-cobs","text":"All k-mers are indexed and searched. Database size and building time: cobs kmcp database size 86.96GB 55.15GB building time 29m55s 21min04s temporary files 160.76GB 935.11G Searching with bacterial genomes or short reads (~1M reads).","title":"KMCP vs COBS"},{"location":"benchmark/searching/#kmcp-vs-mash-and-sourmash","text":"Only FracMinHash (Scaled MinHash) (scale=1000 for Sourmash and KMCP) or MinHash (scale=3400 for Mash) are indexed and searched. Database size and building time: mash sourmash kmcp database size 1.22GB 5.19GB 1.52GB buiding time 13m30s 40m39s 7min59s temporary files - - 1.85GB Searching with bacterial genomes.","title":"KMCP vs Mash and Sourmash"},{"location":"benchmark/searching/#result","text":"","title":"Result"},{"location":"tutorial/","text":"Tutorials Taxonomic profiling Sequence and genome searching","title":"Tutorials"},{"location":"tutorial/#tutorials","text":"Taxonomic profiling Sequence and genome searching","title":"Tutorials"},{"location":"tutorial/profiling/","text":"Metagenomic profiling Requirements Database Prebuilt databases are available. Or build custom databases . Hardware. CPU: \u2265 32 cores preferred. RAM: \u2265 64 GB, depends on file size of the biggest database. Datasets Short reads, single or paired end. Steps Step 1. Preprocessing reads For example, removing adapters and trimming using fastp : fastp -i in_1.fq.gz -I in_2.fq.gz \\ -o out_1.fq.gz -O out_2.fq.gz \\ -l 75 -q 20 -W 4 -M 20 -3 20 --thread 32 \\ --html out.fastp.html Step 2. Removing host reads Tools: bowtie2 is recommended for removing host reads. samtools is also used for processing reads mapping file. pigz is a parallel implementation of gzip , which is much faster than gzip . Host reference genomes: Human: CHM13 . We also provide a database of CHM13 for fast removing human reads. Building the index (~60min): bowtie2-build --threads 32 GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz chm13 Mapping and removing mapped reads: index=~/ws/db/bowtie2/chm13 bowtie2 --threads 32 -x $index -1 in_1.fq.gz -2 in_2.fq.gz \\ | samtools view -buS -f 4 - \\ | samtools fastq - \\ | gzip -c > sample.fq.gz Step 3. Searching Reads can be searched against multiple databases which can be built with different parameters , and the results can be fastly merged for downstream analysis. Attentions Input format should be (gzipped) FASTA or FASTQ from files or stdin. Paired-End reads should be given via -1/--read1 and -2/--read2 . kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz Single-end can be given as positional arguments or -1 / -2 . kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz Single-end mode is recommended for paired-end reads, for higher sensitivity . A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold (default 256 ) to remove duplicates. For long reads or contigs, you should split them into short reads using seqkit sliding , e.g., seqkit sliding -s 100 -W 300-f/--max-fpr The values of tCov and jacc in results only apply to databases built with a single size of k-mer. kmcp search and kmcp profile share some flags , therefore users can use stricter criteria in kmcp profile . -t/--min-query-cov , minimum query coverage, i.e., proportion of matched k-mers and unique k-mers of a query (default 0.55 , close to ~96.5% sequence similarity) -f/--max-fpr , maximum false positive rate of a query. (default 0.05 ) Index files loading modes Using memory-mapped index files with mmap (default): Faster startup speed when index files are buffered in memory. Multiple KMCP processes can share the memory. Loading the whole index files into memory ( -w/--load-whole-db ): This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases . Please switch on this flag ( -w ) when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS). Low memory mode ( --low-mem ): Do not load all index files into memory nor use mmap, using file seeking. It's much slower, >4X slower on SSD and would be much slower on HDD disks. Only use this mode for small number of queries or a huge database that can't be loaded into memory . Performance tips : Increase the value of -j/--threads for acceleratation, but values larger than the the number of CPU cores won't bring extra speedup. Commands Single-end mode is recommended for paired-end reads, for higher sensitivity : # --------------------------------------------------- # single-end (recommended) read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ $read1 \\ $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz Paired-end reads: # --------------------------------------------------- # paired-end read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ --read1 $read1 \\ --read2 $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz Search result format Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging Note: The header line starts with # , you need to assign another comment charactor if using csvtk for analysis. e.g., csvtk filter2 -C '$' -t -f '$qCov > 0.55' mock.kmcp.gz Demo result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx NC_003197.2-64416/1 150 130 7.4626e-15 1 GCF_000006945.2 9 10 4857450 21 90 0.6923 0.0002 0.0002 1 NC_003197.2-64414/1 150 130 7.4626e-15 1 GCF_000006945.2 6 10 4857450 21 130 1.0000 0.0003 0.0003 2 NC_003197.2-64412/1 150 130 7.4626e-15 1 GCF_000006945.2 6 10 4857450 21 121 0.9308 0.0002 0.0002 3 NC_003197.2-64410/1 150 130 7.4626e-15 1 GCF_000006945.2 1 10 4857450 21 101 0.7769 0.0002 0.0002 4 NC_003197.2-64408/1 150 130 7.8754e-15 1 GCF_000006945.2 9 10 4857450 21 83 0.6385 0.0002 0.0002 5 NC_003197.2-64406/1 150 130 7.4626e-15 1 GCF_000006945.2 2 10 4857450 21 103 0.7923 0.0002 0.0002 6 NC_003197.2-64404/1 150 130 7.4671e-15 1 GCF_000006945.2 5 10 4857450 21 86 0.6615 0.0002 0.0002 7 NC_003197.2-64402/1 150 130 7.5574e-15 1 GCF_000006945.2 3 10 4857450 21 84 0.6462 0.0002 0.0002 8 NC_003197.2-64400/1 150 130 7.4626e-15 1 GCF_000006945.2 1 10 4857450 21 89 0.6846 0.0002 0.0002 9 Searching on a computer cluster Update: We recommend analyzing one sample using one computer node, which is easier to setup up. Here, we split genomes of GTDB into 16 partitions and build a database for every partition, so we can use computer cluster to accelerate the searching. The genbank-viral genomes are also diveded into 4 partition. A helper script easy_sbatch is used for batch submitting Slurm jobs via script templates. # --------------------------------------------------- # searching j=32 reads=reads # ----------------- # gtdb dbprefix=~/ws/db/kmcp/gtdb.n16- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # viral dbprefix=~/ws/db/kmcp/genbank-viral.n4- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # fungi dbprefix=~/ws/db/kmcp/refseq-fungi for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # --------------------------------------------------- # wait all job being done # --------------------------------------------------- # merge result and profiling # merge results # there's no need to submit to slurm, which could make it slower, cause the bottleneck is file IO for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') echo $prefix; date kmcp merge $prefix.kmcp@*.tsv.gz --out-file $prefix.kmcp.tsv.gz \\ --quiet --log $prefix.kmcp.tsv.gz.merge.log done # profiling X=taxdump/ T=taxid.map fd kmcp.tsv.gz$ $reads/ \\ | rush -v X=$X -v T=$T \\ 'kmcp profile -X {X} -T {T} {} -o {}.k.profile -C {}.c.profile -s {%:} \\ --log {}.k.profile.log' Step 4. Profiling Input TaxId mapping file(s). Taxdump files. KMCP search results. Methods Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimum proportion of matched chunks ( -p/--min-chunks-fraction ) ( highly recommended ). Another flag -d/--max-chunks-cov-stdev further reduces false positives. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positive of ambiguous matches. You can also disable this step by the flag --no-amb-corr . If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. Abundance are estimated using an Expectation-Maximization (EM) algorithm.. Input files are parsed for multiple times, therefore STDIN is not supported. Accuracy notes : Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate ( -f/--max-fpr ) of a query. And we require part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence to decrease the false positive. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 -U/--min-hic-ureads , minimum number, >= 1 -H/--min-hic-ureads-qcov , minimum query coverage, >= -t/--min-qcov -P/--min-hic-ureads-prop , minimum proportion, higher values increase precision at the cost of sensitivity. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambigous reads with the algorithm in MegaPath. --keep-perfect-match is not recommended, which decreases sensitivity. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation. Profiling modes We preset six profiling modes, available with the flag -m/--mode . 0 (for pathogen detection) 1 (higher recall) 2 (high recall) 3 (default) 4 (high precision) 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.7 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data : Mapping references IDs to TaxIds: -T/--taxid-map NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use taxonkit create-taxdump to create NCBI-style taxdump files, which also generates a TaxId mapping file . Performance notes : Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size . However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results. Commands # taxid mapping files, multiple files supported. taxid_map=gtdb.kmcp/taxid.map,refseq-viral.kmcp/taxid.map,refseq-fungi.kmcp/taxid.map # or concatenate them into a big taxid.map # cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map # taxid_map=taxid.map # taxdump directory taxdump=taxdump sfile=$file.kmcp.tsv.gz kmcp profile \\ --taxid-map $taxid_map \\ --taxdump $taxdump/ \\ --level species \\ --min-query-cov 0.55 \\ --min-chunks-reads 50 \\ --min-chunks-fraction 0.8 \\ --max-chunks-depth-stdev 2 \\ --min-uniq-reads 20 \\ --min-hic-ureads 5 \\ --min-hic-ureads-qcov 0.75 \\ --min-hic-ureads-prop 0.1 \\ $sfile \\ --out-file $sfile.kmcp.profile \\ --metaphlan-report $sfile.metaphlan.profile \\ --cami-report $sfile.cami.profile \\ --sample-id \"0\" \\ --binning-result $sfile.binning.gz Profiling result formats Taxonomic profiling output formats: KMCP ( -o/--out-file ). Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please output CAMI or MetaPhlAn format . CAMI ( -M/--metaphlan-report , --metaphlan-report-version , sample name: -s/--sample-id , taxonomy data: --taxonomy-id ) MetaPhlAn ( -C/--cami-report , sample name: -s/--sample-id )) Taxonomic binning formats: CAMI ( -B/--binning-result ) KMCP format (Tab-delimited format with 17 columns): 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. coverage, Average coverage of the reference 4. score, The 90th percentile of qCov of uniquely matched reads 5. chunksFrac, Genome chunks fraction 6. chunksRelDepth, Relative depths of reference chunks 7. chunksRelDepthStd, The standard deviation of chunksRelDepth 8. reads, Total number of matched reads of this reference 9. ureads, Number of uniquely matched reads 10. hicureads, Number of uniquely matched reads with high-confidence 11. refsize, Reference size 12. refname, Reference name, optional via name mapping file 13. taxid, TaxId of the reference 14. rank, Taxonomic rank 15. taxname, Taxonomic name 16. taxpath, Complete lineage 17. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Demo output: ref percentage coverage score chunksFrac chunksRelDepth chunksRelDepthStd reads ureads hicureads refsize refname taxid rank taxname taxpath taxpathsn GCF_003697165.2 18.663804 1.864553 100.00 1.00 1.04;0.90;1.03;1.00;0.90;1.00;1.00;1.02;1.11;0.99 0.06 60952 27831 15850 4903501 4093283224 species Escherichia coli Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli 609216830;3788559933;329474883;3160438580;2234733759;3334977531;4093283224 GCF_002949675.1 18.201855 1.818404 97.69 1.00 1.04;0.93;1.02;1.03;1.04;0.97;1.04;1.02;0.95;0.98 0.04 53288 17152 8866 4395762 524994882 species Shigella dysenteriae Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Shigella;Shigella dysenteriae 609216830;3788559933;329474883;3160438580;2234733759;2258433137;524994882 GCF_000006945.2 18.143627 1.812587 100.00 1.00 1.02;0.98;0.98;0.99;1.03;0.99;0.98;1.03;0.97;1.02 0.02 58697 57300 40690 4857450 1678121664 species Salmonella enterica Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Salmonella;Salmonella enterica 609216830;3788559933;329474883;3160438580;2234733759;794943543;1678121664 GCF_000742135.1 17.738253 1.772089 100.00 1.00 1.01;1.01;1.02;0.99;1.01;1.01;1.00;0.97;0.96;1.03 0.02 65518 63665 44088 5545864 3958205156 species Klebsiella pneumoniae Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Klebsiella;Klebsiella pneumoniae 609216830;3788559933;329474883;3160438580;2234733759;2440106587;3958205156 CAMI format : @SampleID: @Version:0.10.0 @Ranks:superkingdom|phylum|class|order|family|genus|species|strain @TaxonomyID:ncbi-taxonomy @@TAXID RANK TAXPATH TAXPATHSN PERCENTAGE 2 superkingdom 2 Bacteria 100.000000 1224 phylum 2|1224 Bacteria|Proteobacteria 99.530189 74201 phylum 2|74201 Bacteria|Verrucomicrobia 0.469811 1236 class 2|1224|1236 Bacteria|Proteobacteria|Gammaproteobacteria 99.530189 203494 class 2|74201|203494 Bacteria|Verrucomicrobia|Verrucomicrobiae 0.469811 91347 order 2|1224|1236|91347 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales 99.530189 48461 order 2|74201|203494|48461 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales 0.469811 543 family 2|1224|1236|91347|543 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae 99.530189 1647988 family 2|74201|203494|48461|1647988 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae 0.469811 561 genus 2|1224|1236|91347|543|561 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia 99.530189 239934 genus 2|74201|203494|48461|1647988|239934 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia 0.469811 562 species 2|1224|1236|91347|543|561|562 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli 99.530189 239935 species 2|74201|203494|48461|1647988|239934|239935 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila 0.469811 431946 strain 2|1224|1236|91347|543|561|562|431946 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli SE15 48.321535 83333 strain 2|1224|1236|91347|543|561|562|83333 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli K-12 46.194629 386585 strain 2|1224|1236|91347|543|561|562|386585 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli O157:H7 str. Sakai 5.014025 349741 strain 2|74201|203494|48461|1647988|239934|239935|349741 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila|Akkermansia muciniphila ATCC BAA-835 0.469811 Related tools: taxonkit profile2cami can convert any metagenomic profile table with TaxIds to CAMI format. taxonkit cami-filter can remove taxa of given TaxIds and their descendants in CAMI metagenomic profile. Metaphlan3 format ( --metaphlan-report ): #SampleID #clade_name NCBI_tax_id relative_abundance additional_species k__Bacteria 2 100.000000 k__Bacteria|p__Proteobacteria 1224 99.530189 k__Bacteria|p__Verrucomicrobia 74201 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 1236 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 203494 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 91347 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 48461 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 543 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 1647988 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 561 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 239934 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 562 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 239935 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 431946 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 83333 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 386585 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 349741 0.469811 Metaphlan2 format ( --metaphlan-report-version 2 --metaphlan-report ): #SampleID k__Bacteria 100.000000 k__Bacteria|p__Proteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 0.469811 Binning result : # This is the bioboxes.org binning output format at # https://github.com/bioboxes/rfc/tree/master/data-format @Version:0.10.0 @SampleID: @@SEQUENCEID TAXID NC_000913.3_sliding:1244941-1245090 511145 NC_013654.1_sliding:344871-345020 562 NC_000913.3_sliding:3801041-3801190 511145 NC_013654.1_sliding:752751-752900 562 NC_000913.3_sliding:4080871-4081020 562 NC_000913.3_sliding:3588091-3588240 511145 NC_000913.3_sliding:2249621-2249770 562 NC_013654.1_sliding:2080171-2080320 431946 NC_000913.3_sliding:2354841-2354990 511145 NC_013654.1_sliding:437671-437820 431946","title":"Taxonomic profiling"},{"location":"tutorial/profiling/#metagenomic-profiling","text":"","title":"Metagenomic profiling"},{"location":"tutorial/profiling/#requirements","text":"Database Prebuilt databases are available. Or build custom databases . Hardware. CPU: \u2265 32 cores preferred. RAM: \u2265 64 GB, depends on file size of the biggest database.","title":"Requirements"},{"location":"tutorial/profiling/#datasets","text":"Short reads, single or paired end.","title":"Datasets"},{"location":"tutorial/profiling/#steps","text":"","title":"Steps"},{"location":"tutorial/profiling/#step-1-preprocessing-reads","text":"For example, removing adapters and trimming using fastp : fastp -i in_1.fq.gz -I in_2.fq.gz \\ -o out_1.fq.gz -O out_2.fq.gz \\ -l 75 -q 20 -W 4 -M 20 -3 20 --thread 32 \\ --html out.fastp.html","title":"Step 1. Preprocessing reads"},{"location":"tutorial/profiling/#step-2-removing-host-reads","text":"Tools: bowtie2 is recommended for removing host reads. samtools is also used for processing reads mapping file. pigz is a parallel implementation of gzip , which is much faster than gzip . Host reference genomes: Human: CHM13 . We also provide a database of CHM13 for fast removing human reads. Building the index (~60min): bowtie2-build --threads 32 GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz chm13 Mapping and removing mapped reads: index=~/ws/db/bowtie2/chm13 bowtie2 --threads 32 -x $index -1 in_1.fq.gz -2 in_2.fq.gz \\ | samtools view -buS -f 4 - \\ | samtools fastq - \\ | gzip -c > sample.fq.gz","title":"Step 2. Removing host reads"},{"location":"tutorial/profiling/#step-3-searching","text":"Reads can be searched against multiple databases which can be built with different parameters , and the results can be fastly merged for downstream analysis.","title":"Step 3. Searching"},{"location":"tutorial/profiling/#attentions","text":"Input format should be (gzipped) FASTA or FASTQ from files or stdin. Paired-End reads should be given via -1/--read1 and -2/--read2 . kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz Single-end can be given as positional arguments or -1 / -2 . kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz Single-end mode is recommended for paired-end reads, for higher sensitivity . A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold (default 256 ) to remove duplicates. For long reads or contigs, you should split them into short reads using seqkit sliding , e.g., seqkit sliding -s 100 -W 300-f/--max-fpr The values of tCov and jacc in results only apply to databases built with a single size of k-mer. kmcp search and kmcp profile share some flags , therefore users can use stricter criteria in kmcp profile . -t/--min-query-cov , minimum query coverage, i.e., proportion of matched k-mers and unique k-mers of a query (default 0.55 , close to ~96.5% sequence similarity) -f/--max-fpr , maximum false positive rate of a query. (default 0.05 )","title":"Attentions"},{"location":"tutorial/profiling/#index-files-loading-modes","text":"Using memory-mapped index files with mmap (default): Faster startup speed when index files are buffered in memory. Multiple KMCP processes can share the memory. Loading the whole index files into memory ( -w/--load-whole-db ): This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases . Please switch on this flag ( -w ) when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS). Low memory mode ( --low-mem ): Do not load all index files into memory nor use mmap, using file seeking. It's much slower, >4X slower on SSD and would be much slower on HDD disks. Only use this mode for small number of queries or a huge database that can't be loaded into memory . Performance tips : Increase the value of -j/--threads for acceleratation, but values larger than the the number of CPU cores won't bring extra speedup.","title":"Index files loading modes"},{"location":"tutorial/profiling/#commands","text":"Single-end mode is recommended for paired-end reads, for higher sensitivity : # --------------------------------------------------- # single-end (recommended) read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ $read1 \\ $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz Paired-end reads: # --------------------------------------------------- # paired-end read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ --read1 $read1 \\ --read2 $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz","title":"Commands"},{"location":"tutorial/profiling/#search-result-format","text":"Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging Note: The header line starts with # , you need to assign another comment charactor if using csvtk for analysis. e.g., csvtk filter2 -C '$' -t -f '$qCov > 0.55' mock.kmcp.gz Demo result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx NC_003197.2-64416/1 150 130 7.4626e-15 1 GCF_000006945.2 9 10 4857450 21 90 0.6923 0.0002 0.0002 1 NC_003197.2-64414/1 150 130 7.4626e-15 1 GCF_000006945.2 6 10 4857450 21 130 1.0000 0.0003 0.0003 2 NC_003197.2-64412/1 150 130 7.4626e-15 1 GCF_000006945.2 6 10 4857450 21 121 0.9308 0.0002 0.0002 3 NC_003197.2-64410/1 150 130 7.4626e-15 1 GCF_000006945.2 1 10 4857450 21 101 0.7769 0.0002 0.0002 4 NC_003197.2-64408/1 150 130 7.8754e-15 1 GCF_000006945.2 9 10 4857450 21 83 0.6385 0.0002 0.0002 5 NC_003197.2-64406/1 150 130 7.4626e-15 1 GCF_000006945.2 2 10 4857450 21 103 0.7923 0.0002 0.0002 6 NC_003197.2-64404/1 150 130 7.4671e-15 1 GCF_000006945.2 5 10 4857450 21 86 0.6615 0.0002 0.0002 7 NC_003197.2-64402/1 150 130 7.5574e-15 1 GCF_000006945.2 3 10 4857450 21 84 0.6462 0.0002 0.0002 8 NC_003197.2-64400/1 150 130 7.4626e-15 1 GCF_000006945.2 1 10 4857450 21 89 0.6846 0.0002 0.0002 9","title":"Search result format"},{"location":"tutorial/profiling/#searching-on-a-computer-cluster","text":"Update: We recommend analyzing one sample using one computer node, which is easier to setup up. Here, we split genomes of GTDB into 16 partitions and build a database for every partition, so we can use computer cluster to accelerate the searching. The genbank-viral genomes are also diveded into 4 partition. A helper script easy_sbatch is used for batch submitting Slurm jobs via script templates. # --------------------------------------------------- # searching j=32 reads=reads # ----------------- # gtdb dbprefix=~/ws/db/kmcp/gtdb.n16- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # viral dbprefix=~/ws/db/kmcp/genbank-viral.n4- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # fungi dbprefix=~/ws/db/kmcp/refseq-fungi for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # --------------------------------------------------- # wait all job being done # --------------------------------------------------- # merge result and profiling # merge results # there's no need to submit to slurm, which could make it slower, cause the bottleneck is file IO for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') echo $prefix; date kmcp merge $prefix.kmcp@*.tsv.gz --out-file $prefix.kmcp.tsv.gz \\ --quiet --log $prefix.kmcp.tsv.gz.merge.log done # profiling X=taxdump/ T=taxid.map fd kmcp.tsv.gz$ $reads/ \\ | rush -v X=$X -v T=$T \\ 'kmcp profile -X {X} -T {T} {} -o {}.k.profile -C {}.c.profile -s {%:} \\ --log {}.k.profile.log'","title":"Searching on a computer cluster"},{"location":"tutorial/profiling/#step-4-profiling","text":"","title":"Step 4. Profiling"},{"location":"tutorial/profiling/#input","text":"TaxId mapping file(s). Taxdump files. KMCP search results.","title":"Input"},{"location":"tutorial/profiling/#methods","text":"Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimum proportion of matched chunks ( -p/--min-chunks-fraction ) ( highly recommended ). Another flag -d/--max-chunks-cov-stdev further reduces false positives. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positive of ambiguous matches. You can also disable this step by the flag --no-amb-corr . If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. Abundance are estimated using an Expectation-Maximization (EM) algorithm.. Input files are parsed for multiple times, therefore STDIN is not supported. Accuracy notes : Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate ( -f/--max-fpr ) of a query. And we require part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence to decrease the false positive. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 -U/--min-hic-ureads , minimum number, >= 1 -H/--min-hic-ureads-qcov , minimum query coverage, >= -t/--min-qcov -P/--min-hic-ureads-prop , minimum proportion, higher values increase precision at the cost of sensitivity. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambigous reads with the algorithm in MegaPath. --keep-perfect-match is not recommended, which decreases sensitivity. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation.","title":"Methods"},{"location":"tutorial/profiling/#profiling-modes","text":"We preset six profiling modes, available with the flag -m/--mode . 0 (for pathogen detection) 1 (higher recall) 2 (high recall) 3 (default) 4 (high precision) 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.7 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data : Mapping references IDs to TaxIds: -T/--taxid-map NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use taxonkit create-taxdump to create NCBI-style taxdump files, which also generates a TaxId mapping file . Performance notes : Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size . However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results.","title":"Profiling modes"},{"location":"tutorial/profiling/#commands_1","text":"# taxid mapping files, multiple files supported. taxid_map=gtdb.kmcp/taxid.map,refseq-viral.kmcp/taxid.map,refseq-fungi.kmcp/taxid.map # or concatenate them into a big taxid.map # cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map # taxid_map=taxid.map # taxdump directory taxdump=taxdump sfile=$file.kmcp.tsv.gz kmcp profile \\ --taxid-map $taxid_map \\ --taxdump $taxdump/ \\ --level species \\ --min-query-cov 0.55 \\ --min-chunks-reads 50 \\ --min-chunks-fraction 0.8 \\ --max-chunks-depth-stdev 2 \\ --min-uniq-reads 20 \\ --min-hic-ureads 5 \\ --min-hic-ureads-qcov 0.75 \\ --min-hic-ureads-prop 0.1 \\ $sfile \\ --out-file $sfile.kmcp.profile \\ --metaphlan-report $sfile.metaphlan.profile \\ --cami-report $sfile.cami.profile \\ --sample-id \"0\" \\ --binning-result $sfile.binning.gz","title":"Commands"},{"location":"tutorial/profiling/#profiling-result-formats","text":"Taxonomic profiling output formats: KMCP ( -o/--out-file ). Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please output CAMI or MetaPhlAn format . CAMI ( -M/--metaphlan-report , --metaphlan-report-version , sample name: -s/--sample-id , taxonomy data: --taxonomy-id ) MetaPhlAn ( -C/--cami-report , sample name: -s/--sample-id )) Taxonomic binning formats: CAMI ( -B/--binning-result ) KMCP format (Tab-delimited format with 17 columns): 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. coverage, Average coverage of the reference 4. score, The 90th percentile of qCov of uniquely matched reads 5. chunksFrac, Genome chunks fraction 6. chunksRelDepth, Relative depths of reference chunks 7. chunksRelDepthStd, The standard deviation of chunksRelDepth 8. reads, Total number of matched reads of this reference 9. ureads, Number of uniquely matched reads 10. hicureads, Number of uniquely matched reads with high-confidence 11. refsize, Reference size 12. refname, Reference name, optional via name mapping file 13. taxid, TaxId of the reference 14. rank, Taxonomic rank 15. taxname, Taxonomic name 16. taxpath, Complete lineage 17. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Demo output: ref percentage coverage score chunksFrac chunksRelDepth chunksRelDepthStd reads ureads hicureads refsize refname taxid rank taxname taxpath taxpathsn GCF_003697165.2 18.663804 1.864553 100.00 1.00 1.04;0.90;1.03;1.00;0.90;1.00;1.00;1.02;1.11;0.99 0.06 60952 27831 15850 4903501 4093283224 species Escherichia coli Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli 609216830;3788559933;329474883;3160438580;2234733759;3334977531;4093283224 GCF_002949675.1 18.201855 1.818404 97.69 1.00 1.04;0.93;1.02;1.03;1.04;0.97;1.04;1.02;0.95;0.98 0.04 53288 17152 8866 4395762 524994882 species Shigella dysenteriae Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Shigella;Shigella dysenteriae 609216830;3788559933;329474883;3160438580;2234733759;2258433137;524994882 GCF_000006945.2 18.143627 1.812587 100.00 1.00 1.02;0.98;0.98;0.99;1.03;0.99;0.98;1.03;0.97;1.02 0.02 58697 57300 40690 4857450 1678121664 species Salmonella enterica Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Salmonella;Salmonella enterica 609216830;3788559933;329474883;3160438580;2234733759;794943543;1678121664 GCF_000742135.1 17.738253 1.772089 100.00 1.00 1.01;1.01;1.02;0.99;1.01;1.01;1.00;0.97;0.96;1.03 0.02 65518 63665 44088 5545864 3958205156 species Klebsiella pneumoniae Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Klebsiella;Klebsiella pneumoniae 609216830;3788559933;329474883;3160438580;2234733759;2440106587;3958205156 CAMI format : @SampleID: @Version:0.10.0 @Ranks:superkingdom|phylum|class|order|family|genus|species|strain @TaxonomyID:ncbi-taxonomy @@TAXID RANK TAXPATH TAXPATHSN PERCENTAGE 2 superkingdom 2 Bacteria 100.000000 1224 phylum 2|1224 Bacteria|Proteobacteria 99.530189 74201 phylum 2|74201 Bacteria|Verrucomicrobia 0.469811 1236 class 2|1224|1236 Bacteria|Proteobacteria|Gammaproteobacteria 99.530189 203494 class 2|74201|203494 Bacteria|Verrucomicrobia|Verrucomicrobiae 0.469811 91347 order 2|1224|1236|91347 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales 99.530189 48461 order 2|74201|203494|48461 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales 0.469811 543 family 2|1224|1236|91347|543 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae 99.530189 1647988 family 2|74201|203494|48461|1647988 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae 0.469811 561 genus 2|1224|1236|91347|543|561 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia 99.530189 239934 genus 2|74201|203494|48461|1647988|239934 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia 0.469811 562 species 2|1224|1236|91347|543|561|562 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli 99.530189 239935 species 2|74201|203494|48461|1647988|239934|239935 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila 0.469811 431946 strain 2|1224|1236|91347|543|561|562|431946 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli SE15 48.321535 83333 strain 2|1224|1236|91347|543|561|562|83333 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli K-12 46.194629 386585 strain 2|1224|1236|91347|543|561|562|386585 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli O157:H7 str. Sakai 5.014025 349741 strain 2|74201|203494|48461|1647988|239934|239935|349741 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila|Akkermansia muciniphila ATCC BAA-835 0.469811 Related tools: taxonkit profile2cami can convert any metagenomic profile table with TaxIds to CAMI format. taxonkit cami-filter can remove taxa of given TaxIds and their descendants in CAMI metagenomic profile. Metaphlan3 format ( --metaphlan-report ): #SampleID #clade_name NCBI_tax_id relative_abundance additional_species k__Bacteria 2 100.000000 k__Bacteria|p__Proteobacteria 1224 99.530189 k__Bacteria|p__Verrucomicrobia 74201 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 1236 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 203494 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 91347 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 48461 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 543 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 1647988 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 561 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 239934 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 562 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 239935 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 431946 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 83333 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 386585 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 349741 0.469811 Metaphlan2 format ( --metaphlan-report-version 2 --metaphlan-report ): #SampleID k__Bacteria 100.000000 k__Bacteria|p__Proteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 0.469811 Binning result : # This is the bioboxes.org binning output format at # https://github.com/bioboxes/rfc/tree/master/data-format @Version:0.10.0 @SampleID: @@SEQUENCEID TAXID NC_000913.3_sliding:1244941-1245090 511145 NC_013654.1_sliding:344871-345020 562 NC_000913.3_sliding:3801041-3801190 511145 NC_013654.1_sliding:752751-752900 562 NC_000913.3_sliding:4080871-4081020 562 NC_000913.3_sliding:3588091-3588240 511145 NC_000913.3_sliding:2249621-2249770 562 NC_013654.1_sliding:2080171-2080320 431946 NC_000913.3_sliding:2354841-2354990 511145 NC_013654.1_sliding:437671-437820 431946","title":"Profiling result formats"},{"location":"tutorial/searching/","text":"Sequence and genome searching Using cases Searching sequences against raws reads. For checking existence: Building database with raws reads and searching with query sequences, optionally using k-mer sketches for long query sequences. For abundance estimation: For long sequences (similar to taxonomic profiling): Building database with query sequences, searching with raws reads, and profiling. For short (shorter than reads length) sequences: Not capable. Searching sequences against assemblies/genomes. For checking existence: For short sequences (short reads, e.g., detecting host contamination): Building database with assemblies/genomes and searching with raws reads. For long sequences: Building database with assemblies/genomes using k-mer sketches. For genome similarity estimation: Building database with assemblies/genomes using k-mer sketches. For taxonomic profiling: For short reads: Building database with assemblies/genomes, searching with raws reads, and profiling. For long reads: Split long reads into short reads before searching. Sequence search KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. KMCP reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a small database size and much faster searching speed (check the benchmark ). Step 1. Building databases The database building process is similar to that of metagenomic profiling, with one difference: Reference genomes are not splitted into chunks. Taken GTDB for example: # mask low-complexity region (optional) mkdir -p gtdb.masked find gtdb/ -name \"*.fna.gz\" \\ | rush 'dustmasker -in <(zcat {}) -outfmt fasta \\ | sed -e \"/^>/!s/[a-z]/n/g\" \\ | gzip -c > gtdb.masked/{%}' # compute k-mers # sequences containing \"plasmid\" in name are ignored, # k = 21 kmcp compute -I gtdb.masked/ -k 21 -B plasmid -O gtdb-r202-k21 # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-r202-k21 -O gtdb-r202-k21 -n 1 -f 0.3 # cp name mapping file to database directory cp name.map gtdb-r202-k21.kmcp/ The size of database is 56GB. Step 2. Searching By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS) kmcp search supports FASTA/Q format from STDIN or files ( usage ). kmcp search -d gtdb.kmcp/ test.fq.gz -o result.tsv.gz 22:21:26.017 [INFO] kmcp v0.8.0 22:21:26.017 [INFO] https://github.com/shenwei356/kmcp 22:21:26.017 [INFO] 22:21:26.017 [INFO] checking input files ... 22:21:26.017 [INFO] 1 input file(s) given 22:21:26.018 [INFO] loading database with mmap enabled ... 22:21:26.018 [INFO] number of extra workers for every index file: 4 22:21:26.328 [INFO] database loaded: gtdb.kmcp/ 22:21:26.328 [INFO] 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] minimum query length: 30 22:21:26.328 [INFO] minimum matched k-mers: 10 22:21:26.328 [INFO] minimum query coverage: 0.550000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] 22:21:26.328 [INFO] searching ... 22:21:26.328 [INFO] reading sequence file: test.fq.gz 22:21:26.376 [INFO] 22:21:26.376 [INFO] processed queries: 10, speed: 0.012 million queries per minute 22:21:26.376 [INFO] 90.0000% (9/10) queries matched 22:21:26.376 [INFO] done searching 22:21:26.423 [INFO] 22:21:26.423 [INFO] elapsed time: 405.849288ms 22:21:26.423 [INFO] The result is tab-delimited. #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx S0R0/1 150 130 3.1777e-06 2 GCF_002872255.1 5 10 2583551 21 87 0.6692 0.0003 0.0003 0 S0R0/1 150 130 1.1936e-03 2 GCF_001434585.1 6 10 2221511 21 74 0.5692 0.0003 0.0003 0 S0R0/2 150 130 6.2145e-07 4 GCF_002872255.1 5 10 2583551 21 90 0.6923 0.0004 0.0004 1 S0R0/2 150 130 3.1777e-06 4 GCF_001434585.1 6 10 2221511 21 87 0.6692 0.0004 0.0004 1 S0R0/2 150 130 2.4002e-05 4 GCF_001438655.1 5 10 2038820 21 83 0.6385 0.0004 0.0004 1 Reference IDs (column target ) can be optionally mapped to their names during searching: kmcp search -d gtdb-r202-k21.kmcp/ -N name.map test.fq.gz -o result.tsv.gz Or after searching using csvtk : csvtk replace -t -C $ -f target -k name.map -p '(.+)' -r '{kv}' result.tsv.gz Print the main columns only: csvtk head -t -C $ -n 5 result.tsv.gz \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,FPR,qCov,target \\ | csvtk csv2md -t query FPR qCov target S0R0/1 3.1777e-06 0.6692 GCF_002872255.1 S0R0/1 1.1936e-03 0.5692 GCF_001434585.1 S0R0/2 6.2145e-07 0.6923 GCF_002872255.1 S0R0/2 3.1777e-06 0.6692 GCF_001434585.1 S0R0/2 2.4002e-05 0.6385 GCF_001438655.1 Genome similarity estimation KMCP can be used for fast similarity estimation of newly assembled genome against known reference genomes . BLAST is an option while it focuses on local similarity. Besides, the database is big and the searching speed is relative slow. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Scaled MinHash) (Sourmash). Here KMCP utilizes multiple sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ) and Syncmers ) for genome similarity estimation. Prebuilt databases are available and users can also build custom databases following steps below. Step 1. Building databases The database building process is similar to that of metagenomic profiling, with few differences: K-mer sketches instead of all k-mers are computed. Reference genomes are not splitted into chunks. Smaller false positive rate ( -f ) and more than one hash functions ( -n ) are used to improve accuracy. Using a bigger k-mer size: 31. Supported k-mer (sketches) types: K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): FracMinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Taken GTDB for example: # compute FracMinHash (Scaled MinHash) with scale 1000 # sequence containing \"plasmid\" in name are ignored, # k = 31 kmcp compute -I gtdb/ -k 31 -D 1000 -B plasmid -O gtdb-r202-minhash # build database # number of index files: 8, for server with >= 8 CPU cores # bloom filter parameter: # number of hash function: 3 # false positive rate: 0.001 kmcp index -j 8 -I gtdb-r202-minhash -O gtdb.minhash.kmcp -n 3 -f 0.001 # cp name mapping file to database directory cp taxid.map name.map gtdb.minhash.kmcp/ Attention: For small genomes like viruses, sketching parameters should be adjusted. For examples, setting a smaller scale like 10. Step 2. Searching The searching process is simple and very fast (<1 second). kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genome1 contigs.fasta -o result.tsv The output is in tab-delimited format: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging A full search result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx genome1 9488952 18737 0.0000e+00 2 GCF_000742135.1 0 1 5545784 31 8037 0.4289 0.7365 0.3719 0 genome1 9488952 18737 3.1964e-183 2 GCF_000392875.1 0 1 2881400 31 3985 0.2127 0.7062 0.1954 0 Reference IDs can be optionally mapped to their names, let's print the main columns only: kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genome1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target \\ > result.tsv query jacc target genome1 0.3719 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genome1 0.1954 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence Using closed syncmer: kmcp search --query-whole-file -d gtdb.syncmer.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genome1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target query jacc target genome1 0.3712 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genome1 0.1974 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence","title":"Sequence and genome searching"},{"location":"tutorial/searching/#sequence-and-genome-searching","text":"","title":"Sequence and genome searching"},{"location":"tutorial/searching/#using-cases","text":"Searching sequences against raws reads. For checking existence: Building database with raws reads and searching with query sequences, optionally using k-mer sketches for long query sequences. For abundance estimation: For long sequences (similar to taxonomic profiling): Building database with query sequences, searching with raws reads, and profiling. For short (shorter than reads length) sequences: Not capable. Searching sequences against assemblies/genomes. For checking existence: For short sequences (short reads, e.g., detecting host contamination): Building database with assemblies/genomes and searching with raws reads. For long sequences: Building database with assemblies/genomes using k-mer sketches. For genome similarity estimation: Building database with assemblies/genomes using k-mer sketches. For taxonomic profiling: For short reads: Building database with assemblies/genomes, searching with raws reads, and profiling. For long reads: Split long reads into short reads before searching.","title":"Using cases"},{"location":"tutorial/searching/#sequence-search","text":"KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. KMCP reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a small database size and much faster searching speed (check the benchmark ).","title":"Sequence search"},{"location":"tutorial/searching/#step-1-building-databases","text":"The database building process is similar to that of metagenomic profiling, with one difference: Reference genomes are not splitted into chunks. Taken GTDB for example: # mask low-complexity region (optional) mkdir -p gtdb.masked find gtdb/ -name \"*.fna.gz\" \\ | rush 'dustmasker -in <(zcat {}) -outfmt fasta \\ | sed -e \"/^>/!s/[a-z]/n/g\" \\ | gzip -c > gtdb.masked/{%}' # compute k-mers # sequences containing \"plasmid\" in name are ignored, # k = 21 kmcp compute -I gtdb.masked/ -k 21 -B plasmid -O gtdb-r202-k21 # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-r202-k21 -O gtdb-r202-k21 -n 1 -f 0.3 # cp name mapping file to database directory cp name.map gtdb-r202-k21.kmcp/ The size of database is 56GB.","title":"Step 1. Building databases"},{"location":"tutorial/searching/#step-2-searching","text":"By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS) kmcp search supports FASTA/Q format from STDIN or files ( usage ). kmcp search -d gtdb.kmcp/ test.fq.gz -o result.tsv.gz 22:21:26.017 [INFO] kmcp v0.8.0 22:21:26.017 [INFO] https://github.com/shenwei356/kmcp 22:21:26.017 [INFO] 22:21:26.017 [INFO] checking input files ... 22:21:26.017 [INFO] 1 input file(s) given 22:21:26.018 [INFO] loading database with mmap enabled ... 22:21:26.018 [INFO] number of extra workers for every index file: 4 22:21:26.328 [INFO] database loaded: gtdb.kmcp/ 22:21:26.328 [INFO] 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] minimum query length: 30 22:21:26.328 [INFO] minimum matched k-mers: 10 22:21:26.328 [INFO] minimum query coverage: 0.550000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] 22:21:26.328 [INFO] searching ... 22:21:26.328 [INFO] reading sequence file: test.fq.gz 22:21:26.376 [INFO] 22:21:26.376 [INFO] processed queries: 10, speed: 0.012 million queries per minute 22:21:26.376 [INFO] 90.0000% (9/10) queries matched 22:21:26.376 [INFO] done searching 22:21:26.423 [INFO] 22:21:26.423 [INFO] elapsed time: 405.849288ms 22:21:26.423 [INFO] The result is tab-delimited. #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx S0R0/1 150 130 3.1777e-06 2 GCF_002872255.1 5 10 2583551 21 87 0.6692 0.0003 0.0003 0 S0R0/1 150 130 1.1936e-03 2 GCF_001434585.1 6 10 2221511 21 74 0.5692 0.0003 0.0003 0 S0R0/2 150 130 6.2145e-07 4 GCF_002872255.1 5 10 2583551 21 90 0.6923 0.0004 0.0004 1 S0R0/2 150 130 3.1777e-06 4 GCF_001434585.1 6 10 2221511 21 87 0.6692 0.0004 0.0004 1 S0R0/2 150 130 2.4002e-05 4 GCF_001438655.1 5 10 2038820 21 83 0.6385 0.0004 0.0004 1 Reference IDs (column target ) can be optionally mapped to their names during searching: kmcp search -d gtdb-r202-k21.kmcp/ -N name.map test.fq.gz -o result.tsv.gz Or after searching using csvtk : csvtk replace -t -C $ -f target -k name.map -p '(.+)' -r '{kv}' result.tsv.gz Print the main columns only: csvtk head -t -C $ -n 5 result.tsv.gz \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,FPR,qCov,target \\ | csvtk csv2md -t query FPR qCov target S0R0/1 3.1777e-06 0.6692 GCF_002872255.1 S0R0/1 1.1936e-03 0.5692 GCF_001434585.1 S0R0/2 6.2145e-07 0.6923 GCF_002872255.1 S0R0/2 3.1777e-06 0.6692 GCF_001434585.1 S0R0/2 2.4002e-05 0.6385 GCF_001438655.1","title":"Step 2. Searching"},{"location":"tutorial/searching/#genome-similarity-estimation","text":"KMCP can be used for fast similarity estimation of newly assembled genome against known reference genomes . BLAST is an option while it focuses on local similarity. Besides, the database is big and the searching speed is relative slow. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Scaled MinHash) (Sourmash). Here KMCP utilizes multiple sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ) and Syncmers ) for genome similarity estimation. Prebuilt databases are available and users can also build custom databases following steps below.","title":"Genome similarity estimation"},{"location":"tutorial/searching/#step-1-building-databases_1","text":"The database building process is similar to that of metagenomic profiling, with few differences: K-mer sketches instead of all k-mers are computed. Reference genomes are not splitted into chunks. Smaller false positive rate ( -f ) and more than one hash functions ( -n ) are used to improve accuracy. Using a bigger k-mer size: 31. Supported k-mer (sketches) types: K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): FracMinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Taken GTDB for example: # compute FracMinHash (Scaled MinHash) with scale 1000 # sequence containing \"plasmid\" in name are ignored, # k = 31 kmcp compute -I gtdb/ -k 31 -D 1000 -B plasmid -O gtdb-r202-minhash # build database # number of index files: 8, for server with >= 8 CPU cores # bloom filter parameter: # number of hash function: 3 # false positive rate: 0.001 kmcp index -j 8 -I gtdb-r202-minhash -O gtdb.minhash.kmcp -n 3 -f 0.001 # cp name mapping file to database directory cp taxid.map name.map gtdb.minhash.kmcp/ Attention: For small genomes like viruses, sketching parameters should be adjusted. For examples, setting a smaller scale like 10.","title":"Step 1. Building databases"},{"location":"tutorial/searching/#step-2-searching_1","text":"The searching process is simple and very fast (<1 second). kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genome1 contigs.fasta -o result.tsv The output is in tab-delimited format: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging A full search result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx genome1 9488952 18737 0.0000e+00 2 GCF_000742135.1 0 1 5545784 31 8037 0.4289 0.7365 0.3719 0 genome1 9488952 18737 3.1964e-183 2 GCF_000392875.1 0 1 2881400 31 3985 0.2127 0.7062 0.1954 0 Reference IDs can be optionally mapped to their names, let's print the main columns only: kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genome1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target \\ > result.tsv query jacc target genome1 0.3719 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genome1 0.1954 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence Using closed syncmer: kmcp search --query-whole-file -d gtdb.syncmer.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genome1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target query jacc target genome1 0.3712 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genome1 0.1974 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence","title":"Step 2. Searching"}]}