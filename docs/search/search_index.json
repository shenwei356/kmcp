{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping The preprint KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. bioRxiv 2022.03.07.482835; doi: https://doi.org/10.1101/2022.03.07.482835 Documents Installation Databases Tutorials Taxonomic profiling Sequence and genome searching Usage Benchmarks FAQs What can we do? 1. Accurate metagenomic profiling KMCP adopts a novel metagenomic profiling strategy by splitting reference genomes into 10 or 5 chunks and mappings reads to these chunks via fast k-mer matching, denoted as pseudo-mapping . Benchmarking results on both simulated and real data indicate that KMCP not only allows for accurate taxonomic profiling of archaea, bacteria, and viral populations from metagenomic shotgun sequence data, but also provides confident pathogen detection in infectious clinical samples of low depth (check the benchmark ). Genome collections with custom taxonomy , e.g., GTDB uses its own taxonomy and MGV uses ICTV taxonomy , are also supported by generating NCBI-style taxdump files with taxonkit create-taxdump . 2. Fast sequence search against large scales of genomic datasets KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. We reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a smaller index size and much faster searching speed ( 4x-10x faster than COBS ) (check the tutorial and benchmark ). 3. Fast genome similarity estimation KMCP can also be used for fast similarity estimation of assemblies/genomes against known reference genomes. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Sourmash). KMCP supports multiple k-mer sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ), and Closed Syncmers ) for genome similarity estimation. And KMCP is 4x-7x faster than Mash/Sourmash (check the tutorial and benchmark ). Features Easy to install Statically linked executable binaries for multiple platforms (Linux/Windows/macOS, amd64). No dependencies, no configurations. conda install -c bioconda kmcp Easy to use Supporting shell autocompletion . Detailed usage and tutorials . Building database is easy and fast ~25 min for 47894 genomes from GTDB-r202 on a sever with 40 CPU threads and solid disk drive. Fast searching speed The index structure is modified from COBS, while KMCP is 4x-10x faster . Automatically scales to exploit all available CPU cores. Searching time is linearly related to the number of reference genomes. Scalable searching . Searching results against multiple databases can be fast merged . This brings many benefits: There's no need to re-built the database with newly added reference genomes . HPC cluster could linearly accelerate searching with each computation node hosting a database built with a part of reference genomes. Computers with limited main memory would also support searching by building small databases. Accurate taxonomic profiling Some k-mer based taxonomic profilers suffer from high false positive rates, while KMCP adopts multiple strategies to improve specificity and keeps high sensitivity at the same time . In addition to archaea and bacteria, KMCP performed well on viruses/phages . KMCP also provides confident infectious pathogen detection . Preset six modes for multiple scenarios . Supports CAMI and MetaPhlAn profiling format . Installation Download executable binaries , or install using conda: conda install -c bioconda kmcp SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters. Commands Subcommand Function compute Generate k-mers (sketch) from FASTA/Q sequences index Construct database from k-mer files search Search sequences against a database merge Merge search results from multiple databases profile Generate taxonomic profile from search results utils filter Filter search results and find species/assembly-specific queries utils merge-regions Merge species/assembly-specific regions utils unik-info Print information of .unik file utils index-info Print information of index file utils cov2simi Convert k-mer coverage to sequence similarity utils query-fpr Compute the maximal false positive rate of a query Quickstart # compute k-mers kmcp compute -k 21 --split-number 10 --split-overlap 100 \\ --in-dir genomes/ --out-dir genomes-k21-n10 # index k-mers kmcp index --false-positive-rate 0.1 --num-hash 1 \\ --in-dir genomes-k21-n10/ --out-dir genomes.kmcp # delete temporary files # rm -rf genomes-k21-n10/ # search kmcp search --db-dir genomes.kmcp/ test.fa.gz --out-file search.kmcp@db1.kmcp.tsv.gz # merge search results against multiple databases kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz # profile and binning kmcp profile search.kmcp.tsv.gz \\ --taxid-map taxid.map \\ --taxdump taxdump/ \\ --out-prefix search.tsv.gz.k.profile \\ --metaphlan-report search.tsv.gz.m.profile \\ --cami-report search.tsv.gz.c.profile \\ --binning-result search.tsv.gz.binning.gz Support Please open an issue to report bugs, propose new functions or ask for help. License MIT License Acknowledgments Zhi-Luo Deng (Helmholtz Centre for Infection Research, Germany) gave a lot of valuable advice on metagenomic profiling and benchmarking. Robert Clausecker (Zuse Institute Berlin, Germany) wrote the high-performance vectorized positional popcount package ( pospop ) during my development of KMCP , which greatly accelerated the bit-matrix searching.","title":"Home"},{"location":"#kmcp-accurate-metagenomic-profiling-of-both-prokaryotic-and-viral-populations-by-pseudo-mapping","text":"","title":"KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping"},{"location":"#the-preprint","text":"KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. bioRxiv 2022.03.07.482835; doi: https://doi.org/10.1101/2022.03.07.482835","title":"The preprint"},{"location":"#documents","text":"Installation Databases Tutorials Taxonomic profiling Sequence and genome searching Usage Benchmarks FAQs","title":"Documents"},{"location":"#what-can-we-do","text":"","title":"What can we do?"},{"location":"#1-accurate-metagenomic-profiling","text":"KMCP adopts a novel metagenomic profiling strategy by splitting reference genomes into 10 or 5 chunks and mappings reads to these chunks via fast k-mer matching, denoted as pseudo-mapping . Benchmarking results on both simulated and real data indicate that KMCP not only allows for accurate taxonomic profiling of archaea, bacteria, and viral populations from metagenomic shotgun sequence data, but also provides confident pathogen detection in infectious clinical samples of low depth (check the benchmark ). Genome collections with custom taxonomy , e.g., GTDB uses its own taxonomy and MGV uses ICTV taxonomy , are also supported by generating NCBI-style taxdump files with taxonkit create-taxdump .","title":"1. Accurate metagenomic profiling"},{"location":"#2-fast-sequence-search-against-large-scales-of-genomic-datasets","text":"KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. We reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a smaller index size and much faster searching speed ( 4x-10x faster than COBS ) (check the tutorial and benchmark ).","title":"2. Fast sequence search against large scales of genomic datasets"},{"location":"#3-fast-genome-similarity-estimation","text":"KMCP can also be used for fast similarity estimation of assemblies/genomes against known reference genomes. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Sourmash). KMCP supports multiple k-mer sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ), and Closed Syncmers ) for genome similarity estimation. And KMCP is 4x-7x faster than Mash/Sourmash (check the tutorial and benchmark ).","title":"3. Fast genome similarity estimation"},{"location":"#features","text":"Easy to install Statically linked executable binaries for multiple platforms (Linux/Windows/macOS, amd64). No dependencies, no configurations. conda install -c bioconda kmcp Easy to use Supporting shell autocompletion . Detailed usage and tutorials . Building database is easy and fast ~25 min for 47894 genomes from GTDB-r202 on a sever with 40 CPU threads and solid disk drive. Fast searching speed The index structure is modified from COBS, while KMCP is 4x-10x faster . Automatically scales to exploit all available CPU cores. Searching time is linearly related to the number of reference genomes. Scalable searching . Searching results against multiple databases can be fast merged . This brings many benefits: There's no need to re-built the database with newly added reference genomes . HPC cluster could linearly accelerate searching with each computation node hosting a database built with a part of reference genomes. Computers with limited main memory would also support searching by building small databases. Accurate taxonomic profiling Some k-mer based taxonomic profilers suffer from high false positive rates, while KMCP adopts multiple strategies to improve specificity and keeps high sensitivity at the same time . In addition to archaea and bacteria, KMCP performed well on viruses/phages . KMCP also provides confident infectious pathogen detection . Preset six modes for multiple scenarios . Supports CAMI and MetaPhlAn profiling format .","title":"Features"},{"location":"#installation","text":"Download executable binaries , or install using conda: conda install -c bioconda kmcp SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters.","title":"Installation"},{"location":"#commands","text":"Subcommand Function compute Generate k-mers (sketch) from FASTA/Q sequences index Construct database from k-mer files search Search sequences against a database merge Merge search results from multiple databases profile Generate taxonomic profile from search results utils filter Filter search results and find species/assembly-specific queries utils merge-regions Merge species/assembly-specific regions utils unik-info Print information of .unik file utils index-info Print information of index file utils cov2simi Convert k-mer coverage to sequence similarity utils query-fpr Compute the maximal false positive rate of a query","title":"Commands"},{"location":"#quickstart","text":"# compute k-mers kmcp compute -k 21 --split-number 10 --split-overlap 100 \\ --in-dir genomes/ --out-dir genomes-k21-n10 # index k-mers kmcp index --false-positive-rate 0.1 --num-hash 1 \\ --in-dir genomes-k21-n10/ --out-dir genomes.kmcp # delete temporary files # rm -rf genomes-k21-n10/ # search kmcp search --db-dir genomes.kmcp/ test.fa.gz --out-file search.kmcp@db1.kmcp.tsv.gz # merge search results against multiple databases kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz # profile and binning kmcp profile search.kmcp.tsv.gz \\ --taxid-map taxid.map \\ --taxdump taxdump/ \\ --out-prefix search.tsv.gz.k.profile \\ --metaphlan-report search.tsv.gz.m.profile \\ --cami-report search.tsv.gz.c.profile \\ --binning-result search.tsv.gz.binning.gz","title":"Quickstart"},{"location":"#support","text":"Please open an issue to report bugs, propose new functions or ask for help.","title":"Support"},{"location":"#license","text":"MIT License","title":"License"},{"location":"#acknowledgments","text":"Zhi-Luo Deng (Helmholtz Centre for Infection Research, Germany) gave a lot of valuable advice on metagenomic profiling and benchmarking. Robert Clausecker (Zuse Institute Berlin, Germany) wrote the high-performance vectorized positional popcount package ( pospop ) during my development of KMCP , which greatly accelerated the bit-matrix searching.","title":"Acknowledgments"},{"location":"database/","text":"Database Prebuilt databases All prebuilt databases and the used reference genomes are available at: OneDrive for global users . CowTransfer for Chinese users and global users . Please click the \"kmcp+105 more files\" link to browse directories and files, and choose an indiviual file to download . A command-line tool is also available for downloading a single file with the link listed in tables below. e.g., transfer https://shenwei356.cowtransfer.com/s/75737ae002fc45 Please check file integrity with `md5sum` after download the files: md5sum -c gtdb.kmcp.tar.gz.md5.txt Hardware requirements Prebuilt databases were built for computers with >= 32 CPU cores in consideration of better parallelization, and computers should have at least 64 GB. By default, kmcp search loads the whole database into main memory (via mmap ) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. To reduce memory requirements on computers without enough memory, users can split the reference genomes into partitions and build a smaller database for each partition, so that the biggest database can be loaded into RAM . This can also accelerate searching on a computer cluster, where every node searches against a small database. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis . A). Databases for metagenomic profiling These databases are created following steps below . Users can also build custom databases , it's simple and fast. DB source #species #assemblies parameters archive file size Bacteria and Archaea GTDB r202 28073+ 47894 k=21, chunks=10; fpr=0.3, hashes=1 gtdb.kmcp.tar.gz (50.16 GB, md5 ), CowTransfer link ( md5 ) 58.02 GB Bacteria and Archaea HumGut 1594+ 30691 k=21, chunks=10; fpr=0.3, hashes=1 humgut.kmcp.tar.gz (18.77 GB, md5 ), CowTransfer link ( md5 ) 21.52 GB Fungi Refseq r208 398 403 k=21, chunks=10; fpr=0.3, hashes=1 refseq-fungi.kmcp.tar.gz (3.67 GB, md5 ), CowTransfer link ( md5 ) 4.18 GB Viruses GenBank 246 23632 27936 k=21, chunks=5; fpr=0.05, hashes=1 genbank-viral.kmcp.tar.gz (1.15 GB, md5 ), CowTransfer link ( md5 ) 3.75 GB Human CHM13 1 1 k=21, chunks=1024; fpr=0.3, hashes=1 human-chm13.kmcp.tar.gz (818 MB, md5 ), CowTransfer link ( md5 ) 946 MB *based on NCBI taxonomy data 2021-12-06. + is used because some species are unclassfied xxx. Taxonomy data : Taxonomy dump file: taxdump.tar.gz (2021-12-06, md5 ) Taxonomy data for HumGut : Taxonomy dump file: taxdump-humgut.tar.gz ( md5 ) Taxid mapping file: taxid-humgut.map ( md5 ) Name mapping file: name-humgut.map ( md5 ) B). Databases for genome similarity estimation Check the tutorial . FracMinHash (Scaled MinHash): kingdoms source parameters file size Bacteria and Archaea GTDB r202 k=31, scale=1000 gtdb.minhash.kmcp.tar.gz (710 MB, md5 ) 1.52 GB Fungi Refseq r208 k=31, scale=1000 refseq-fungi.minhash.kmcp.tar.gz (49 MB, md5 ) 98 MB Viruses Genbank 246 k=31, scale=10 genbank-viral.minhash.kmcp.tar.gz (580 MB, md5 ) 1.19 GB Viruses Refseq r208 k=31, scale=10 refseq-viral.minhash.kmcp.tar.gz (205 MB, md5 ) 555 MB Closed Syncmers: kingdoms source parameters file size Bacteria and Archaea GTDB r202 k=31, s=15, scale=60 gtdb.syncmer.kmcp.tar.gz (1.03 GB, md5 ) 2.28 GB Fungi Refseq r208 k=31, s=15, scale=60 refseq-fungi.syncmer.kmcp.tar.gz (73 MB, md5 ) 145 MB Viruses Genbank 246 k=31, s=10 genbank-viral.syncmer.kmcp.tar.gz (473 MB, md5 ) 1.06 GB Viruses Refseq r208 k=31, s=21 refseq-viral.syncmer.kmcp.tar.gz (162 MB, md5 ) 441 MB C). Databases of plasmid source # assembly type parameters file size Refseq r208 37318 All k-mers k=21 refseq-plasmid.kmcp.tar.gz (5.29 GB, md5 ) 7.80 GB Refseq r208 37318 FracMinHash K=31, scale=10 refseq-plasmid.minhash.kmcp.tar.gz (1.01 GB, md5 ) 2.00 GB Refseq r208 37318 Closed Syncmer K=31, s=21 refseq-plasmid.syncmer.kmcp.tar.gz (806 MB, md5 ) 1.54 GB Building databases GTDB Tools: brename for batching renaming files. rush for executing jobs in parallel. seqkit for FASTA file processing. csvtk for tsv/csv data manipulations. taxonkit for NCBI taxonomy data manipulations. kmcp for metagenomic profiling. Files: gtdb_genomes_reps_r202.tar.gz ar122_metadata_r202.tar.gz bac120_metadata_r202.tar.gz Uncompressing and renaming: # uncompress mkdir -p gtdb tar -zxvf gtdb_genomes_reps_r202.tar.gz -C gtdb # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' gtdb Mapping file: tar -zxvf ar122_metadata_r202.tar.gz bac120_metadata_r202.tar.gz # assembly accession -> full head find gtdb/ -name \"*.fna.gz\" \\ | rush -k 'echo -ne \"{%@(.+).fna}\\t$(seqkit sort --quiet -lr {} | head -n 1 | seqkit seq -n)\\n\" ' \\ > name.map # assembly accession -> taxid (cat ar122_metadata_r202.tsv; sed 1d bac120_metadata_r202.tsv) \\ | csvtk cut -t -f accession,ncbi_taxid \\ | csvtk replace -t -p '^.._' \\ | csvtk grep -t -P <(cut -f 1 name.map) \\ | csvtk del-header \\ > taxid.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > taxid.map.stats.tsv # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump/ -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 24743 strain 4097 subspecies 90 forma specialis 58 no rank 26 isolate 23 serotype 1 Building database (all k-mers, for profiling on short-reads): input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -I $input -O gtdb-r202-k21-n10 -k 21 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-r202-k21-n10 -O gtdb.kmcp -n 1 -f 0.3 \\ --log gtdb.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.kmcp/ Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): input=gtdb find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=16 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $input.n$n- # create database for every chunks for f in $input.n$n-*; do echo $f # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -i $f -O $f-k21-n10 -k 21 -n 10 -l 100 -B plasmid \\ --log $f-k21-n10.log -j 24 --force # build database # number of index files: 24, for server with >= 24 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 24 -I $f-k21-n10 -O $f.kmcp -n 1 -f 0.3 \\ --log $f.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done Building database (k-mer sketches, for profiling on long-reads): # ------------------------------------------------------------------------------------- # Closed Syncmers with s=16 input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # s = 16 # Closed Syncmers kmcp compute -I $input -O gtdb-r202-k21-n10-S16 -k 21 -S 16 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10-S16.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.2 kmcp index -j 32 -I gtdb-r202-k21-n10-S16 -O gtdb.sync16.kmcp -n 1 -f 0.2 \\ --log gtdb.sync16.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.sync16.kmcp/ # ------------------------------------------------------------------------------------- # FracMinhash/Scaled MinHash with d=5 input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # D = 5 # FracMinhash kmcp compute -I $input -O gtdb-r202-k21-n10-D5 -k 21 -D 5 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10-D5.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.2 kmcp index -j 32 -I gtdb-r202-k21-n10-D5 -O gtdb.minh5.kmcp -n 1 -f 0.2 \\ --log gtdb.minh5.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.minh5.kmcp/ # ------------------------------------------------------------------------------------- # Minimizer with W=5 input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # W = 5 # Minimizer kmcp compute -I $input -O gtdb-r202-k21-n10-W5 -k 21 -W 5 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10-W5.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.2 kmcp index -j 32 -I gtdb-r202-k21-n10-W5 -O gtdb.mini5.kmcp -n 1 -f 0.2 \\ --log gtdb.mini5.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.mini5.kmcp/ RefSeq viral or fungi Tools genome_updater (0.4.1) for downloading genomes from NCBI. Downloading viral and fungi sequences: name=fungi # name=viral # -k for dry-run # -i for fix time genome_updater.sh \\ -d \"refseq\"\\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"refseq-$name\" \\ -t 12 \\ -m -a -p # cd to 2021-09-30_19-35-19 # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- # for viral, only splitting into 5 chunks name=viral input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n5 \\ -k 21 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5/ -O refseq-viral.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.kmcp/ # ----------------------------------------------------------------- # for fungi name=fungi input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10.log -j 32 --force kmcp index -I refseq-$name-k21-n10/ -O refseq-fungi.kmcp \\ -j 32 -f 0.3 -n 1 \\ --log refseq-fungi.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.kmcp/ Building database (k-mer sketches, for profiling on long-reads): # ----------------------------------------------------------------- # for viral, only splitting into 5 chunks name=viral input=files.renamed # --------------------------------------------- # here we compute Closed Syncmers with s=16 kmcp compute -I $input -O refseq-$name-k21-n5-S16 \\ -k 21 -S 16 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5-S16.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5-S16/ -O refseq-viral.sync16.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.sync16.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.sync16.kmcp/ # --------------------------------------------- # here we compute FracMinHash with D=5 kmcp compute -I $input -O refseq-$name-k21-n5-D5 \\ -k 21 -D 5 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5-D5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5-D5/ -O refseq-viral.minh5.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.minh5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.minh5.kmcp/ # ----------------------------------------------------------------- # for fungi name=fungi input=files.renamed # --------------------------------------------- # here we compute Closed Syncmers with s=16 kmcp compute -I $input -O refseq-$name-k21-n10-S16 \\ -k 21 -S 16 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10-S16.log -j 32 --force kmcp index -I refseq-$name-k21-n10-S16/ -O refseq-fungi.sync16.kmcp \\ -j 32 -f 0.2 -n 1 \\ --log refseq-fungi.sync16.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.sync16.kmcp/ # --------------------------------------------- # here we compute FracMinHash with D=5 kmcp compute -I $input -O refseq-$name-k21-n10-D5 \\ -k 21 -D 5 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10-D5.log -j 32 --force kmcp index -I refseq-$name-k21-n10-D5/ -O refseq-fungi.minh5.kmcp \\ -j 32 -f 0.2 -n 1 \\ --log refseq-fungi.minh5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.minh5.kmcp/ # --------------------------------------------- # here we compute Minimizer with W=5 kmcp compute -I $input -O refseq-$name-k21-n10-W5 \\ -k 21 -W 5 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10-W5.log -j 32 --force kmcp index -I refseq-$name-k21-n10-W5/ -O refseq-fungi.mini5.kmcp \\ -j 32 -f 0.2 -n 1 \\ --log refseq-fungi.mini5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.mini5.kmcp/ Genbank viral Tools genome_updater for downloading genomes from NCBI. Downloading viral sequences: name=viral # -k for dry-run # -i for fix # keep at most 5 genomes for a taxid: # genome_updater v0.4.1: -A 5 -c \"\" -l \"\" # genome_updater v0.2.5: -j taxids:5 -c \"all\" -l \"all\" time genome_updater.sh \\ -d \"genbank\"\\ -A 5 \\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"genbank-$name\" \\ -t 12 \\ -m -a -p # cd genbank-viral/2021-12-06_15-27-37/ # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Keep at most 5 genomes for a taxid (optional) # ----------------------------------------------------------------- # keep at most 5 genomes for a taxid # backup cat taxid.map > taxid.all.map cat name.map > name.all.map # keep at most 5 ids for a taxid cat taxid.all.map \\ | csvtk sort -Ht -k 2:n -k 1:n \\ | csvtk uniq -Ht -f 2 -n 5 \\ > taxid.map cat name.all.map \\ | csvtk grep -Ht -P <(cut -f 1 taxid.map) \\ > name.map # organize files input=files.renamed output=files.renamed.slim mkdir -p $output cd $output find ../$input -name \"*.fna.gz\" \\ | csvtk mutate -Ht -p '/([^\\/]+).fna.gz' \\ | csvtk grep -Ht -f 2 -P <(cut -f 1 ../taxid.map) \\ | rush 'ln -s {1}' cd .. # check number of genomes ls $output | wc -l wc -l taxid.map Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- # for viral, only splitting into 5 chunks name=viral input=files.renamed.slim # ---------------- # all kmers kmcp compute -I $input -O genbank-$name-k21-n5 \\ -k 21 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -I genbank-$name-k21-n5/ -O genbank-viral.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log genbank-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.kmcp/ Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): name=genbank-viral input=files.renamed.slim find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=4 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $name.n$n- # create database for every chunks for f in $name.n$n-*; do echo $f kmcp compute -i $f -O $f-k21-n5 \\ -k 21 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log $f-k21-n5.log -j 24 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I $f-k21-n5/ -O $f.kmcp \\ -j 24 -f 0.05 -n 1 -x 100K -8 1M \\ --log $f.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done Building database (k-mer sketches, for profiling on long-reads): name=viral input=files.renamed.slim # ---------------------------------------------- # here we compute Closed Syncmers with s=16 kmcp compute -I $input -O genbank-$name-k21-n5-S16 \\ -k 21 -S 16 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5-S16.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I genbank-$name-k21-n5-S16/ -O genbank-viral.sync16.kmcp \\ -j 32 -f 0.001 -n 3 -x 50K -8 1M \\ --log genbank-viral.sync16.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.sync16.kmcp/ # ---------------------------------------------- # here we compute FracMinHash with D=5 kmcp compute -I $input -O genbank-$name-k21-n5-D5 \\ -k 21 -D 5 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5-D5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I genbank-$name-k21-n5-D5/ -O genbank-viral.minh5.kmcp \\ -j 32 -f 0.001 -n 3 -x 50K -8 1M \\ --log genbank-viral.minh5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.minh5.kmcp/ # ---------------------------------------------- # here we compute Minimizer with W=5 kmcp compute -I $input -O genbank-$name-k21-n5-W5 \\ -k 21 -W 5 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5-W5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I genbank-$name-k21-n5-W5/ -O genbank-viral.mini5.kmcp \\ -j 32 -f 0.001 -n 3 -x 50K -8 1M \\ --log genbank-viral.mini5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.mini5.kmcp/ Human genome Downloading human genome file from CHM13 : # v1.1: wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.3_CHM13_T2T_v1.1/GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.4_T2T-CHM13v2.0/GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz Building database (all k-mers, < 6min): # v1.1: input=GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 input=GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz # splitting human genome into 1024 chunks. # The regular expression '^(\\w{3}_\\d{9}\\.\\d+).+' is for extracting 'GCA_009914755.4' from the file name. kmcp compute $input -O human-chm13-k21-n1024 \\ --ref-name-regexp '^(\\w{3}_\\d{9}\\.\\d+).+' \\ -k 21 \\ --split-number 1024 --split-overlap 100 \\ --log human-chm13-k21-n1024.log -j 32 --force # using small false positive rate: 0.3 # using more hash functions: 1 kmcp index -I human-chm13-k21-n1024/ -O human-chm13.kmcp \\ -j 8 -f 0.3 -n 1 \\ --log human-chm13.kmcp.log --force # taxid.map echo -ne \"GCA_009914755.4\\t9606\\n\" > taxid.map # name.mapp echo -ne \"GCA_009914755.4\\tHomo sapiens isolate CHM13\\n\" > name.map # cp name mapping file to database directory cp taxid.map name.map human-chm13.kmcp/ Refseq plasmid Downloading plasmid sequences: wget https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ cat index.html \\ | perl -ne 'next unless /\"(plasmid.*genomic.fna.gz)\"/; print \"$1\\n\"' > files.txt baseurl=https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ outdir=archives; mkdir -p $outdir cat files.txt \\ | rush -j 12 -v b=$baseurl -v out=$outdir \\ 'wget -c {b}/{} -o /dev/null -O {out}/{}' -c -C download.rush # name mapping seqkit seq -n archives/*.fna.gz \\ | sed -E 's/\\s+/\\t/' \\ > name.map # length stats seqkit stats -b -a -j 12 -T archives/*.fna.gz > archives.stats.tsv # split to individual files for f in archives/*.fna.gz; do \\ seqkit split2 -s 1 --quiet $f -O refseq-plasmid; \\ done # rename files with sequence ID find refseq-plasmid -name \"*.fna.gz\" \\ | rush 'mv {} {/}/$(seqkit seq -ni {}).fna.gz' Building database (all k-mers): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k21-n5 \\ -k 21 --circular \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5/ -O refseq-$name.kmcp \\ -j 32 -f 0.01 -n 3 -x 200K -X 1024 \\ --log refseq-$name.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.kmcp/ Building database (FracMinHash/Scaled MinHash): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-D10 \\ -k 31 --circular --scale 10 \\ --log refseq-$name-k31-D10.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-D10/ -O refseq-$name.minhash.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.minhash.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.minhash.kmcp/ Building database (Closed Syncmer): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-S21 \\ -k 31 --circular --syncmer-s 21 \\ --log refseq-$name-k31-S21.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-S21/ -O refseq-$name.syncmer.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.syncmer.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.syncmer.kmcp/ Building databases (prokaryotic genome collections) HumGut (30,691 clusters) HumGut is a comprehensive Human Gut prokaryotic genomes collection filtered by metagenome data. In this work, we aimed to create a collection of the most prevalent healthy human gut prokaryotic genomes, to be used as a reference database, including both MAGs from the human gut and ordinary RefSeq genomes. Dataset Genomes: HumGut.tar.gz Metadata: HumGut.tsv Taxdump files: NCBI taxonomy: ncbi_names.dump and ncbi_nodes.dump GTDB taxomomy: gtdb_names.dump and gtdb_nodes.dump Uncompressing and renaming: # uncompress tar -zxvf HumGut.tar.gz # taxdump files mkdir taxdump mv ncbi_names.dmp taxdump/names.dmp mv ncbi_nodes.dmp taxdump/nodes.dmp Mapping file: # assembly accession -> taxid cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid.map # assembly accession -> name cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_organism_name \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > name.map Stats: # ------------------------------------------------------------- # NCBI taxonomy # number of species/strains cat taxid.map \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.stats species 23604 strain 6816 subspecies 161 no rank 69 serotype 38 genus 3 # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats; species 1240 strain 458 subspecies 15 genus 1 no rank 1 serotype 1 # ------------------------------------------------------------- # GTDB taxonomy cat HumGut.tsv \\ | csvtk cut -t -f genome_file,gtdbtk_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid-gtdb.map cat taxid-gtdb.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-gtdb -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid-gtdb.map.uniq.stats species 2810 genus 434 family 52 order 12 class 2 Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I fna/ -k 21 -n 10 -B plasmid -O humgut-k21-n10 -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I humgut-k21-n10 -O humgut.kmcp -n 1 -f 0.3 cp taxid.map taxid-gtdb.map humgut.kmcp/ proGenomes2 (12,000 species) proGenomes v2.1 provides 84,096 consistently annotated bacterial and archaeal genomes from over 12000 species. Dataset: Representative genomes: contigs.representatives.fasta.gz NCBI taxonomy: proGenomes2.1_specI_lineageNCBI.tab Organize sequences: # download wget https://progenomes.embl.de/data/repGenomes/freeze12.contigs.representatives.fasta.gz # unzip for splitting by genome ID time seqkit seq freeze12.contigs.representatives.fasta.gz -o freeze12.contigs.representatives.fasta # split by genome ID seqkit split --by-id --two-pass --id-regexp '(^\\d+\\.\\w+)\\.' freeze12.contigs.representatives.fasta --out-dir genomes # batch rename brename -p '^.+id_' genomes # compress genomes for saving space find genomes/ -name \"*.fasta\" | rush 'gzip {}' Mapping file: # download wget https://progenomes.embl.de/data/proGenomes2.1_specI_lineageNCBI.tab ls genomes/ | sed 's/.fasta.gz//' > id.txt # id -> taxid cut -f 1 proGenomes2.1_specI_lineageNCBI.tab \\ | awk -F . '{print $0\"\\t\"$1}' \\ | csvtk grep -Ht -P id.txt \\ > taxid.map cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 8088 strain 3262 subspecies 37 isolate 25 no rank 16 forma specialis 14 biotype 1 # use the taxonomy verion of the refseq-virus: 2021-10-01 # id -> name cat taxid.map \\ | taxonkit lineage --data-dir taxdump -i 2 -n -L \\ | cut -f 1,3 \\ > name.map Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I genomes/ -k 21 -n 10 -B plasmid -O progenomes-k21-n10 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I progenomes-k21-n10 -O progenomes.kmcp -n 1 -f 0.3 cp taxid.map name.map progenomes.kmcp/ Building databases (viral genome collections) MGV (54,118 species) Bacteriophages have important roles in the ecology of the human gut microbiome but are under-represented in reference data- bases. To address this problem, we assembled the Metagenomic Gut Virus catalogue that comprises 189,680 viral genomes from 11,810 publicly available human stool metagenomes. Over 75% of genomes represent double-stranded DNA phages that infect members of the Bacteroidia and Clostridia classes. Based on sequence clustering we identified 54,118 candidate viral spe- cies, 92% of which were not found in existing databases. The Metagenomic Gut Virus catalogue improves detection of viruses in stool metagenomes and accounts for nearly 40% of CRISPR spacers found in human gut Bacteria and Archaea. We also pro- duced a catalogue of 459,375 viral protein clusters to explore the functional potential of the gut virome. This revealed tens of thousands of diversity-generating retroelements, which use error-prone reverse transcription to mutate target genes and may be involved in the molecular arms race between phages and their bacterial hosts. https://doi.org/10.1038/s41564-021-00928-6 https://portal.nersc.gov/MGV/ Basic information (optional) $ seqkit stats mgv_contigs.fna file format type num_seqs sum_len min_len avg_len max_len mgv_contigs.fna FASTA DNA 189,680 8,803,222,510 1,244 46,410.9 553,716 # Genome completeness Complete: n=26,030 >90% complete: n=53,220 50-90% complete: n=110,430 $ seqkit seq -n mgv_contigs.fna | head -n 3 MGV-GENOME-0364295 MGV-GENOME-0364296 MGV-GENOME-0364303 Stats (optional) cat mgv_contig_info.tsv \\ | csvtk cut -t -f completeness \\ | csvtk plot hist -o completeness.hist.png # Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness == 100' \\ | csvtk nrows 32577 # >90% complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90' \\ | csvtk nrows 78814 # < 79250 # checkv_quality $ cat mgv_contig_info.tsv \\ | csvtk cut -t -f checkv_quality \\ | csvtk freq -t -nr | more checkv_quality frequency Medium-quality 110430 High-quality 53220 Complete 26030 # >90% complete && checkv_quality == High-quality/Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk nrows 78813 Stats of high-quality genomes (optional) # high-quality genomes cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ > mgv.hq.tsv # number of families cat mgv.hq.tsv \\ | csvtk freq -t -f ictv_family -nr \\ | csvtk nrow -t 19 # number of species cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id -nr \\ | csvtk nrow -t 26779 # baltimore cat mgv.hq.tsv \\ | csvtk freq -t -f baltimore -nr \\ | csvtk pretty -t baltimore frequency --------- --------- dsDNA 76527 ssDNA 2215 NULL 62 DNA 7 dsDNA-RT 1 ssRNA-RT 1 # prophage? cat mgv.hq.tsv \\ | csvtk freq -t -f prophage -nr \\ | csvtk pretty -t prophage frequency -------- --------- No 58366 Yes 20447 # species with both prophage and lytic phages cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id,prophage \\ | csvtk freq -t -f votu_id \\ | csvtk filter2 -t -f '$frequency > 1' \\ | csvtk nrow -t 2745 Prepare genomes: # ID cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk cut -t -f contig_id \\ | csvtk del-header \\ > mgv.hq.tsv.id # extract sequences seqkit grep -f mgv.hq.tsv.id mgv_contigs.fna -o mgv.hq.fasta.gz # split into files with one genome seqkit split2 -s 1 mgv.hq.fasta.gz -O mgv # rename with find mgv/ -name \"*.fasta.gz\" \\ | rush -j 20 'mv {} {/}/$(seqkit seq -ni {}).fa.gz' Create taxdump files and taxid.map with taxonkit (version >= v0.12.0): cat mgv_contig_info.tsv \\ | csvtk cut -t -f ictv_order,ictv_family,ictv_genus,votu_id,contig_id \\ | csvtk del-header \\ > mgv.taxonomy.tsv taxonkit create-taxdump mgv.taxonomy.tsv --out-dir mgv-taxdump \\ --force -A 5 -R order,family,genus,species cp mgv-taxdump/taxid.map . # name.map cat mgv-taxdump/taxid.map \\ | taxonkit lineage --data-dir mgv-taxdump/ -i 2 \\ | cut -f 1,3 \\ > name.map head -n 5 name.map MGV-GENOME-0364295 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364296 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364303 Caudovirales;crAss-phage;OTU-05782 MGV-GENOME-0364311 Caudovirales;crAss-phage;OTU-01114 MGV-GENOME-0364312 Caudovirales;crAss-phage;OTU-23935 Building database: # compute k-mers # reference genomes are split into 5 chunks # k = 21 kmcp compute -I mgv/ -k 21 -n 5 -O mgv-k21-n5 --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -j 32 -I mgv-k21-n5 -O mgv.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log mgv.kmcp.log cp taxid.map name.map mgv.kmcp/ Building custom databases Files: Genome files (Gzip-compressed) FASTA/Q format. One genome per file with the reference identifier in the file name . TaxId mapping file (for metagenomic profiling) Two-column (reference identifier and TaxId) tab-delimited. NCBI taxonomy dump files (for metagenomic profiling). You can use taxonkit create-taxdump to create NCBI-style taxdump files for custom genome collections, which also generates a TaxId mapping file. names.dmp nodes.dmp merged.dmp (optional) delnodes.dmp (optional) Tools: kmcp for metagenomic profiling. rush for executing jobs in parallel. brename for batching renaming files (optional) Memory notes : By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database. Step 1. Computing k-mers Input: Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, Or a directory containing sequence files via the flag -I/--in-dir , with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp . The default pattern matches files with extension of fa , fasta , fna , fq , fastq , fa.gz , fasta.gz , fna.gz , fq.gz or fastq.gz . You may rename the sequence files for convenience using brename . because the sequence/genome identifier in the index and search results would be: For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. For splitting sequence mode (see details below): same to 1). For computing k-mers for each sequence: the sequence identifier. Unwanted sequences like plasmid sequences can be filtered out by the name via regular expression(s) ( -B/--seq-name-filter ). How to compute k-mers : By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. It also supports splitting sequences into chunks, this could increase the specificity in profiling results at the cost of a slower searching speed . Splitting sequences : Sequences can be split into chunks by a chunk size ( -s/--split-size ) or number of chunks ( -n/--split-number ) with overlap ( -l/--split-overlap ). In this mode, the sequences of each genome should be saved in an individual file . When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter ) in a sequence file are concatenated with k-1 Ns before splitting . Both sequence/reference IDs and chunks indices are saved for later use, in form of meta/description data in .unik files, and will be reported in kmcp search results. Metadata : Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. When parsing whole sequence files or splitting by number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp , e.g., ^(\\w{3}_\\d{9}\\.\\d+) for RefSeq assembly accessions . Multiple sizes of k-mers are supported, but a single k-mer size is good enough. Supported k-mer (sketches) types : K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): Scaled MinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Output : All outputted .unik files are saved in ${outdir} , with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree /xxx/yyy/zzz/ is built for > 1000 output files. For splitting sequence mode ( --split-size > 0 or --split-number > 0 ), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik A summary file ( ${outdir}/_info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . Performance tips : Decrease value of -j/--threads for data in hard disk drives (HDD) to reduce I/O pressure. Commands ( usage ): # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks, # k = 21 kmcp compute --in-dir refs/ \\ --kmer 21 \\ --split-number 10 \\ --split-overlap 100 \\ --seq-name-filter plasmid \\ --ref-name-regexp '(.+).fasta.gz' \\ --out-dir refs-k21-n10 # demo output 13:11:13.397 [INFO] kmcp v0.8.0 13:11:13.397 [INFO] https://github.com/shenwei356/kmcp 13:11:13.397 [INFO] 13:11:13.397 [INFO] checking input files ... 13:11:13.398 [INFO] 9 input file(s) given 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] input and output: 13:11:13.398 [INFO] input directory: refs/ 13:11:13.398 [INFO] regular expression of input files: (?i)\\.(f[aq](st[aq])?|fna)(.gz)?$ 13:11:13.398 [INFO] *regular expression for extracting reference name from file name: (?i)(.+).fasta.gz 13:11:13.398 [INFO] *regular expressions for filtering out sequences: [plasmid] 13:11:13.398 [INFO] output directory: refs-k21-n10 13:11:13.398 [INFO] 13:11:13.398 [INFO] sequences splitting: true 13:11:13.398 [INFO] split parts: 10, overlap: 100 bp 13:11:13.398 [INFO] 13:11:13.398 [INFO] k-mer (sketches) computing: 13:11:13.398 [INFO] k-mer size(s): 21 13:11:13.398 [INFO] circular genome: false 13:11:13.398 [INFO] saving exact number of k-mers: true 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] 13:11:13.398 [INFO] computing ... processed files: 9 / 9 [======================================] ETA: 0s. done 13:11:13.845 [INFO] 13:11:13.845 [INFO] elapsed time: 453.870411ms 13:11:13.845 [INFO] A summary file ( _info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . #path name chunkIdx idxNum genomeSize kmers refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_0.unik NC_010655.1 0 10 2664102 264247 refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_1.unik NC_010655.1 1 10 2664102 266237 refs-k21-n10/595/162/595/NC_012971.2.fasta.gz/NC_012971.2-chunk_0.unik NC_012971.2 0 10 4558953 450494 refs-k21-n10/292/039/292/NC_000913.3.fasta.gz/NC_000913.3-chunk_0.unik NC_000913.3 0 10 4641652 459277 refs-k21-n10/934/859/934/NC_013654.1.fasta.gz/NC_013654.1-chunk_0.unik NC_013654.1 0 10 4717338 470575 Meta data in the .unik file can be showed using kmcp utils unik-info : kmcp utils unik-info refs-k21-n10/072/380/072/NZ_CP028116.1.fasta.gz/NZ_CP028116.1-chunk_0.unik -a Step 2. Building databases KMCP builds index for k-mers (sketches) with a modified Compact Bit-sliced Signature Index ( COBS ). We completely rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed (check benchmark ). Input : The output directory generated by kmcp compute . Database size and searching accuracy : Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size . -f/--false-positive-rate : the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. -n/--num-hash : large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. The value of block size -b/--block-size is better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 Use flag -x/--block-sizeX-kmers-t , -8/--block-size8-kmers-t , and -1/--block-size1-kmers-t to separately create indexes for inputs with huge number of k-mers, for precise control of database size. Taxonomy data : No taxonomy data are included in the database. Taxonomy information are only needed in kmcp profile . Performance tips : The umber of blocks ( .uniki files) is better to be smaller than or equal to the number of CPU cores for faster searching speed. We can set -j/--threads to control the blocks number . When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. #threads files are simultaneously opened, and the max number of opened files is limited by the flag -F/--max-open-files . You may use a small value of -F/--max-open-files for hard disk drive storages. When the database is used in a new computer with more CPU cores, kmcp search could automatically scale to utilize as many cores as possible. Commands ( usage ): # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -I refs-k21-n10 \\ --threads 32 \\ --num-hash 1 \\ --false-positive-rate 0.3 \\ --out-dir refs.kmcp # demo output 22:44:17.710 [INFO] kmcp v0.8.0 22:44:17.710 [INFO] https://github.com/shenwei356/kmcp 22:44:17.710 [INFO] 22:44:17.710 [INFO] loading .unik file infos from file: refs-k21-n10/_info.txt 22:44:17.716 [INFO] 90 cached file infos loaded 22:44:17.716 [INFO] 22:44:17.716 [INFO] -------------------- [main parameters] -------------------- 22:44:17.716 [INFO] number of hashes: 1 22:44:17.716 [INFO] false positive rate: 0.300000 22:44:17.717 [INFO] k-mer size(s): 21 22:44:17.717 [INFO] split seqequence size: 0, overlap: 0 22:44:17.717 [INFO] block-sizeX-kmers-t: 10.00 M 22:44:17.717 [INFO] block-sizeX : 256.00 22:44:17.717 [INFO] block-size8-kmers-t: 20.00 M 22:44:17.717 [INFO] block-size1-kmers-t: 200.00 M 22:44:17.717 [INFO] -------------------- [main parameters] -------------------- 22:44:17.717 [INFO] 22:44:17.717 [INFO] building index ... 22:44:17.726 [WARN] ignore -X/--block-size (256) which is >= -b/--block-size (8) 22:44:17.726 [INFO] 22:44:17.726 [INFO] block size: 8 22:44:17.726 [INFO] number of index files: 32 (may be more) 22:44:17.726 [INFO] 22:44:17.726 [block #001] 1 / 1 100 % 22:44:17.726 [block #002] 1 / 1 100 % 22:44:17.726 [block #003] 1 / 1 100 % 22:44:17.726 [block #004] 1 / 1 100 % 22:44:17.726 [block #005] 1 / 1 100 % 22:44:17.726 [block #006] 1 / 1 100 % 22:44:17.726 [block #007] 1 / 1 100 % 22:44:17.726 [block #008] 1 / 1 100 % 22:44:17.726 [block #009] 1 / 1 100 % 22:44:17.727 [block #010] 1 / 1 100 % 22:44:17.727 [block #011] 1 / 1 100 % 22:44:17.727 [block #012] 1 / 1 100 % [saved index files] 12 / 12 ETA: 0s. done 22:44:17.933 [INFO] 22:44:17.933 [INFO] kmcp database with 42713316 k-mers saved to refs.kmcp 22:44:17.933 [INFO] total file size: 15.66 MB 22:44:17.933 [INFO] total index files: 12 22:44:17.933 [INFO] 22:44:17.933 [INFO] elapsed time: 223.524128ms 22:44:17.933 [INFO] Output: refs.kmcp/ \u2514\u2500\u2500 R001 \u251c\u2500\u2500 _block001.uniki \u251c\u2500\u2500 _block002.uniki \u251c\u2500\u2500 _block003.uniki \u251c\u2500\u2500 _block004.uniki \u251c\u2500\u2500 _block005.uniki \u251c\u2500\u2500 _block006.uniki \u251c\u2500\u2500 _block007.uniki \u251c\u2500\u2500 _block008.uniki \u251c\u2500\u2500 _block009.uniki \u251c\u2500\u2500 _block010.uniki \u251c\u2500\u2500 _block011.uniki \u251c\u2500\u2500 _block012.uniki \u251c\u2500\u2500 __db.yml \u2514\u2500\u2500 __name_mapping.tsv __db.yml contains configuration of the database, and _blockXXX.uniki are index files. kmcp utils index-info could show the basic information of index files: kmcp utils index-info refs.kmcp/R001/_block001.uniki file k canonical num-hashes num-sigs num-names refs.kmcp/R001/_block001.uniki 21 true 1 746442 8 What's next? Check the tutorials .","title":"Databases"},{"location":"database/#database","text":"","title":"Database"},{"location":"database/#prebuilt-databases","text":"All prebuilt databases and the used reference genomes are available at: OneDrive for global users . CowTransfer for Chinese users and global users . Please click the \"kmcp+105 more files\" link to browse directories and files, and choose an indiviual file to download . A command-line tool is also available for downloading a single file with the link listed in tables below. e.g., transfer https://shenwei356.cowtransfer.com/s/75737ae002fc45 Please check file integrity with `md5sum` after download the files: md5sum -c gtdb.kmcp.tar.gz.md5.txt Hardware requirements Prebuilt databases were built for computers with >= 32 CPU cores in consideration of better parallelization, and computers should have at least 64 GB. By default, kmcp search loads the whole database into main memory (via mmap ) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. To reduce memory requirements on computers without enough memory, users can split the reference genomes into partitions and build a smaller database for each partition, so that the biggest database can be loaded into RAM . This can also accelerate searching on a computer cluster, where every node searches against a small database. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis .","title":"Prebuilt databases"},{"location":"database/#a-databases-for-metagenomic-profiling","text":"These databases are created following steps below . Users can also build custom databases , it's simple and fast. DB source #species #assemblies parameters archive file size Bacteria and Archaea GTDB r202 28073+ 47894 k=21, chunks=10; fpr=0.3, hashes=1 gtdb.kmcp.tar.gz (50.16 GB, md5 ), CowTransfer link ( md5 ) 58.02 GB Bacteria and Archaea HumGut 1594+ 30691 k=21, chunks=10; fpr=0.3, hashes=1 humgut.kmcp.tar.gz (18.77 GB, md5 ), CowTransfer link ( md5 ) 21.52 GB Fungi Refseq r208 398 403 k=21, chunks=10; fpr=0.3, hashes=1 refseq-fungi.kmcp.tar.gz (3.67 GB, md5 ), CowTransfer link ( md5 ) 4.18 GB Viruses GenBank 246 23632 27936 k=21, chunks=5; fpr=0.05, hashes=1 genbank-viral.kmcp.tar.gz (1.15 GB, md5 ), CowTransfer link ( md5 ) 3.75 GB Human CHM13 1 1 k=21, chunks=1024; fpr=0.3, hashes=1 human-chm13.kmcp.tar.gz (818 MB, md5 ), CowTransfer link ( md5 ) 946 MB *based on NCBI taxonomy data 2021-12-06. + is used because some species are unclassfied xxx. Taxonomy data : Taxonomy dump file: taxdump.tar.gz (2021-12-06, md5 ) Taxonomy data for HumGut : Taxonomy dump file: taxdump-humgut.tar.gz ( md5 ) Taxid mapping file: taxid-humgut.map ( md5 ) Name mapping file: name-humgut.map ( md5 )","title":"A). Databases for metagenomic profiling"},{"location":"database/#b-databases-for-genome-similarity-estimation","text":"Check the tutorial . FracMinHash (Scaled MinHash): kingdoms source parameters file size Bacteria and Archaea GTDB r202 k=31, scale=1000 gtdb.minhash.kmcp.tar.gz (710 MB, md5 ) 1.52 GB Fungi Refseq r208 k=31, scale=1000 refseq-fungi.minhash.kmcp.tar.gz (49 MB, md5 ) 98 MB Viruses Genbank 246 k=31, scale=10 genbank-viral.minhash.kmcp.tar.gz (580 MB, md5 ) 1.19 GB Viruses Refseq r208 k=31, scale=10 refseq-viral.minhash.kmcp.tar.gz (205 MB, md5 ) 555 MB Closed Syncmers: kingdoms source parameters file size Bacteria and Archaea GTDB r202 k=31, s=15, scale=60 gtdb.syncmer.kmcp.tar.gz (1.03 GB, md5 ) 2.28 GB Fungi Refseq r208 k=31, s=15, scale=60 refseq-fungi.syncmer.kmcp.tar.gz (73 MB, md5 ) 145 MB Viruses Genbank 246 k=31, s=10 genbank-viral.syncmer.kmcp.tar.gz (473 MB, md5 ) 1.06 GB Viruses Refseq r208 k=31, s=21 refseq-viral.syncmer.kmcp.tar.gz (162 MB, md5 ) 441 MB","title":"B). Databases for genome similarity estimation"},{"location":"database/#c-databases-of-plasmid","text":"source # assembly type parameters file size Refseq r208 37318 All k-mers k=21 refseq-plasmid.kmcp.tar.gz (5.29 GB, md5 ) 7.80 GB Refseq r208 37318 FracMinHash K=31, scale=10 refseq-plasmid.minhash.kmcp.tar.gz (1.01 GB, md5 ) 2.00 GB Refseq r208 37318 Closed Syncmer K=31, s=21 refseq-plasmid.syncmer.kmcp.tar.gz (806 MB, md5 ) 1.54 GB","title":"C). Databases of plasmid"},{"location":"database/#building-databases","text":"","title":"Building databases"},{"location":"database/#gtdb","text":"Tools: brename for batching renaming files. rush for executing jobs in parallel. seqkit for FASTA file processing. csvtk for tsv/csv data manipulations. taxonkit for NCBI taxonomy data manipulations. kmcp for metagenomic profiling. Files: gtdb_genomes_reps_r202.tar.gz ar122_metadata_r202.tar.gz bac120_metadata_r202.tar.gz Uncompressing and renaming: # uncompress mkdir -p gtdb tar -zxvf gtdb_genomes_reps_r202.tar.gz -C gtdb # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' gtdb Mapping file: tar -zxvf ar122_metadata_r202.tar.gz bac120_metadata_r202.tar.gz # assembly accession -> full head find gtdb/ -name \"*.fna.gz\" \\ | rush -k 'echo -ne \"{%@(.+).fna}\\t$(seqkit sort --quiet -lr {} | head -n 1 | seqkit seq -n)\\n\" ' \\ > name.map # assembly accession -> taxid (cat ar122_metadata_r202.tsv; sed 1d bac120_metadata_r202.tsv) \\ | csvtk cut -t -f accession,ncbi_taxid \\ | csvtk replace -t -p '^.._' \\ | csvtk grep -t -P <(cut -f 1 name.map) \\ | csvtk del-header \\ > taxid.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > taxid.map.stats.tsv # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump/ -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 24743 strain 4097 subspecies 90 forma specialis 58 no rank 26 isolate 23 serotype 1 Building database (all k-mers, for profiling on short-reads): input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -I $input -O gtdb-r202-k21-n10 -k 21 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-r202-k21-n10 -O gtdb.kmcp -n 1 -f 0.3 \\ --log gtdb.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.kmcp/ Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): input=gtdb find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=16 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $input.n$n- # create database for every chunks for f in $input.n$n-*; do echo $f # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 kmcp compute -i $f -O $f-k21-n10 -k 21 -n 10 -l 100 -B plasmid \\ --log $f-k21-n10.log -j 24 --force # build database # number of index files: 24, for server with >= 24 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 24 -I $f-k21-n10 -O $f.kmcp -n 1 -f 0.3 \\ --log $f.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done Building database (k-mer sketches, for profiling on long-reads): # ------------------------------------------------------------------------------------- # Closed Syncmers with s=16 input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # s = 16 # Closed Syncmers kmcp compute -I $input -O gtdb-r202-k21-n10-S16 -k 21 -S 16 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10-S16.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.2 kmcp index -j 32 -I gtdb-r202-k21-n10-S16 -O gtdb.sync16.kmcp -n 1 -f 0.2 \\ --log gtdb.sync16.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.sync16.kmcp/ # ------------------------------------------------------------------------------------- # FracMinhash/Scaled MinHash with d=5 input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # D = 5 # FracMinhash kmcp compute -I $input -O gtdb-r202-k21-n10-D5 -k 21 -D 5 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10-D5.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.2 kmcp index -j 32 -I gtdb-r202-k21-n10-D5 -O gtdb.minh5.kmcp -n 1 -f 0.2 \\ --log gtdb.minh5.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.minh5.kmcp/ # ------------------------------------------------------------------------------------- # Minimizer with W=5 input=gtdb # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks with 100bp overlap # k = 21 # W = 5 # Minimizer kmcp compute -I $input -O gtdb-r202-k21-n10-W5 -k 21 -W 5 -n 10 -l 100 -B plasmid \\ --log gtdb-r202-k21-n10-W5.log -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.2 kmcp index -j 32 -I gtdb-r202-k21-n10-W5 -O gtdb.mini5.kmcp -n 1 -f 0.2 \\ --log gtdb.mini5.kmcp.log # cp taxid and name mapping file to database directory cp taxid.map name.map gtdb.mini5.kmcp/","title":"GTDB"},{"location":"database/#refseq-viral-or-fungi","text":"Tools genome_updater (0.4.1) for downloading genomes from NCBI. Downloading viral and fungi sequences: name=fungi # name=viral # -k for dry-run # -i for fix time genome_updater.sh \\ -d \"refseq\"\\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"refseq-$name\" \\ -t 12 \\ -m -a -p # cd to 2021-09-30_19-35-19 # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- # for viral, only splitting into 5 chunks name=viral input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n5 \\ -k 21 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5/ -O refseq-viral.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.kmcp/ # ----------------------------------------------------------------- # for fungi name=fungi input=files.renamed # ------------- # all kmers kmcp compute -I $input -O refseq-$name-k21-n10 \\ -k 21 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10.log -j 32 --force kmcp index -I refseq-$name-k21-n10/ -O refseq-fungi.kmcp \\ -j 32 -f 0.3 -n 1 \\ --log refseq-fungi.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.kmcp/ Building database (k-mer sketches, for profiling on long-reads): # ----------------------------------------------------------------- # for viral, only splitting into 5 chunks name=viral input=files.renamed # --------------------------------------------- # here we compute Closed Syncmers with s=16 kmcp compute -I $input -O refseq-$name-k21-n5-S16 \\ -k 21 -S 16 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5-S16.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5-S16/ -O refseq-viral.sync16.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.sync16.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.sync16.kmcp/ # --------------------------------------------- # here we compute FracMinHash with D=5 kmcp compute -I $input -O refseq-$name-k21-n5-D5 \\ -k 21 -D 5 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5-D5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5-D5/ -O refseq-viral.minh5.kmcp \\ -j 32 -f 0.001 -n 3 -x 100K \\ --log refseq-viral.minh5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-viral.minh5.kmcp/ # ----------------------------------------------------------------- # for fungi name=fungi input=files.renamed # --------------------------------------------- # here we compute Closed Syncmers with s=16 kmcp compute -I $input -O refseq-$name-k21-n10-S16 \\ -k 21 -S 16 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10-S16.log -j 32 --force kmcp index -I refseq-$name-k21-n10-S16/ -O refseq-fungi.sync16.kmcp \\ -j 32 -f 0.2 -n 1 \\ --log refseq-fungi.sync16.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.sync16.kmcp/ # --------------------------------------------- # here we compute FracMinHash with D=5 kmcp compute -I $input -O refseq-$name-k21-n10-D5 \\ -k 21 -D 5 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10-D5.log -j 32 --force kmcp index -I refseq-$name-k21-n10-D5/ -O refseq-fungi.minh5.kmcp \\ -j 32 -f 0.2 -n 1 \\ --log refseq-fungi.minh5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.minh5.kmcp/ # --------------------------------------------- # here we compute Minimizer with W=5 kmcp compute -I $input -O refseq-$name-k21-n10-W5 \\ -k 21 -W 5 --seq-name-filter plasmid \\ --split-number 10 --split-overlap 100 \\ --log refseq-$name-k21-n10-W5.log -j 32 --force kmcp index -I refseq-$name-k21-n10-W5/ -O refseq-fungi.mini5.kmcp \\ -j 32 -f 0.2 -n 1 \\ --log refseq-fungi.mini5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map refseq-fungi.mini5.kmcp/","title":"RefSeq viral or fungi"},{"location":"database/#genbank-viral","text":"Tools genome_updater for downloading genomes from NCBI. Downloading viral sequences: name=viral # -k for dry-run # -i for fix # keep at most 5 genomes for a taxid: # genome_updater v0.4.1: -A 5 -c \"\" -l \"\" # genome_updater v0.2.5: -j taxids:5 -c \"all\" -l \"all\" time genome_updater.sh \\ -d \"genbank\"\\ -A 5 \\ -g $name \\ -c \"\" \\ -l \"\" \\ -f \"genomic.fna.gz\" \\ -o \"genbank-$name\" \\ -t 12 \\ -m -a -p # cd genbank-viral/2021-12-06_15-27-37/ # taxdump mkdir -p taxdump tar -zxvf taxdump.tar.gz -C taxdump # assembly accession -> taxid cut -f 1,6 assembly_summary.txt > taxid.map # assembly accession -> name cut -f 1,8 assembly_summary.txt > name.map # stats (optional) cat taxid.map \\ | csvtk freq -Ht -f 2 -nr \\ | taxonkit lineage -r -n -L --data-dir taxdump/ \\ | taxonkit reformat -I 1 -f '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}' --data-dir taxdump/ \\ | csvtk add-header -t -n 'taxid,count,name,rank,superkindom,phylum,class,order,family,genus,species' \\ > taxid.map.stats.tsv seqkit stats -T -j 12 --infile-list <(find files -name \"*.fna.gz\") > files.stats.tsv # ------------------------------------------------------ # create another directory linking to genome files input=files.renamed mkdir -p $input # create soft links cd $input find ../files -name \"*.fna.gz\" \\ | rush 'ln -s {}' cd .. # rename brename -R -p '^(\\w{3}_\\d{9}\\.\\d+).+' -r '$1.fna.gz' $input Keep at most 5 genomes for a taxid (optional) # ----------------------------------------------------------------- # keep at most 5 genomes for a taxid # backup cat taxid.map > taxid.all.map cat name.map > name.all.map # keep at most 5 ids for a taxid cat taxid.all.map \\ | csvtk sort -Ht -k 2:n -k 1:n \\ | csvtk uniq -Ht -f 2 -n 5 \\ > taxid.map cat name.all.map \\ | csvtk grep -Ht -P <(cut -f 1 taxid.map) \\ > name.map # organize files input=files.renamed output=files.renamed.slim mkdir -p $output cd $output find ../$input -name \"*.fna.gz\" \\ | csvtk mutate -Ht -p '/([^\\/]+).fna.gz' \\ | csvtk grep -Ht -f 2 -P <(cut -f 1 ../taxid.map) \\ | rush 'ln -s {1}' cd .. # check number of genomes ls $output | wc -l wc -l taxid.map Building database (all k-mers, for profiling on short-reads): # ----------------------------------------------------------------- # for viral, only splitting into 5 chunks name=viral input=files.renamed.slim # ---------------- # all kmers kmcp compute -I $input -O genbank-$name-k21-n5 \\ -k 21 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -I genbank-$name-k21-n5/ -O genbank-viral.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log genbank-viral.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.kmcp/ Building small databases (all k-mers, for profiling with a computer cluster or computer with limited RAM): name=genbank-viral input=files.renamed.slim find $input -name \"*.fna.gz\" > $input.files.txt # sort files by genome size, so we can split them into chunks with similar genome sizes cp $input.files.txt $input.files0.txt cat $input.files0.txt \\ | rush -k 'echo -e {}\"\\t\"$(seqkit stats -T {} | sed 1d | cut -f 5)' \\ > $input.files0.size.txt cat $input.files0.size.txt \\ | csvtk sort -Ht -k 2:nr \\ | csvtk cut -t -f 1 \\ > $input.files.txt # number of databases n=4 # split into $n chunks using round robin distribution split -n r/$n -d $input.files.txt $name.n$n- # create database for every chunks for f in $name.n$n-*; do echo $f kmcp compute -i $f -O $f-k21-n5 \\ -k 21 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log $f-k21-n5.log -j 24 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I $f-k21-n5/ -O $f.kmcp \\ -j 24 -f 0.05 -n 1 -x 100K -8 1M \\ --log $f.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map $f.kmcp/ done Building database (k-mer sketches, for profiling on long-reads): name=viral input=files.renamed.slim # ---------------------------------------------- # here we compute Closed Syncmers with s=16 kmcp compute -I $input -O genbank-$name-k21-n5-S16 \\ -k 21 -S 16 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5-S16.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I genbank-$name-k21-n5-S16/ -O genbank-viral.sync16.kmcp \\ -j 32 -f 0.001 -n 3 -x 50K -8 1M \\ --log genbank-viral.sync16.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.sync16.kmcp/ # ---------------------------------------------- # here we compute FracMinHash with D=5 kmcp compute -I $input -O genbank-$name-k21-n5-D5 \\ -k 21 -D 5 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5-D5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I genbank-$name-k21-n5-D5/ -O genbank-viral.minh5.kmcp \\ -j 32 -f 0.001 -n 3 -x 50K -8 1M \\ --log genbank-viral.minh5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.minh5.kmcp/ # ---------------------------------------------- # here we compute Minimizer with W=5 kmcp compute -I $input -O genbank-$name-k21-n5-W5 \\ -k 21 -W 5 --seq-name-filter plasmid \\ --split-number 5 --split-overlap 100 \\ --log genbank-$name-k21-n5-W5.log -j 32 --force # viral genomes are small: # using small false positive rate: 0.001 # using more hash functions: 3 kmcp index -I genbank-$name-k21-n5-W5/ -O genbank-viral.mini5.kmcp \\ -j 32 -f 0.001 -n 3 -x 50K -8 1M \\ --log genbank-viral.mini5.kmcp.log --force # cp taxid and name mapping file to database directory cp taxid.map name.map genbank-viral.mini5.kmcp/","title":"Genbank viral"},{"location":"database/#human-genome","text":"Downloading human genome file from CHM13 : # v1.1: wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.3_CHM13_T2T_v1.1/GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/914/755/GCA_009914755.4_T2T-CHM13v2.0/GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz Building database (all k-mers, < 6min): # v1.1: input=GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz # v2.0 input=GCA_009914755.4_T2T-CHM13v2.0_genomic.fna.gz # splitting human genome into 1024 chunks. # The regular expression '^(\\w{3}_\\d{9}\\.\\d+).+' is for extracting 'GCA_009914755.4' from the file name. kmcp compute $input -O human-chm13-k21-n1024 \\ --ref-name-regexp '^(\\w{3}_\\d{9}\\.\\d+).+' \\ -k 21 \\ --split-number 1024 --split-overlap 100 \\ --log human-chm13-k21-n1024.log -j 32 --force # using small false positive rate: 0.3 # using more hash functions: 1 kmcp index -I human-chm13-k21-n1024/ -O human-chm13.kmcp \\ -j 8 -f 0.3 -n 1 \\ --log human-chm13.kmcp.log --force # taxid.map echo -ne \"GCA_009914755.4\\t9606\\n\" > taxid.map # name.mapp echo -ne \"GCA_009914755.4\\tHomo sapiens isolate CHM13\\n\" > name.map # cp name mapping file to database directory cp taxid.map name.map human-chm13.kmcp/","title":"Human genome"},{"location":"database/#refseq-plasmid","text":"Downloading plasmid sequences: wget https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ cat index.html \\ | perl -ne 'next unless /\"(plasmid.*genomic.fna.gz)\"/; print \"$1\\n\"' > files.txt baseurl=https://ftp.ncbi.nlm.nih.gov/refseq/release/plasmid/ outdir=archives; mkdir -p $outdir cat files.txt \\ | rush -j 12 -v b=$baseurl -v out=$outdir \\ 'wget -c {b}/{} -o /dev/null -O {out}/{}' -c -C download.rush # name mapping seqkit seq -n archives/*.fna.gz \\ | sed -E 's/\\s+/\\t/' \\ > name.map # length stats seqkit stats -b -a -j 12 -T archives/*.fna.gz > archives.stats.tsv # split to individual files for f in archives/*.fna.gz; do \\ seqkit split2 -s 1 --quiet $f -O refseq-plasmid; \\ done # rename files with sequence ID find refseq-plasmid -name \"*.fna.gz\" \\ | rush 'mv {} {/}/$(seqkit seq -ni {}).fna.gz' Building database (all k-mers): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k21-n5 \\ -k 21 --circular \\ --split-number 5 --split-overlap 100 \\ --log refseq-$name-k21-n5.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k21-n5/ -O refseq-$name.kmcp \\ -j 32 -f 0.01 -n 3 -x 200K -X 1024 \\ --log refseq-$name.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.kmcp/ Building database (FracMinHash/Scaled MinHash): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-D10 \\ -k 31 --circular --scale 10 \\ --log refseq-$name-k31-D10.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-D10/ -O refseq-$name.minhash.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.minhash.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.minhash.kmcp/ Building database (Closed Syncmer): name=plasmid input=refseq-plasmid kmcp compute -I $input -O refseq-$name-k31-S21 \\ -k 31 --circular --syncmer-s 21 \\ --log refseq-$name-k31-S21.log -j 32 --force # using small false positive rate: 0.01 # using more hash functions: 3 kmcp index -I refseq-$name-k31-S21/ -O refseq-$name.syncmer.kmcp \\ -j 8 -f 0.01 -n 3 -x 100K -X 1024 \\ --log refseq-$name.syncmer.kmcp.log --force # cp name mapping file to database directory cp name.map refseq-$name.syncmer.kmcp/","title":"Refseq plasmid"},{"location":"database/#building-databases-prokaryotic-genome-collections","text":"","title":"Building databases (prokaryotic genome collections)"},{"location":"database/#humgut-30691-clusters","text":"HumGut is a comprehensive Human Gut prokaryotic genomes collection filtered by metagenome data. In this work, we aimed to create a collection of the most prevalent healthy human gut prokaryotic genomes, to be used as a reference database, including both MAGs from the human gut and ordinary RefSeq genomes. Dataset Genomes: HumGut.tar.gz Metadata: HumGut.tsv Taxdump files: NCBI taxonomy: ncbi_names.dump and ncbi_nodes.dump GTDB taxomomy: gtdb_names.dump and gtdb_nodes.dump Uncompressing and renaming: # uncompress tar -zxvf HumGut.tar.gz # taxdump files mkdir taxdump mv ncbi_names.dmp taxdump/names.dmp mv ncbi_nodes.dmp taxdump/nodes.dmp Mapping file: # assembly accession -> taxid cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid.map # assembly accession -> name cat HumGut.tsv \\ | csvtk cut -t -f genome_file,ncbi_organism_name \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > name.map Stats: # ------------------------------------------------------------- # NCBI taxonomy # number of species/strains cat taxid.map \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.stats species 23604 strain 6816 subspecies 161 no rank 69 serotype 38 genus 3 # number of unique species/strains cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-ncbi -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats; species 1240 strain 458 subspecies 15 genus 1 no rank 1 serotype 1 # ------------------------------------------------------------- # GTDB taxonomy cat HumGut.tsv \\ | csvtk cut -t -f genome_file,gtdbtk_tax_id \\ | csvtk replace -f genome_file -t -p '\\.fna\\.gz$' \\ | csvtk del-header \\ > taxid-gtdb.map cat taxid-gtdb.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump-gtdb -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid-gtdb.map.uniq.stats species 2810 genus 434 family 52 order 12 class 2 Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I fna/ -k 21 -n 10 -B plasmid -O humgut-k21-n10 -j 32 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I humgut-k21-n10 -O humgut.kmcp -n 1 -f 0.3 cp taxid.map taxid-gtdb.map humgut.kmcp/","title":"HumGut (30,691 clusters)"},{"location":"database/#progenomes2-12000-species","text":"proGenomes v2.1 provides 84,096 consistently annotated bacterial and archaeal genomes from over 12000 species. Dataset: Representative genomes: contigs.representatives.fasta.gz NCBI taxonomy: proGenomes2.1_specI_lineageNCBI.tab Organize sequences: # download wget https://progenomes.embl.de/data/repGenomes/freeze12.contigs.representatives.fasta.gz # unzip for splitting by genome ID time seqkit seq freeze12.contigs.representatives.fasta.gz -o freeze12.contigs.representatives.fasta # split by genome ID seqkit split --by-id --two-pass --id-regexp '(^\\d+\\.\\w+)\\.' freeze12.contigs.representatives.fasta --out-dir genomes # batch rename brename -p '^.+id_' genomes # compress genomes for saving space find genomes/ -name \"*.fasta\" | rush 'gzip {}' Mapping file: # download wget https://progenomes.embl.de/data/proGenomes2.1_specI_lineageNCBI.tab ls genomes/ | sed 's/.fasta.gz//' > id.txt # id -> taxid cut -f 1 proGenomes2.1_specI_lineageNCBI.tab \\ | awk -F . '{print $0\"\\t\"$1}' \\ | csvtk grep -Ht -P id.txt \\ > taxid.map cat taxid.map \\ | csvtk uniq -Ht -f 2 \\ | taxonkit lineage --data-dir taxdump -i 2 -r -n -L \\ | csvtk freq -Ht -f 4 -nr \\ | csvtk pretty -H -t \\ | tee taxid.map.uniq.stats species 8088 strain 3262 subspecies 37 isolate 25 no rank 16 forma specialis 14 biotype 1 # use the taxonomy verion of the refseq-virus: 2021-10-01 # id -> name cat taxid.map \\ | taxonkit lineage --data-dir taxdump -i 2 -n -L \\ | cut -f 1,3 \\ > name.map Building database: # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks # k = 21 kmcp compute -I genomes/ -k 21 -n 10 -B plasmid -O progenomes-k21-n10 --force # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I progenomes-k21-n10 -O progenomes.kmcp -n 1 -f 0.3 cp taxid.map name.map progenomes.kmcp/","title":"proGenomes2 (12,000 species)"},{"location":"database/#building-databases-viral-genome-collections","text":"","title":"Building databases (viral genome collections)"},{"location":"database/#mgv-54118-species","text":"Bacteriophages have important roles in the ecology of the human gut microbiome but are under-represented in reference data- bases. To address this problem, we assembled the Metagenomic Gut Virus catalogue that comprises 189,680 viral genomes from 11,810 publicly available human stool metagenomes. Over 75% of genomes represent double-stranded DNA phages that infect members of the Bacteroidia and Clostridia classes. Based on sequence clustering we identified 54,118 candidate viral spe- cies, 92% of which were not found in existing databases. The Metagenomic Gut Virus catalogue improves detection of viruses in stool metagenomes and accounts for nearly 40% of CRISPR spacers found in human gut Bacteria and Archaea. We also pro- duced a catalogue of 459,375 viral protein clusters to explore the functional potential of the gut virome. This revealed tens of thousands of diversity-generating retroelements, which use error-prone reverse transcription to mutate target genes and may be involved in the molecular arms race between phages and their bacterial hosts. https://doi.org/10.1038/s41564-021-00928-6 https://portal.nersc.gov/MGV/ Basic information (optional) $ seqkit stats mgv_contigs.fna file format type num_seqs sum_len min_len avg_len max_len mgv_contigs.fna FASTA DNA 189,680 8,803,222,510 1,244 46,410.9 553,716 # Genome completeness Complete: n=26,030 >90% complete: n=53,220 50-90% complete: n=110,430 $ seqkit seq -n mgv_contigs.fna | head -n 3 MGV-GENOME-0364295 MGV-GENOME-0364296 MGV-GENOME-0364303 Stats (optional) cat mgv_contig_info.tsv \\ | csvtk cut -t -f completeness \\ | csvtk plot hist -o completeness.hist.png # Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness == 100' \\ | csvtk nrows 32577 # >90% complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90' \\ | csvtk nrows 78814 # < 79250 # checkv_quality $ cat mgv_contig_info.tsv \\ | csvtk cut -t -f checkv_quality \\ | csvtk freq -t -nr | more checkv_quality frequency Medium-quality 110430 High-quality 53220 Complete 26030 # >90% complete && checkv_quality == High-quality/Complete $ cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk nrows 78813 Stats of high-quality genomes (optional) # high-quality genomes cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ > mgv.hq.tsv # number of families cat mgv.hq.tsv \\ | csvtk freq -t -f ictv_family -nr \\ | csvtk nrow -t 19 # number of species cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id -nr \\ | csvtk nrow -t 26779 # baltimore cat mgv.hq.tsv \\ | csvtk freq -t -f baltimore -nr \\ | csvtk pretty -t baltimore frequency --------- --------- dsDNA 76527 ssDNA 2215 NULL 62 DNA 7 dsDNA-RT 1 ssRNA-RT 1 # prophage? cat mgv.hq.tsv \\ | csvtk freq -t -f prophage -nr \\ | csvtk pretty -t prophage frequency -------- --------- No 58366 Yes 20447 # species with both prophage and lytic phages cat mgv.hq.tsv \\ | csvtk freq -t -f votu_id,prophage \\ | csvtk freq -t -f votu_id \\ | csvtk filter2 -t -f '$frequency > 1' \\ | csvtk nrow -t 2745 Prepare genomes: # ID cat mgv_contig_info.tsv \\ | csvtk filter2 -t -f '$completeness >= 90 && $checkv_quality != \"Medium-quality\"' \\ | csvtk cut -t -f contig_id \\ | csvtk del-header \\ > mgv.hq.tsv.id # extract sequences seqkit grep -f mgv.hq.tsv.id mgv_contigs.fna -o mgv.hq.fasta.gz # split into files with one genome seqkit split2 -s 1 mgv.hq.fasta.gz -O mgv # rename with find mgv/ -name \"*.fasta.gz\" \\ | rush -j 20 'mv {} {/}/$(seqkit seq -ni {}).fa.gz' Create taxdump files and taxid.map with taxonkit (version >= v0.12.0): cat mgv_contig_info.tsv \\ | csvtk cut -t -f ictv_order,ictv_family,ictv_genus,votu_id,contig_id \\ | csvtk del-header \\ > mgv.taxonomy.tsv taxonkit create-taxdump mgv.taxonomy.tsv --out-dir mgv-taxdump \\ --force -A 5 -R order,family,genus,species cp mgv-taxdump/taxid.map . # name.map cat mgv-taxdump/taxid.map \\ | taxonkit lineage --data-dir mgv-taxdump/ -i 2 \\ | cut -f 1,3 \\ > name.map head -n 5 name.map MGV-GENOME-0364295 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364296 Caudovirales;crAss-phage;OTU-61123 MGV-GENOME-0364303 Caudovirales;crAss-phage;OTU-05782 MGV-GENOME-0364311 Caudovirales;crAss-phage;OTU-01114 MGV-GENOME-0364312 Caudovirales;crAss-phage;OTU-23935 Building database: # compute k-mers # reference genomes are split into 5 chunks # k = 21 kmcp compute -I mgv/ -k 21 -n 5 -O mgv-k21-n5 --force # viral genomes are small: # using small false positive rate: 0.05 # still using one hash function: 1 kmcp index -j 32 -I mgv-k21-n5 -O mgv.kmcp \\ -j 32 -f 0.05 -n 1 -x 100K -8 1M \\ --log mgv.kmcp.log cp taxid.map name.map mgv.kmcp/","title":"MGV (54,118 species)"},{"location":"database/#building-custom-databases","text":"Files: Genome files (Gzip-compressed) FASTA/Q format. One genome per file with the reference identifier in the file name . TaxId mapping file (for metagenomic profiling) Two-column (reference identifier and TaxId) tab-delimited. NCBI taxonomy dump files (for metagenomic profiling). You can use taxonkit create-taxdump to create NCBI-style taxdump files for custom genome collections, which also generates a TaxId mapping file. names.dmp nodes.dmp merged.dmp (optional) delnodes.dmp (optional) Tools: kmcp for metagenomic profiling. rush for executing jobs in parallel. brename for batching renaming files (optional) Memory notes : By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database.","title":"Building custom databases"},{"location":"database/#step-1-computing-k-mers","text":"Input: Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, Or a directory containing sequence files via the flag -I/--in-dir , with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp . The default pattern matches files with extension of fa , fasta , fna , fq , fastq , fa.gz , fasta.gz , fna.gz , fq.gz or fastq.gz . You may rename the sequence files for convenience using brename . because the sequence/genome identifier in the index and search results would be: For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. For splitting sequence mode (see details below): same to 1). For computing k-mers for each sequence: the sequence identifier. Unwanted sequences like plasmid sequences can be filtered out by the name via regular expression(s) ( -B/--seq-name-filter ). How to compute k-mers : By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. It also supports splitting sequences into chunks, this could increase the specificity in profiling results at the cost of a slower searching speed . Splitting sequences : Sequences can be split into chunks by a chunk size ( -s/--split-size ) or number of chunks ( -n/--split-number ) with overlap ( -l/--split-overlap ). In this mode, the sequences of each genome should be saved in an individual file . When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter ) in a sequence file are concatenated with k-1 Ns before splitting . Both sequence/reference IDs and chunks indices are saved for later use, in form of meta/description data in .unik files, and will be reported in kmcp search results. Metadata : Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. When parsing whole sequence files or splitting by number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp , e.g., ^(\\w{3}_\\d{9}\\.\\d+) for RefSeq assembly accessions . Multiple sizes of k-mers are supported, but a single k-mer size is good enough. Supported k-mer (sketches) types : K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): Scaled MinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Output : All outputted .unik files are saved in ${outdir} , with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree /xxx/yyy/zzz/ is built for > 1000 output files. For splitting sequence mode ( --split-size > 0 or --split-number > 0 ), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik A summary file ( ${outdir}/_info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . Performance tips : Decrease value of -j/--threads for data in hard disk drives (HDD) to reduce I/O pressure. Commands ( usage ): # compute k-mers # sequence containing \"plasmid\" in name are ignored, # reference genomes are split into 10 chunks, # k = 21 kmcp compute --in-dir refs/ \\ --kmer 21 \\ --split-number 10 \\ --split-overlap 100 \\ --seq-name-filter plasmid \\ --ref-name-regexp '(.+).fasta.gz' \\ --out-dir refs-k21-n10 # demo output 13:11:13.397 [INFO] kmcp v0.8.0 13:11:13.397 [INFO] https://github.com/shenwei356/kmcp 13:11:13.397 [INFO] 13:11:13.397 [INFO] checking input files ... 13:11:13.398 [INFO] 9 input file(s) given 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] input and output: 13:11:13.398 [INFO] input directory: refs/ 13:11:13.398 [INFO] regular expression of input files: (?i)\\.(f[aq](st[aq])?|fna)(.gz)?$ 13:11:13.398 [INFO] *regular expression for extracting reference name from file name: (?i)(.+).fasta.gz 13:11:13.398 [INFO] *regular expressions for filtering out sequences: [plasmid] 13:11:13.398 [INFO] output directory: refs-k21-n10 13:11:13.398 [INFO] 13:11:13.398 [INFO] sequences splitting: true 13:11:13.398 [INFO] split parts: 10, overlap: 100 bp 13:11:13.398 [INFO] 13:11:13.398 [INFO] k-mer (sketches) computing: 13:11:13.398 [INFO] k-mer size(s): 21 13:11:13.398 [INFO] circular genome: false 13:11:13.398 [INFO] saving exact number of k-mers: true 13:11:13.398 [INFO] 13:11:13.398 [INFO] -------------------- [main parameters] -------------------- 13:11:13.398 [INFO] 13:11:13.398 [INFO] computing ... processed files: 9 / 9 [======================================] ETA: 0s. done 13:11:13.845 [INFO] 13:11:13.845 [INFO] elapsed time: 453.870411ms 13:11:13.845 [INFO] A summary file ( _info.txt ) is generated for later use. Users need to check if the reference IDs (column name ) are what supposed to be . #path name chunkIdx idxNum genomeSize kmers refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_0.unik NC_010655.1 0 10 2664102 264247 refs-k21-n10/672/060/672/NC_010655.1.fasta.gz/NC_010655.1-chunk_1.unik NC_010655.1 1 10 2664102 266237 refs-k21-n10/595/162/595/NC_012971.2.fasta.gz/NC_012971.2-chunk_0.unik NC_012971.2 0 10 4558953 450494 refs-k21-n10/292/039/292/NC_000913.3.fasta.gz/NC_000913.3-chunk_0.unik NC_000913.3 0 10 4641652 459277 refs-k21-n10/934/859/934/NC_013654.1.fasta.gz/NC_013654.1-chunk_0.unik NC_013654.1 0 10 4717338 470575 Meta data in the .unik file can be showed using kmcp utils unik-info : kmcp utils unik-info refs-k21-n10/072/380/072/NZ_CP028116.1.fasta.gz/NZ_CP028116.1-chunk_0.unik -a","title":"Step 1. Computing k-mers"},{"location":"database/#step-2-building-databases","text":"KMCP builds index for k-mers (sketches) with a modified Compact Bit-sliced Signature Index ( COBS ). We completely rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed (check benchmark ). Input : The output directory generated by kmcp compute . Database size and searching accuracy : Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size . -f/--false-positive-rate : the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. -n/--num-hash : large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. The value of block size -b/--block-size is better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 Use flag -x/--block-sizeX-kmers-t , -8/--block-size8-kmers-t , and -1/--block-size1-kmers-t to separately create indexes for inputs with huge number of k-mers, for precise control of database size. Taxonomy data : No taxonomy data are included in the database. Taxonomy information are only needed in kmcp profile . Performance tips : The umber of blocks ( .uniki files) is better to be smaller than or equal to the number of CPU cores for faster searching speed. We can set -j/--threads to control the blocks number . When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. #threads files are simultaneously opened, and the max number of opened files is limited by the flag -F/--max-open-files . You may use a small value of -F/--max-open-files for hard disk drive storages. When the database is used in a new computer with more CPU cores, kmcp search could automatically scale to utilize as many cores as possible. Commands ( usage ): # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -I refs-k21-n10 \\ --threads 32 \\ --num-hash 1 \\ --false-positive-rate 0.3 \\ --out-dir refs.kmcp # demo output 22:44:17.710 [INFO] kmcp v0.8.0 22:44:17.710 [INFO] https://github.com/shenwei356/kmcp 22:44:17.710 [INFO] 22:44:17.710 [INFO] loading .unik file infos from file: refs-k21-n10/_info.txt 22:44:17.716 [INFO] 90 cached file infos loaded 22:44:17.716 [INFO] 22:44:17.716 [INFO] -------------------- [main parameters] -------------------- 22:44:17.716 [INFO] number of hashes: 1 22:44:17.716 [INFO] false positive rate: 0.300000 22:44:17.717 [INFO] k-mer size(s): 21 22:44:17.717 [INFO] split seqequence size: 0, overlap: 0 22:44:17.717 [INFO] block-sizeX-kmers-t: 10.00 M 22:44:17.717 [INFO] block-sizeX : 256.00 22:44:17.717 [INFO] block-size8-kmers-t: 20.00 M 22:44:17.717 [INFO] block-size1-kmers-t: 200.00 M 22:44:17.717 [INFO] -------------------- [main parameters] -------------------- 22:44:17.717 [INFO] 22:44:17.717 [INFO] building index ... 22:44:17.726 [WARN] ignore -X/--block-size (256) which is >= -b/--block-size (8) 22:44:17.726 [INFO] 22:44:17.726 [INFO] block size: 8 22:44:17.726 [INFO] number of index files: 32 (may be more) 22:44:17.726 [INFO] 22:44:17.726 [block #001] 1 / 1 100 % 22:44:17.726 [block #002] 1 / 1 100 % 22:44:17.726 [block #003] 1 / 1 100 % 22:44:17.726 [block #004] 1 / 1 100 % 22:44:17.726 [block #005] 1 / 1 100 % 22:44:17.726 [block #006] 1 / 1 100 % 22:44:17.726 [block #007] 1 / 1 100 % 22:44:17.726 [block #008] 1 / 1 100 % 22:44:17.726 [block #009] 1 / 1 100 % 22:44:17.727 [block #010] 1 / 1 100 % 22:44:17.727 [block #011] 1 / 1 100 % 22:44:17.727 [block #012] 1 / 1 100 % [saved index files] 12 / 12 ETA: 0s. done 22:44:17.933 [INFO] 22:44:17.933 [INFO] kmcp database with 42713316 k-mers saved to refs.kmcp 22:44:17.933 [INFO] total file size: 15.66 MB 22:44:17.933 [INFO] total index files: 12 22:44:17.933 [INFO] 22:44:17.933 [INFO] elapsed time: 223.524128ms 22:44:17.933 [INFO] Output: refs.kmcp/ \u2514\u2500\u2500 R001 \u251c\u2500\u2500 _block001.uniki \u251c\u2500\u2500 _block002.uniki \u251c\u2500\u2500 _block003.uniki \u251c\u2500\u2500 _block004.uniki \u251c\u2500\u2500 _block005.uniki \u251c\u2500\u2500 _block006.uniki \u251c\u2500\u2500 _block007.uniki \u251c\u2500\u2500 _block008.uniki \u251c\u2500\u2500 _block009.uniki \u251c\u2500\u2500 _block010.uniki \u251c\u2500\u2500 _block011.uniki \u251c\u2500\u2500 _block012.uniki \u251c\u2500\u2500 __db.yml \u2514\u2500\u2500 __name_mapping.tsv __db.yml contains configuration of the database, and _blockXXX.uniki are index files. kmcp utils index-info could show the basic information of index files: kmcp utils index-info refs.kmcp/R001/_block001.uniki file k canonical num-hashes num-sigs num-names refs.kmcp/R001/_block001.uniki 21 true 1 746442 8 What's next? Check the tutorials .","title":"Step 2. Building databases"},{"location":"download/","text":"Download KMCP is implemented in Go programming language, statically-linked executable binary files are freely available . SIMD instructions support SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters. Current Version v0.8.3 - 2022-08-15 kmcp : fix compling from source for ARM architectures. #17 search : fix panic for paired-end reads with read2 shorter than the value of --min-query-len . #10 fix log. #8 new flag -f/--max-fpr : maximal false positive rate of a query (default 0.05). It reduces outputting unnecessary when searching with a low minimal query coverage ( -t/--min-query-cov ). profile : recommend to use the flag --no-amb-corr to disable ambiguous reads correction when >= 1000 candidates are detected. fix loging when using --level strain and no taxonomy given. Links OS Arch File, \u4e2d\u56fd\u955c\u50cf Download Count Linux 64-bit kmcp_linux_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf Linux arm64 kmcp_linux_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS 64-bit kmcp_darwin_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS arm64 kmcp_darwin_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf Windows 64-bit kmcp_windows_amd64.exe.tar.gz , \u4e2d\u56fd\u955c\u50cf Notes: please open an issue to request binaries for other platforms or compile from the source . run kmcp version to check update !!! run kmcp autocompletion to update shell autocompletion script !!! Installation Method 1: Install using conda conda install -c bioconda kmcp Method 2: Download binaries Download the compressed executable file of your operating system, and decompress it with tar -zxvf *.tar.gz command or other tools. And then: For Linux-like systems If you have root privilege, simply copy it to /usr/local/bin : sudo cp kmcp /usr/local/bin/ Or copy to anywhere in the environment variable PATH : mkdir -p $HOME/bin/; cp kmcp $HOME/bin/ For Windows , just copy kmcp.exe to C:\\WINDOWS\\system32 . Method 3: Compile from source Install go wget https://go.dev/dl/go1.17.13.linux-amd64.tar.gz tar -zxf go1.17.13.linux-amd64.tar.gz -C $HOME/ # or # echo \"export PATH=$PATH:$HOME/go/bin\" >> ~/.bashrc # source ~/.bashrc export PATH=$PATH:$HOME/go/bin Compile KMCP # ------------- the latest stable version ------------- go get -v -u github.com/shenwei356/kmcp/kmcp # The executable binary file is located in: # ~/go/bin/kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ~/go/bin/kmcp $HOME/bin/ # --------------- the devlopment version -------------- git clone https://github.com/shenwei356/kmcp cd kmcp/kmcp/ go build # The executable binary file is located in: # ./kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ./kmcp $HOME/bin/ Shell-completion Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish Release History v0.8.2 - 2022-03-26 search : flag -g/--query-whole-file : fix panic for invalid input. add gaps of k-1 bp before concatatenating seqs. add warning for invalid input. profile : allow modifying parts of parameters in preset profiling modes . #5 decrease thresholds of minimal reads and unique reads in preset profiling modes 1 and 2 for low coverage sequence data. the profiling results generated with mode 3 in the manuscript are not affected . v0.8.1 - 2022-03-07 update help message, show common usages, add examples, add notes to important options. v0.8.0 - 2022-02-24 commands: new command utils cov2simi : Convert k-mer coverage to sequence similarity. new command utils query-fpr : Compute the maximal false positive rate of a query. compute : update doc. add flags compatibility check. search : output the false positive rate of each match, rather than the FPR upper bound of the query . this could save some short queries with high similarity. change default values of reads filter, because clinical data contain many short reads . -c/--min-uniq-reads : 30 -> 10 . -m/--min-query-len : 70 -> 30 . update doc. profile : rename flags: --keep-main-matches -> --keep-main-matches . --keep-perfect-match -> --keep-perfect-matches . change default values: --max-qcov-gap : 0.2 -> 0.4 . mode 0 (pathogen detection): switch on flag --keep-main-matches use --max-qcov-gap 0.4 update doc. v0.7.1 - 2022-02-08 profile : new flag --metaphlan-report-version and the default value is 3 . #4 column name renamed: from fragsFrac , fragsRelDepth , fragsRelDepthStd to chunksFrac , chunksRelDepth , chunksRelDepthStd . fix computation of chunksRelDepth . slightly improve sensitivity for -m 0 . v0.7.0 - 2022-01-24 commands: new command utils filter : Filter search results and find species-specific queries. new command utils merge-regions : Merge species/assembly-specific regions. rename info to utils index-info . compute : skip k-mer containing Ns. when splitting genome into fragments, sequences are concatenated with k-1 'N's instead of directly concatenation. It eliminates fake k-mers at the concatenation position. set default value for flag -N/--ref-name-regexp : (?i)(.+)\\.(f[aq](st[aq])?|fna)(.gz)?$ . fix a rare bug when splitting FASTQ files. search : support searching with paired-end reads which has a higher specificity and a lower sensitivity. A flag --try-se is added for search read1/read2 when the paired end reads have no hits. fix matches order of a query. fix queries with many Ns. change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-scores from 5 to 0 , i.e., keep all matches by default. new flag -w/--load-whole-db : load all index files into memory. 10-25% faster. better log. merge : fix adding up hits . fix bug of incorrect order, reduce memory usage. support one input file. profile : change analysis workflow, using 4 stages. output format change: new column coverage , fragsRelDepth and fragsRelDepthStd . change default file extension of binning file. check if the taxid of a target is given by taxid mapping file. automatically switch to the new taxid for a merged one. change computation of score . new flag -d/--max-frags-depth-stdev . new option -m/--mode . change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-qcovs from 5 to 0 (keep all matches). change default value of falg -f/--max-fpr from 0.01 to 0.05 . change default value of flag -H/--min-hic-ureads-qcov from 0.8 to 0.75 (similarity ~98% ). faster search result parsing. v0.6.0 - 2021-08-13 new command: merge : merge search results from multiple databases. compute : fix splitting very short genomes. remove flag -e/--exact-number , making it default. index : do not roundup sizes of indexes. The searching speed is not affected and even faster due to optimization of search command. use three k-mers thresholds to control index file size. better control of cocurrency number and better progress bar. do not support RAMBO index anymore. search : 1.37X speedup, and faster for database with two or more hash functions. new flag -S/--do-not-sort . profile : fix a nil pointer bug when no taxid mapping data given. fix number of ureads. new flag -m/--keep-main-matches and --max-score-gap v0.5.0 - 2021-06-24 compute : support multiple sizes of k-mer. fix bug of --by-seq . more log. index : default block size is computed by -j/--threads instead of number of CPUs. search : show real-time processing speed. new flag -g/--query-whole-file . new flag -u/--kmer-dedup-threshold . new flag -m/--min-query-len . increase speed for database with mulitple hashes. profile : better decision of the existence of a reference. new flag -B/--binning-result for output reads binning result. new flag -m/--norm-abund . v0.4.0 - 2021-04-08 new command: profile for generating taxonomic profile from search result. compute : new flag -B/--seq-name-filter for filtering out unwanted sequences like plasmid. new flag -N/--ref-name-regexp for extracting reference name from sequence file. search : change default threshold value. new flag -n/--keep-top-scores for keeping matches with the top N score. v0.3.0 - 2021-03-16 use --quiet to replace --verbose , making printing log info default. search : fix computing intersetion between repeats. fix closing mmap on Windows. change output format and add Jaccard Index. speedup by parallelizing name mapping and database closing. flush result immediately. keep the output order by default compute : change default file regexp for matching .fna files. autocompletion : support bash, zsh, fish, powershell. v0.2.1 - 2020-12-31 index : reduce memory occupation. v0.2.0 - 2020-12-30 Add support of RAMBO like indexing. Limit to only one input database. Change output format. v0.1.0 - 2020-xx-xx First release with basic function.","title":"Download"},{"location":"download/#download","text":"KMCP is implemented in Go programming language, statically-linked executable binary files are freely available .","title":"Download"},{"location":"download/#simd-instructions-support","text":"SIMD extensions including AVX512 , AVX2 , SSE2 are sequentially detected and used in two packages for better searching performance. pand , for accelerating searching on databases constructed with multiple hash functions. pospop , for batch counting matched k-mers in bloom filters.","title":"SIMD instructions support"},{"location":"download/#current-version","text":"","title":"Current Version"},{"location":"download/#v083-2022-08-15","text":"kmcp : fix compling from source for ARM architectures. #17 search : fix panic for paired-end reads with read2 shorter than the value of --min-query-len . #10 fix log. #8 new flag -f/--max-fpr : maximal false positive rate of a query (default 0.05). It reduces outputting unnecessary when searching with a low minimal query coverage ( -t/--min-query-cov ). profile : recommend to use the flag --no-amb-corr to disable ambiguous reads correction when >= 1000 candidates are detected. fix loging when using --level strain and no taxonomy given.","title":"v0.8.3 - 2022-08-15"},{"location":"download/#links","text":"OS Arch File, \u4e2d\u56fd\u955c\u50cf Download Count Linux 64-bit kmcp_linux_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf Linux arm64 kmcp_linux_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS 64-bit kmcp_darwin_amd64.tar.gz , \u4e2d\u56fd\u955c\u50cf macOS arm64 kmcp_darwin_arm64.tar.gz , \u4e2d\u56fd\u955c\u50cf Windows 64-bit kmcp_windows_amd64.exe.tar.gz , \u4e2d\u56fd\u955c\u50cf Notes: please open an issue to request binaries for other platforms or compile from the source . run kmcp version to check update !!! run kmcp autocompletion to update shell autocompletion script !!!","title":"Links"},{"location":"download/#installation","text":"","title":"Installation"},{"location":"download/#method-1-install-using-conda","text":"conda install -c bioconda kmcp","title":"Method 1: Install using conda"},{"location":"download/#method-2-download-binaries","text":"Download the compressed executable file of your operating system, and decompress it with tar -zxvf *.tar.gz command or other tools. And then: For Linux-like systems If you have root privilege, simply copy it to /usr/local/bin : sudo cp kmcp /usr/local/bin/ Or copy to anywhere in the environment variable PATH : mkdir -p $HOME/bin/; cp kmcp $HOME/bin/ For Windows , just copy kmcp.exe to C:\\WINDOWS\\system32 .","title":"Method 2: Download binaries"},{"location":"download/#method-3-compile-from-source","text":"Install go wget https://go.dev/dl/go1.17.13.linux-amd64.tar.gz tar -zxf go1.17.13.linux-amd64.tar.gz -C $HOME/ # or # echo \"export PATH=$PATH:$HOME/go/bin\" >> ~/.bashrc # source ~/.bashrc export PATH=$PATH:$HOME/go/bin Compile KMCP # ------------- the latest stable version ------------- go get -v -u github.com/shenwei356/kmcp/kmcp # The executable binary file is located in: # ~/go/bin/kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ~/go/bin/kmcp $HOME/bin/ # --------------- the devlopment version -------------- git clone https://github.com/shenwei356/kmcp cd kmcp/kmcp/ go build # The executable binary file is located in: # ./kmcp # You can also move it to anywhere in the $PATH mkdir -p $HOME/bin cp ./kmcp $HOME/bin/","title":"Method 3: Compile from source"},{"location":"download/#shell-completion","text":"Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish","title":"Shell-completion"},{"location":"download/#release-history","text":"","title":"Release History"},{"location":"download/#v082-2022-03-26","text":"search : flag -g/--query-whole-file : fix panic for invalid input. add gaps of k-1 bp before concatatenating seqs. add warning for invalid input. profile : allow modifying parts of parameters in preset profiling modes . #5 decrease thresholds of minimal reads and unique reads in preset profiling modes 1 and 2 for low coverage sequence data. the profiling results generated with mode 3 in the manuscript are not affected .","title":"v0.8.2 - 2022-03-26"},{"location":"download/#v081-2022-03-07","text":"update help message, show common usages, add examples, add notes to important options.","title":"v0.8.1 - 2022-03-07"},{"location":"download/#v080-2022-02-24","text":"commands: new command utils cov2simi : Convert k-mer coverage to sequence similarity. new command utils query-fpr : Compute the maximal false positive rate of a query. compute : update doc. add flags compatibility check. search : output the false positive rate of each match, rather than the FPR upper bound of the query . this could save some short queries with high similarity. change default values of reads filter, because clinical data contain many short reads . -c/--min-uniq-reads : 30 -> 10 . -m/--min-query-len : 70 -> 30 . update doc. profile : rename flags: --keep-main-matches -> --keep-main-matches . --keep-perfect-match -> --keep-perfect-matches . change default values: --max-qcov-gap : 0.2 -> 0.4 . mode 0 (pathogen detection): switch on flag --keep-main-matches use --max-qcov-gap 0.4 update doc.","title":"v0.8.0 - 2022-02-24"},{"location":"download/#v071-2022-02-08","text":"profile : new flag --metaphlan-report-version and the default value is 3 . #4 column name renamed: from fragsFrac , fragsRelDepth , fragsRelDepthStd to chunksFrac , chunksRelDepth , chunksRelDepthStd . fix computation of chunksRelDepth . slightly improve sensitivity for -m 0 .","title":"v0.7.1 - 2022-02-08"},{"location":"download/#v070-2022-01-24","text":"commands: new command utils filter : Filter search results and find species-specific queries. new command utils merge-regions : Merge species/assembly-specific regions. rename info to utils index-info . compute : skip k-mer containing Ns. when splitting genome into fragments, sequences are concatenated with k-1 'N's instead of directly concatenation. It eliminates fake k-mers at the concatenation position. set default value for flag -N/--ref-name-regexp : (?i)(.+)\\.(f[aq](st[aq])?|fna)(.gz)?$ . fix a rare bug when splitting FASTQ files. search : support searching with paired-end reads which has a higher specificity and a lower sensitivity. A flag --try-se is added for search read1/read2 when the paired end reads have no hits. fix matches order of a query. fix queries with many Ns. change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-scores from 5 to 0 , i.e., keep all matches by default. new flag -w/--load-whole-db : load all index files into memory. 10-25% faster. better log. merge : fix adding up hits . fix bug of incorrect order, reduce memory usage. support one input file. profile : change analysis workflow, using 4 stages. output format change: new column coverage , fragsRelDepth and fragsRelDepthStd . change default file extension of binning file. check if the taxid of a target is given by taxid mapping file. automatically switch to the new taxid for a merged one. change computation of score . new flag -d/--max-frags-depth-stdev . new option -m/--mode . change default value of flag -t/--min-query-qcov from 0.6 to 0.55 (similarity ~96.5% ). change default value of flag -n/--keep-top-qcovs from 5 to 0 (keep all matches). change default value of falg -f/--max-fpr from 0.01 to 0.05 . change default value of flag -H/--min-hic-ureads-qcov from 0.8 to 0.75 (similarity ~98% ). faster search result parsing.","title":"v0.7.0 - 2022-01-24"},{"location":"download/#v060-2021-08-13","text":"new command: merge : merge search results from multiple databases. compute : fix splitting very short genomes. remove flag -e/--exact-number , making it default. index : do not roundup sizes of indexes. The searching speed is not affected and even faster due to optimization of search command. use three k-mers thresholds to control index file size. better control of cocurrency number and better progress bar. do not support RAMBO index anymore. search : 1.37X speedup, and faster for database with two or more hash functions. new flag -S/--do-not-sort . profile : fix a nil pointer bug when no taxid mapping data given. fix number of ureads. new flag -m/--keep-main-matches and --max-score-gap","title":"v0.6.0 - 2021-08-13"},{"location":"download/#v050-2021-06-24","text":"compute : support multiple sizes of k-mer. fix bug of --by-seq . more log. index : default block size is computed by -j/--threads instead of number of CPUs. search : show real-time processing speed. new flag -g/--query-whole-file . new flag -u/--kmer-dedup-threshold . new flag -m/--min-query-len . increase speed for database with mulitple hashes. profile : better decision of the existence of a reference. new flag -B/--binning-result for output reads binning result. new flag -m/--norm-abund .","title":"v0.5.0 - 2021-06-24"},{"location":"download/#v040-2021-04-08","text":"new command: profile for generating taxonomic profile from search result. compute : new flag -B/--seq-name-filter for filtering out unwanted sequences like plasmid. new flag -N/--ref-name-regexp for extracting reference name from sequence file. search : change default threshold value. new flag -n/--keep-top-scores for keeping matches with the top N score.","title":"v0.4.0 - 2021-04-08"},{"location":"download/#v030-2021-03-16","text":"use --quiet to replace --verbose , making printing log info default. search : fix computing intersetion between repeats. fix closing mmap on Windows. change output format and add Jaccard Index. speedup by parallelizing name mapping and database closing. flush result immediately. keep the output order by default compute : change default file regexp for matching .fna files. autocompletion : support bash, zsh, fish, powershell.","title":"v0.3.0 - 2021-03-16"},{"location":"download/#v021-2020-12-31","text":"index : reduce memory occupation.","title":"v0.2.1 - 2020-12-31"},{"location":"download/#v020-2020-12-30","text":"Add support of RAMBO like indexing. Limit to only one input database. Change output format.","title":"v0.2.0 - 2020-12-30"},{"location":"download/#v010-2020-xx-xx","text":"First release with basic function.","title":"v0.1.0 - 2020-xx-xx"},{"location":"faq/","text":"Frequently Asked Questions General How can I run KMCP on a computer without enough main memory? By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database. Database building Can I create a database with custom genome collections for profiling? Yes, you can use taxonkit create-taxdump to create NCBI-style taxdump files for profiling, which also generates a taxid.map file. What k-mer size should I use to build the database? Multiple k-mer sizes are supported, but one value is good enough. Bigger k-mer sizes bring high specificity at the cost of decrease of sensitivity. k = 21 is recommended for metagenomic profiling. How to add new genomes to the database? KMCP builds database very fast, you can either rebuilt the database after adding new genomes, or create a separate database with the new genomes, search against these databases, and merge the results. Unexpected EOF error Some files could corrupt during downloading, we recommend checking sequence file integrity using seqkit ( gzip -t failed for some files in my tests). List corrupted files # corrupted files find $genomes -name \"*.gz\" \\ | rush 'seqkit seq -w 0 {} > /dev/null; if [ $? -ne 0 ]; then echo {}; fi' \\ > failed.txt # empty files find $genomes -name \"*.gz\" -size 0 >> failed.txt Delete these files: cat failed.txt | rush '/bin/rm {}' Redownload these files. For example, from rush cache file download.rush (list of succeed commands), where the commands are in one line. grep -f failed.txt download.rush \\ | sed 's/__CMD__//g' \\ | rush '{}' For genome_updater , URLs of genomes can be found in files like 2021-09-30_13-32-30_url_downloaded.txt , you can extract URLs using grep -f failed.txt -v *url_downloaded.txt or some other ways, and batch redownload them using parallel . Searching Why are the CPU usages are very low, not 100%? Please check the log (in terminal not the log file via the option --log ), it should report current searching speed, like: 21:07:42.949 [INFO] reading sequence file: t_t3.fa.gz processed queries: 1253376, speed: 8.127 million queries per minute If the speed is very slow. Are you running KMCP in a computer cluster (HPC)? If yes, please switch on -w/--load-whole-db . Because the default database loading mode would be very slow for network-attached storage (NAS). Are the reference genomes are highly similar? E.g., tens of thousands of genomes of a same species? If yes, check the search result to see if there are thousands of matches for a read. You may choose another graph-based sequence searching tool. Can I run multiple KMCP processes in a machine? Yes you can. But note that KMCP search is CPU- and RAM-intense. So please to limit the number of CPUs cores to use for each process with the falg -j/--threads , of which the default value is the available CPUs cores of the machine. Profiling Where is the taxid.map? Each prebuilt database contains a taxid.map file in its directory. You can concatenate them into a big one: $ cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map $ head -n 5 taxid.map GCA_004025655.1 10243 GCA_004025395.1 10243 GCA_004025355.1 10243 GCA_003971405.1 10243 GCA_003971385.1 10243 Or set the options -T/--taxid-map multiple times: kmcp profile -T gtdb.kmcp/taxid.map -T refseq-viral.kmcp/taxid.map -T refseq-fungi.kmcp/taxid.map ... For other custom genome collections, you can use taxonkit create-taxdump to create NCBI-style taxdump files for custom taxonomy, e.g., GTDB and ICTV , which also generates a taxid.map file. Unknown taxid? 19:54:54.632 [ERRO] unknown taxid for NZ_CP028116.1, please check taxid mapping file(s) If the kmcp profile reports this, you may need to check if the taxid mapping file contain all the reference IDs. And make sure the reference IDs match these in the database, the later ones are listed in: $ head -n 5 $kmcp_db_dir/R001/__name_mapping.tsv NC_013654.1 NC_013654.1 NC_000913.3 NC_000913.3 NC_010655.1 NC_010655.1 NC_012971.2 NC_012971.2 NC_011750.1 NC_011750.1 There's another case: you used --name-map in kmcp search . Please don't do this if you will use the search result for metagenomic profiling which needs the original reference IDs. We add a note now: -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs. How to tune parts of options when using preset profiling modes? Sorry it's not supported due to the limitation of the command-line argument parsers. You need explicitly set all relevant options of the mode. It's available since v0.8.2.","title":"FAQs"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#general","text":"","title":"General"},{"location":"faq/#how-can-i-run-kmcp-on-a-computer-without-enough-main-memory","text":"By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Another way is dividing the reference genomes into several parts and building smaller databases for all parts, so that the biggest database can be loaded into RAM. After performing database searching, search results on all small databases can be merged with kmcp merge for downstream analysis. Buiding small databases can also accelerate searching on a computer cluster , where every node searches a part of the database.","title":"How can I run KMCP on a computer without enough main memory?"},{"location":"faq/#database-building","text":"","title":"Database building"},{"location":"faq/#can-i-create-a-database-with-custom-genome-collections-for-profiling","text":"Yes, you can use taxonkit create-taxdump to create NCBI-style taxdump files for profiling, which also generates a taxid.map file.","title":"Can I create a database with custom genome collections for profiling?"},{"location":"faq/#what-k-mer-size-should-i-use-to-build-the-database","text":"Multiple k-mer sizes are supported, but one value is good enough. Bigger k-mer sizes bring high specificity at the cost of decrease of sensitivity. k = 21 is recommended for metagenomic profiling.","title":"What k-mer size should I use to build the database?"},{"location":"faq/#how-to-add-new-genomes-to-the-database","text":"KMCP builds database very fast, you can either rebuilt the database after adding new genomes, or create a separate database with the new genomes, search against these databases, and merge the results.","title":"How to add new genomes to the database?"},{"location":"faq/#unexpected-eof-error","text":"Some files could corrupt during downloading, we recommend checking sequence file integrity using seqkit ( gzip -t failed for some files in my tests). List corrupted files # corrupted files find $genomes -name \"*.gz\" \\ | rush 'seqkit seq -w 0 {} > /dev/null; if [ $? -ne 0 ]; then echo {}; fi' \\ > failed.txt # empty files find $genomes -name \"*.gz\" -size 0 >> failed.txt Delete these files: cat failed.txt | rush '/bin/rm {}' Redownload these files. For example, from rush cache file download.rush (list of succeed commands), where the commands are in one line. grep -f failed.txt download.rush \\ | sed 's/__CMD__//g' \\ | rush '{}' For genome_updater , URLs of genomes can be found in files like 2021-09-30_13-32-30_url_downloaded.txt , you can extract URLs using grep -f failed.txt -v *url_downloaded.txt or some other ways, and batch redownload them using parallel .","title":"Unexpected EOF error"},{"location":"faq/#searching","text":"","title":"Searching"},{"location":"faq/#why-are-the-cpu-usages-are-very-low-not-100","text":"Please check the log (in terminal not the log file via the option --log ), it should report current searching speed, like: 21:07:42.949 [INFO] reading sequence file: t_t3.fa.gz processed queries: 1253376, speed: 8.127 million queries per minute If the speed is very slow. Are you running KMCP in a computer cluster (HPC)? If yes, please switch on -w/--load-whole-db . Because the default database loading mode would be very slow for network-attached storage (NAS). Are the reference genomes are highly similar? E.g., tens of thousands of genomes of a same species? If yes, check the search result to see if there are thousands of matches for a read. You may choose another graph-based sequence searching tool.","title":"Why are the CPU usages are very low, not 100%?"},{"location":"faq/#can-i-run-multiple-kmcp-processes-in-a-machine","text":"Yes you can. But note that KMCP search is CPU- and RAM-intense. So please to limit the number of CPUs cores to use for each process with the falg -j/--threads , of which the default value is the available CPUs cores of the machine.","title":"Can I run multiple KMCP processes in a machine?"},{"location":"faq/#profiling","text":"","title":"Profiling"},{"location":"faq/#where-is-the-taxidmap","text":"Each prebuilt database contains a taxid.map file in its directory. You can concatenate them into a big one: $ cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map $ head -n 5 taxid.map GCA_004025655.1 10243 GCA_004025395.1 10243 GCA_004025355.1 10243 GCA_003971405.1 10243 GCA_003971385.1 10243 Or set the options -T/--taxid-map multiple times: kmcp profile -T gtdb.kmcp/taxid.map -T refseq-viral.kmcp/taxid.map -T refseq-fungi.kmcp/taxid.map ... For other custom genome collections, you can use taxonkit create-taxdump to create NCBI-style taxdump files for custom taxonomy, e.g., GTDB and ICTV , which also generates a taxid.map file.","title":"Where is the taxid.map?"},{"location":"faq/#unknown-taxid","text":"19:54:54.632 [ERRO] unknown taxid for NZ_CP028116.1, please check taxid mapping file(s) If the kmcp profile reports this, you may need to check if the taxid mapping file contain all the reference IDs. And make sure the reference IDs match these in the database, the later ones are listed in: $ head -n 5 $kmcp_db_dir/R001/__name_mapping.tsv NC_013654.1 NC_013654.1 NC_000913.3 NC_000913.3 NC_010655.1 NC_010655.1 NC_012971.2 NC_012971.2 NC_011750.1 NC_011750.1 There's another case: you used --name-map in kmcp search . Please don't do this if you will use the search result for metagenomic profiling which needs the original reference IDs. We add a note now: -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs.","title":"Unknown taxid?"},{"location":"faq/#how-to-tune-parts-of-options-when-using-preset-profiling-modes","text":"Sorry it's not supported due to the limitation of the command-line argument parsers. You need explicitly set all relevant options of the mode. It's available since v0.8.2.","title":"How to tune parts of options when using preset profiling modes?"},{"location":"usage/","text":"Usage KMCP is a command-line tool consisting of several subcommands. Program: kmcp (K-mer-based Metagenomic Classification and Profiling) Version: v0.8.3 Documents: https://bioinf.shenwei.me/kmcp Source code: https://github.com/shenwei356/kmcp KMCP is a tool for metagenomic classification and profiling. KMCP can also be used for: 1. Fast sequence search against large scales of genomic datasets as BIGSI and COBS do. 2. Fast assembly/genome similarity estimation as Mash and sourmash do, by utilizing Minimizer, FracMinHash (Scaled MinHash), or Closed Syncmers. Usage: kmcp [command] Available Commands: autocompletion Generate shell autocompletion script compute Generate k-mers (sketches) from FASTA/Q sequences index Construct database from k-mer files merge Merge search results from multiple databases profile Generate taxonomic profile from search results search Search sequences against a database utils Some utilities version Print version information and check for update Flags: -h, --help help for kmcp -i, --infile-list string \u25ba File of input files list (one file per line). If given, they are appended to files from CLI arguments. --log string \u25ba Log file. -q, --quiet \u25ba Do not print any verbose information. But you can write them to file with --log. -j, --threads int \u25ba Number of CPUs cores to use. (default 16) compute Generate k-mers (sketches) from FASTA/Q sequences Input: 1. Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, 2. Or a directory containing sequence files via the flag -I/--in-dir, with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp. Attention: You may rename the sequence files for convenience because the sequence/genome identifier in the index and search results would be: 1). For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. 2). For splitting sequence mode (see details below): same to 1). 3). For computing k-mers for each sequence: the sequence identifier. Attentions: 1. Unwanted sequences like plasmid can be filtered out by the name via regular expressions (-B/--seq-name-filter). 2. By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. 3. It also supports splitting sequences into chunks, this could increase the specificity in search results at the cost of a slower searching speed. 4. Multiple sizes of k-mers are supported. Supported k-mer (sketches) types: 1. K-mer: 1). ntHash of k-mer (-k) 2. K-mer sketchs (all using ntHash): 1). FracMinHash (-k -D), previously named Scaled MinHash 2). Minimizer (-k -W), optionally scaling/down-sampling (-D) 3). Closed Syncmer (-k -S), optionally scaling/down-sampling (-D) Splitting sequences: 1. Sequences can be splitted into chunks by a chunk size (-s/--split-size) or number of chunks (-n/--split-number) with overlap (-l/--split-overlap). In this mode, the sequences of each genome should be saved in an individual file. 2. When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter) in a sequence file are concatenated with k-1 'N's before splitting. 3. Both sequence IDs and chunks indices are saved for later use, in form of meta/description data in .unik files. Metadata: 1. Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. 2. When parsing whole sequence files or splitting by the number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp, e.g., \"^(\\w{3}_\\d{9}\\.\\d+)\" for RefSeq records. Output: 1. All outputted .unik files are saved in ${outdir}, with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree '/xxx/yyy/zzz/' is built for > 1000 output files. 2. For splitting sequence mode (--split-size > 0 or --split-number > 0), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik 3. A summary file (\"${outdir}/_info.txt\") is generated for later use. Users need to check if the reference IDs (column \"name\") are what supposed to be. Performance tips: 1. Decrease the value of -j/--threads for data in hard disk drives to reduce I/O pressure. Next step: 1. Check the summary file (${outdir}/_info.txt) to see if the reference IDs (column \"name\") are what supposed to be. 2. Run \"kmcp index\" with the output directory. Examples: 1. From few sequence files: kmcp compute -k 21 -n 5 -l 100 -O tmp-k21-n5-l100 NC_045512.2.fna.gz 2. From a list file: kmcp compute -k 21 -n 10 -l 100 -O tmp-k21-10-l100 -i list.txt 3. From a directory containing many sequence files: kmcp compute -k 21 -n 10 -l 100 -B plasmid \\ -O gtdb-k21-n10-l100 -I gtdb-genomes/ Usage: kmcp compute [flags] [-k <k>] [-n <chunks>] [-l <overlap>] {[-I <seqs dir>] | <seq files>} -O <out dir> Flags: --by-seq \u25ba Compute k-mers (sketches) for each sequence, instead of the whole file. --circular \u25ba Input sequences are circular. -c, --compress \u25ba Output gzipped .unik files, it's slower and can save little space. -r, --file-regexp string \u25ba Regular expression for matching sequence files in -I/--in-dir, case ignored. (default \"\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") --force \u25ba Overwrite existed output directory. -h, --help help for compute -I, --in-dir string \u25ba Directory containing FASTA/Q files. Directory symlinks are followed. -k, --kmer ints \u25ba K-mer size(s). (default [21]) -W, --minimizer-w int \u25ba Minimizer window size. -O, --out-dir string \u25ba Output directory. -N, --ref-name-regexp string \u25ba Regular expression (must contains \"(\" and \")\") for extracting reference name from filename. (default \"(?i)(.+)\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") -D, --scale int \u25ba Scale of the FracMinHash (Scaled MinHash), or down-sample factor for Syncmers and Minimizer. (default 1) -B, --seq-name-filter strings \u25ba List of regular expressions for filtering out sequences by header/name, case ignored. -m, --split-min-ref int \u25ba Only splitting sequences >= X bp. (default 1000) -n, --split-number int \u25ba Chunk number for splitting sequences, incompatible with -s/--split-size. -l, --split-overlap int \u25ba Chunk overlap for splitting sequences. -s, --split-size int \u25ba Chunk size for splitting sequences, incompatible with -n/--split-number. -S, --syncmer-s int \u25ba Length of the s-mer in Closed Syncmers. index Construct database from k-mer files We build the index for k-mers (sketches) with a modified compact bit-sliced signature index (COBS). We totally rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed. Input: The output directory generated by \"kmcp compute\". Database size and searching accuracy: 0. Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size. 1. -f/--false-positive-rate: the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. 2. -n/--num-hash: large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. 3. The value of block size -b/--block-size better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 4. Use flag -x/--block-sizeX-kmers-t, -8/--block-size8-kmers-t, and -1/--block-size1-kmers-t to separately create indexes for inputs with huge number of k-mers, for precise control of database size. References: 1. COBS: https://arxiv.org/abs/1905.09624 Taxonomy data: 1. No taxonomy data are included in the database. 2. Taxonomy information are only needed in \"profile\" command. Performance tips: 1. The number of blocks (.uniki files) better to be smaller than or equal to the number of CPU cores for faster searching speed. We can set -j/--threads to control blocks number. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. 2. #threads files are simultaneously opened, and max number of opened files is limited by the flag -F/--max-open-files. You may use a small value of -F/--max-open-files for hard disk drive storages. 3. When the database is used in a new computer with more CPU cores, 'kmcp search' could automatically scale to utilize as many cores as possible. Examples: 1. For bacteria genomes: kmcp index -f 0.3 -n 1 -j 32 -I gtdb-k21-n10-l100/ -O gtdb.kmcp 2. For viruses, use -x and -8 to control index size of the largest chunks: kmcp index -f 0.05 -n 1 -j 32 -x 100K -8 1M \\ -I genbank-viral-k21-n5-l100/ -O genbank-viral.kmcp Usage: kmcp index [flags] [-f <fpr>] [-n <hashes>] [-j <blocks>] -I <compute output> -O <kmcp db> Flags: -a, --alias string \u25ba Database alias/name. (default: basename of --out-dir). You can also manually edit it in info file: ${outdir}/__db.yml. -b, --block-size int \u25ba Block size, better be multiple of 64 for large number of input files. (default: min(#.files/#theads, 8)) -1, --block-size1-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, an individual index is created for this file. Supported units: K, M, G. (default \"200M\") -8, --block-size8-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to 8. Supported units: K, M, G. (default \"20M\") -X, --block-sizeX int \u25ba If k-mers of single .unik file exceeds --block-sizeX-kmers-t, block size is changed to this value. (default 256) -x, --block-sizeX-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to --block-sizeX. Supported units: K, M, G. (default \"10M\") --dry-run \u25ba Dry run, useful for adjusting parameters (highly recommended). -f, --false-positive-rate float \u25ba False positive rate of the bloom filters, range: (0, 1). (default 0.3) --file-regexp string \u25ba Regular expression for matching files in -I/--in-dir, case ignored. (default \".unik$\") --force \u25ba Overwrite existed output directory. -h, --help help for index -I, --in-dir string \u25ba Directory containing .unik files. Directory symlinks are followed. -F, --max-open-files int \u25ba Maximal number of opened files, please use a small value for hard disk drive storage. (default 256) -n, --num-hash int \u25ba Number of hash functions in bloom filters. (default 1) -O, --out-dir string \u25ba Output directory. (default: ${indir}.kmcp-db) search Search sequences against a database Attentions: 1. Input format should be (gzipped) FASTA or FASTQ from files or stdin. - Paired-end files should be given via -1/--read1 and -2/--read2. kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz - Single-end can be given as positional arguments or -1/-2. kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz **Single-end mode is recommended for paired-end reads, for higher sensitivity**. 2. A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold to remove duplicates. 3. For long reads or contigs, you should split them into short reads using \"seqkit sliding\", e.g., seqkit sliding -s 100 -W 300 Shared flags between \"search\" and \"profile\": 1. -t/--min-query-cov. Index files loading modes: 1. Using memory-mapped index files with mmap (default): - Faster startup speed when index files are buffered in memory. - Multiple KMCP processes can share the memory. 2. Loading the whole index files into memory (-w/--load-whole-db): - This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. - It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases. - Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storages (NAS). 3. Low memory mode (--low-mem): - Do not load all index files into memory nor use mmap, using file seeking. - It's much slower, >4X slower on SSD and would be much slower on HDD disks. - Only use this mode for small number of queries or a huge database that can't be loaded into memory. Output format: Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging The values of tCov and jacc in results only apply to databases built with a single size of k-mer. Performance tips: 1. Increase the value of -j/--threads for acceleratation, but values larger than the number of CPU cores won't bring extra speedup. 2. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. Examples: 1. Single-end mode (recommended) kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz sample_1_unpaired.fq.gz sample_2_unpaired.fq.gz 2. Paired-end mode kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ -1 sample_1.fq.gz -2 sample_2.fq.gz 3. In computer cluster, where databases are saved in NAS storages. kmcp search -w -d gtdb.n16-00.kmcp -o sample.kmcp@gtdb.n16-00.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz Usage: kmcp search [flags] [-w] -d <kmcp db> [-t <min-query-cov>] [read1.fq.gz] [read2.fq.gz] [unpaired.fq.gz] [-o read.tsv.gz] Flags: -d, --db-dir string \u25ba Database directory created by \"kmcp index\". Please add -w/--load-whole-db for databases on network-attached storages (NAS), e.g., a computer cluster environment. -D, --default-name-map \u25ba Load ${db}/__name_mapping.tsv for mapping name first. -S, --do-not-sort \u25ba Do not sort matches of a query. -h, --help help for search -n, --keep-top-scores int \u25ba Keep matches with the top N scores for a query, 0 for all. -K, --keep-unmatched \u25ba Keep unmatched query sequence information. -u, --kmer-dedup-threshold int \u25ba Remove duplicated kmers for a query with >= X k-mers. (default 256) -w, --load-whole-db \u25ba Load all index files into memory, it's faster for small databases but needs more memory. Use this for databases on network-attached storages (NAS). Please read \"Index files loading modes\" in \"kmcp search -h\". --low-mem \u25ba Do not load all index files into memory nor use mmap, the searching would be very very slow for a large number of queries. Please read \"Index files loading modes\" in \"kmcp search -h\". -f, --max-fpr float \u25ba Maximal false positive rate of a query. (default 0.05) -c, --min-kmers int \u25ba Minimal number of matched k-mers (sketches). (default 10) -t, --min-query-cov float \u25ba Minimal query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. (default 0.55) -m, --min-query-len int \u25ba Minimal query length. (default 30) -T, --min-target-cov float \u25ba Minimal target coverage, i.e., proportion of matched k-mers and unique k-mers of a target. -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs. -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") --query-id string \u25ba Custom query Id when using the whole file as a query. -g, --query-whole-file \u25ba Use the whole file as a query, e.g., for genome similarity estimation against k-mer sketch database. -1, --read1 string \u25ba (Gzipped) read1 file. -2, --read2 string \u25ba (Gzipped) read2 file. -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\") --try-se \u25ba If paired-end reads have no hits, re-search with read1, if still fails, try read2. -G, --use-filename \u25ba Use file name as query ID when using the whole file as a query. merge Merge search results from multiple databases Input: *. Searching results of the same reads in different databases. *. The order of multiple input reads files should be the same during searching. *. When only one input given, we just copy and write to the input file. This is friendly to workflows which assume multiple inputs are given. Example: kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz Usage: kmcp merge [flags] [-o read.tsv.gz] [<search results> ...] Flags: -n, --field-hits int \u25ba Field of hits. (default 5) -f, --field-queryIdx int \u25ba Field of queryIdx. (default 15) -h, --help help for merge -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\") profile Generate taxonomic profile from search results Methods: 1. Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimal proportion of matched chunks (-p/--min-chunks-fraction). (***highly recommended***) Another flag -d/--max-chunks-depth-stdev further reduces false positives. 2. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. 3. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positive of ambiguous matches. You can also disable this step by the flag --no-amb-corr. If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. 4. Multi-aligned queries are proportionally assigned to references with a similar strategy in Metalign. 5. Input files are parsed four times, therefore STDIN is not supported. Reference: 1. MegaPath: https://doi.org/10.1186/s12864-020-06875-6 2. Metalign: https://doi.org/10.1186/s13059-020-02159-0 Accuracy notes: *. Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate (-f/--max-fpr) of a query. *. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 *. -U/--min-hic-ureads, minimal number, >= 1 *. -H/--min-hic-ureads-qcov, minimal query coverage, >= -t/--min-qcov *. -P/--min-hic-ureads-prop, minimal proportion, higher values increase precision at the cost of sensitivity. *. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambiguous reads with the algorithm in MegaPath. *. --keep-perfect-matches is not recommended, which decreases sensitivity. *. --keep-main-matches is not recommended, which affects accuracy of abundance estimation. *. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation. Profiling modes: We preset six profiling modes, available with the flag -m/--mode: - 0 (for pathogen detection) - 1 (higher recall) - 2 (high recall) - 3 (default) - 4 (high precision) - 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.55 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use \"taxonkit create-taxdump\" (https://github.com/shenwei356/taxonkit) to create NCBI-style taxdump files, which also generates a TaxId mapping file. Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. *3. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results. Profiling output formats: 1. KMCP (-o/--out-prefix) Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please output CAMI or MetaPhlAn format. 2. CAMI (-M/--metaphlan-report, --metaphlan-report-version, -s/--sample-id, --taxonomy-id) Related tools (https://github.com/shenwei356/taxonkit): - taxonkit profile2cami: convert any metagenomic profile table with TaxIds to CAMI format. Use this if you forget to output CAMI format. - taxonkit cami-filter: remove taxa of given TaxIds and their descendants in CAMI metagenomic profile. 3. MetaPhlAn (-C/--cami-report, -s/--sample-id) KMCP format: Tab-delimited format with 16 columns: 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. score, The 90th percentile of qCov of uniquely matched reads 4. chunksFrac, Genome chunks fraction 5. chunksRelDepth, Relative depths of reference chunks 6. chunksRelDepthStd, The strandard deviation of chunksRelDepth 7. reads, Total number of matched reads of this reference 8. ureads, Number of uniquely matched reads 9. hicureads, Number of uniquely matched reads with high-confidence 10. refsize, Reference size 11. refname, Reference name, optional via name mapping file 12. taxid, TaxId of the reference 13. rank, Taxonomic rank 14. taxname, Taxonomic name 15. taxpath, Complete lineage 16. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Taxonomic binning formats: 1. CAMI (-B/--binning-result) Examples: 1. Default mode: kmcp profile -X taxdump/ -T taxid.map -m 3 \\ sample.kmcp.tsv.gz -o sample.k.profile \\ -C sample.c.profile -s sample 2. For pathogen detection (you may create databases with lower FPR, e.g., kmcp index -f 0.1 -n 2 for bacteria and fungi genomes, and search with low k-mer coverage threshold -t 0.4): kmcp profile -X taxdump/ -T taxid.map -m 3 -t 0.4 \\ sample.kmcp.tsv.gz -o sample.k.profile Usage: kmcp profile [flags] [-X <taxdump dir>] [-T <taxid.map>] [-m <mode>] [-o <kmcp profile>] <search results> Flags: -B, --binning-result string \u25ba Save extra binning result in CAMI report. -C, --cami-report string \u25ba Save extra CAMI-like report. --debug string \u25ba Debug output file. -F, --filter-low-pct float \u25ba Filter out predictions with the smallest relative abundances summing up X%. Range: [0,100). -h, --help help for profile --keep-main-matches \u25ba Only keep main matches, abandon matches with sharply decreased qcov (> --max-qcov-gap). --keep-perfect-matches \u25ba Only keep the perfect matches (qcov == 1) if there are. -n, --keep-top-qcovs int \u25ba Keep matches with the top N qcovs for a query, 0 for all. --level string \u25ba Level to estimate abundance at. Available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp profile -h\" for details. (default 5000) -d, --max-chunks-depth-stdev float \u25ba Maximal standard deviation of relative depths of all chunks. (default 2) -f, --max-fpr float \u25ba Maximal false positive rate of a read in search result. (default 0.05) -R, --max-mismatch-err float \u25ba Maximal error rate of a read being matched to a wrong reference, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) --max-qcov-gap float \u25ba Max qcov gap between adjacent matches. (default 0.4) -M, --metaphlan-report string \u25ba Save extra metaphlan-like report. --metaphlan-report-version string \u25ba Metaphlan report version (2 or 3) (default \"3\") -p, --min-chunks-fraction float \u25ba Minimal fraction of matched reference chunks with reads >= -r/--min-chunks-reads. (default 0.8) -r, --min-chunks-reads int \u25ba Minimal number of reads for a reference chunk. (default 50) -D, --min-dreads-prop float \u25ba Minimal proportion of distinct reads, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) -U, --min-hic-ureads int \u25ba Minimal number of high-confidence uniquely matched reads for a reference. (default 5) -P, --min-hic-ureads-prop float \u25ba Minimal proportion of high-confidence uniquely matched reads. (default 0.1) -H, --min-hic-ureads-qcov float \u25ba Minimal query coverage of high-confidence uniquely matched reads. (default 0.75) -t, --min-query-cov float \u25ba Minimal query coverage of a read in search result. (default 0.55) -u, --min-uniq-reads int \u25ba Minimal number of uniquely matched reads for a reference. (default 20) -m, --mode int \u25ba Profiling mode, type \"kmcp profile -h\" for details. available values: 0 (for pathogen detection), 1 (higherrecall), 2 (high recall), 3 (default), 4 (high precision), 5 (higher precision). (default 3) -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to reference names. --no-amb-corr \u25ba Do not correct ambiguous reads. Use this flag to reduce analysis time if the stage 1/4 produces thousands of candidates. --norm-abund string \u25ba Method for normalize abundance of a reference by the mean/min/max abundance in all chunks, available values: mean, min, max. (default \"mean\") -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") --rank-prefix strings \u25ba Prefixes of taxon name in certain ranks, used with --metaphlan-report. (default [k__,p__,c__,o__,f__,g__,s__,t__]) -s, --sample-id string \u25ba Sample ID in result file. -S, --separator string \u25ba Separator of TaxIds and taxonomy names. (default \";\") --show-rank strings \u25ba Only show TaxIds and names of these ranks. (default [superkingdom,phylum,class,order,family,genus,species,strain]) -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds. --taxonomy-id string \u25ba Taxonomy ID in result file. utils Some utilities Usage: kmcp utils [command] Available Commands: cov2simi Convert k-mer coverage to sequence similarity filter Filter search results and find species/assembly-specific queries index-info Print information of index file merge-regions Merge species/assembly-specific regions query-fpr Compute the maximal false positive rate of a query unik-info Print information of .unik file filter Filter search results and find species/assembly-specific queries Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils filter [flags] Flags: -h, --help help for filter --level string \u25ba Level to filter. available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils filter -h\" for details. (default 5000) -f, --max-fpr float \u25ba Maximal false positive rate of a read in search result. (default 0.05) -t, --min-query-cov float \u25ba Minimal query coverage of a read in search result. (default 0.55) -H, --no-header-row \u25ba Do not print header row. -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds. index-info Print information of index file Usage: kmcp utils index-info [flags] Flags: -a, --all \u25ba Show all information. -b, --basename \u25ba Only output basenames of files. -h, --help help for index-info -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") merge-regions Merge species/assembly-specific regions Steps: # 1. Simulating reads and searching on one or more databases. seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db1.kmcp -o ref.fna.gz.kmcp@db1.tsv.gz seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db2.kmcp -o ref.fna.gz.kmcp@db2.tsv.gz # 2. Merging and filtering searching results kmcp merge ref.fna.gz.kmcp@*.tsv.gz \\ | kmcp utils filter -X taxdump -T taxid.map \\ -o ref.fna.gz.kmcp.uniq.tsv.gz # 3. Merging regions. # Here the value of --min-overlap should be k-1. kmcp utils merge-regions --min-overlap 20 ref.fna.gz.kmcp.uniq.tsv.gz \\ -o ref.fna.gz.kmcp.uniq.tsv.gz.bed Output (BED6 format): 1. chrom - chromosome name 2. chromStart - starting position (0-based) 3. chromEnd - ending position (0-based) 4. name - \"species-specific\" or \"assembly-specific\" 5. score - 0-1000, 1000 for \"assembly-specific\", others for \"\"species-specific\" 6. strand - \".\" Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils merge-regions [flags] Flags: -h, --help help for merge-regions -I, --ignore-type \u25ba Merge species and assembly-specific regions. -f, --max-fpr float \u25ba Maximal false positive rate of a read in search result. (default 0.05) -g, --max-gap int \u25ba Maximal distance of starting positions of two adjacent regions, 0 for no limitation, 1 for no merging. --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils merge-regions -h\" for details. (default 5000) -l, --min-overlap int \u25ba Minimal overlap of two adjacent regions, recommend K-1. (default 1) -t, --min-query-cov float \u25ba Minimal query coverage of a read in search result. (default 0.55) -a, --name-assembly string \u25ba Name of assembly-specific regions. (default \"assembly-specific\") -s, --name-species string \u25ba Name of species-specific regions. (default \"species-specific\") -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") -r, --regexp string \u25ba Regular expression for extract reference name and query locations. (default \"^(.+)_sliding:(\\\\d+)\\\\-(\\\\d+)$\") unik-info Print information of .unik file Tips: 1. For lots of small files (especially on SDD), use big value of '-j' to parallelize counting. Usage: kmcp utils unik-info [flags] Flags: -a, --all \u25ba All information, including the number of k-mers. -b, --basename \u25ba Only output basename of files. -h, --help help for unik-info -o, --out-file string \u25ba Out file (\"-\" for stdout, suffix .gz for gzipped out.) (default \"-\") -e, --skip-err \u25ba Skip error, only show warning message. --symbol-false string \u25ba Smybol for false. (default \"\u2715\") --symbol-true string \u25ba Smybol for true. (default \"\u2713\") -T, --tabular \u25ba Output in machine-friendly tabular format. cov2simi Convert k-mer coverage to sequence similarity similarity = 87.456 + 26.410*qcov - 22.008*qcov*qcov + 7.325*qcov*qcov*qcov Usage: kmcp utils cov2simi [flags] Flags: -h, --help help for cov2simi -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") -t, --query-cov float \u25ba K-mer query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. range: [0, 1] query-fpr Compute the the maximimal false positive rate of a query Solomon and Kingsford apply a Chernoff bound and show that the false positive probability for a query is: fpr \u2264 exp( -n(t-f)^2 / (2(1-f)) ) Where: f, the false positive rate of the bloom filters t, the minimal proportion of matched k-mers and unique k-mers of a query n, the number of unique k-mers of the query Reference: 1. SBT: https://doi.org/10.1038/nbt.3442 2. COBS: https://arxiv.org/abs/1905.09624v2 Usage: kmcp utils query-fpr [flags] Flags: -f, --false-positive-rate float \u25ba False positive rate of the bloom filters in the database. range: (0, 1) (default 0.3) -h, --help help for query-fpr -t, --min-query-cov float \u25ba Minimal query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. range: [0, 1] (default 0.55) -n, --num-kmers int \u25ba Number of unique k-mers of the query. (default 80) -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") autocompletion Generate shell autocompletion script Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish Usage: kmcp autocompletion [flags] Flags: --file string autocompletion file (default \"/home/shenwei/.bash_completion.d/kmcp.sh\") -h, --help help for autocompletion --shell string autocompletion type (bash|zsh|fish|powershell) (default \"bash\")","title":"Usage"},{"location":"usage/#usage","text":"KMCP is a command-line tool consisting of several subcommands. Program: kmcp (K-mer-based Metagenomic Classification and Profiling) Version: v0.8.3 Documents: https://bioinf.shenwei.me/kmcp Source code: https://github.com/shenwei356/kmcp KMCP is a tool for metagenomic classification and profiling. KMCP can also be used for: 1. Fast sequence search against large scales of genomic datasets as BIGSI and COBS do. 2. Fast assembly/genome similarity estimation as Mash and sourmash do, by utilizing Minimizer, FracMinHash (Scaled MinHash), or Closed Syncmers. Usage: kmcp [command] Available Commands: autocompletion Generate shell autocompletion script compute Generate k-mers (sketches) from FASTA/Q sequences index Construct database from k-mer files merge Merge search results from multiple databases profile Generate taxonomic profile from search results search Search sequences against a database utils Some utilities version Print version information and check for update Flags: -h, --help help for kmcp -i, --infile-list string \u25ba File of input files list (one file per line). If given, they are appended to files from CLI arguments. --log string \u25ba Log file. -q, --quiet \u25ba Do not print any verbose information. But you can write them to file with --log. -j, --threads int \u25ba Number of CPUs cores to use. (default 16)","title":"Usage"},{"location":"usage/#compute","text":"Generate k-mers (sketches) from FASTA/Q sequences Input: 1. Input plain or gzipped FASTA/Q files can be given via positional arguments or the flag -i/--infile-list with the list of input files, 2. Or a directory containing sequence files via the flag -I/--in-dir, with multiple-level sub-directories allowed. A regular expression for matching sequencing files is available via the flag -r/--file-regexp. Attention: You may rename the sequence files for convenience because the sequence/genome identifier in the index and search results would be: 1). For the default mode (computing k-mers for the whole file): the basename of file with common FASTA/Q file extension removed, captured via the flag -N/--ref-name-regexp. 2). For splitting sequence mode (see details below): same to 1). 3). For computing k-mers for each sequence: the sequence identifier. Attentions: 1. Unwanted sequences like plasmid can be filtered out by the name via regular expressions (-B/--seq-name-filter). 2. By default, kmcp computes k-mers (sketches) of every file, you can also use --by-seq to compute for every sequence, where sequence IDs in all input files are better to be distinct. 3. It also supports splitting sequences into chunks, this could increase the specificity in search results at the cost of a slower searching speed. 4. Multiple sizes of k-mers are supported. Supported k-mer (sketches) types: 1. K-mer: 1). ntHash of k-mer (-k) 2. K-mer sketchs (all using ntHash): 1). FracMinHash (-k -D), previously named Scaled MinHash 2). Minimizer (-k -W), optionally scaling/down-sampling (-D) 3). Closed Syncmer (-k -S), optionally scaling/down-sampling (-D) Splitting sequences: 1. Sequences can be splitted into chunks by a chunk size (-s/--split-size) or number of chunks (-n/--split-number) with overlap (-l/--split-overlap). In this mode, the sequences of each genome should be saved in an individual file. 2. When splitting by number of chunks, all sequences (except for these matching any regular expression given by -B/--seq-name-filter) in a sequence file are concatenated with k-1 'N's before splitting. 3. Both sequence IDs and chunks indices are saved for later use, in form of meta/description data in .unik files. Metadata: 1. Every outputted .unik file contains the sequence/reference ID, chunk index, number of chunks, and genome size of reference. 2. When parsing whole sequence files or splitting by the number of chunks, the identifier of a reference is the basename of the input file by default. It can also be extracted from the input file name via -N/--ref-name-regexp, e.g., \"^(\\w{3}_\\d{9}\\.\\d+)\" for RefSeq records. Output: 1. All outputted .unik files are saved in ${outdir}, with path ${outdir}/xxx/yyy/zzz/${infile}-id_${seqID}.unik where dirctory tree '/xxx/yyy/zzz/' is built for > 1000 output files. 2. For splitting sequence mode (--split-size > 0 or --split-number > 0), output files are: ${outdir}//xxx/yyy/zzz/${infile}/{seqID}-chunk_${chunkIdx}.unik 3. A summary file (\"${outdir}/_info.txt\") is generated for later use. Users need to check if the reference IDs (column \"name\") are what supposed to be. Performance tips: 1. Decrease the value of -j/--threads for data in hard disk drives to reduce I/O pressure. Next step: 1. Check the summary file (${outdir}/_info.txt) to see if the reference IDs (column \"name\") are what supposed to be. 2. Run \"kmcp index\" with the output directory. Examples: 1. From few sequence files: kmcp compute -k 21 -n 5 -l 100 -O tmp-k21-n5-l100 NC_045512.2.fna.gz 2. From a list file: kmcp compute -k 21 -n 10 -l 100 -O tmp-k21-10-l100 -i list.txt 3. From a directory containing many sequence files: kmcp compute -k 21 -n 10 -l 100 -B plasmid \\ -O gtdb-k21-n10-l100 -I gtdb-genomes/ Usage: kmcp compute [flags] [-k <k>] [-n <chunks>] [-l <overlap>] {[-I <seqs dir>] | <seq files>} -O <out dir> Flags: --by-seq \u25ba Compute k-mers (sketches) for each sequence, instead of the whole file. --circular \u25ba Input sequences are circular. -c, --compress \u25ba Output gzipped .unik files, it's slower and can save little space. -r, --file-regexp string \u25ba Regular expression for matching sequence files in -I/--in-dir, case ignored. (default \"\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") --force \u25ba Overwrite existed output directory. -h, --help help for compute -I, --in-dir string \u25ba Directory containing FASTA/Q files. Directory symlinks are followed. -k, --kmer ints \u25ba K-mer size(s). (default [21]) -W, --minimizer-w int \u25ba Minimizer window size. -O, --out-dir string \u25ba Output directory. -N, --ref-name-regexp string \u25ba Regular expression (must contains \"(\" and \")\") for extracting reference name from filename. (default \"(?i)(.+)\\\\.(f[aq](st[aq])?|fna)(.gz)?$\") -D, --scale int \u25ba Scale of the FracMinHash (Scaled MinHash), or down-sample factor for Syncmers and Minimizer. (default 1) -B, --seq-name-filter strings \u25ba List of regular expressions for filtering out sequences by header/name, case ignored. -m, --split-min-ref int \u25ba Only splitting sequences >= X bp. (default 1000) -n, --split-number int \u25ba Chunk number for splitting sequences, incompatible with -s/--split-size. -l, --split-overlap int \u25ba Chunk overlap for splitting sequences. -s, --split-size int \u25ba Chunk size for splitting sequences, incompatible with -n/--split-number. -S, --syncmer-s int \u25ba Length of the s-mer in Closed Syncmers.","title":"compute"},{"location":"usage/#index","text":"Construct database from k-mer files We build the index for k-mers (sketches) with a modified compact bit-sliced signature index (COBS). We totally rewrite the algorithms, data structure, and file format, and have improved the indexing and searching speed. Input: The output directory generated by \"kmcp compute\". Database size and searching accuracy: 0. Use --dry-run to adjust parameters and check the final number of index files (#index-files) and the total file size. 1. -f/--false-positive-rate: the default value 0.3 is enough for a query with tens of k-mers (see BIGSI/COBS paper). Small values could largely increase the size of the database. 2. -n/--num-hash: large values could reduce the database size, at the cost of a slower searching speed. Values <=4 are recommended. 3. The value of block size -b/--block-size better to be multiple of 64. The default value is: (#unikFiles/#threads + 7) / 8 * 8 4. Use flag -x/--block-sizeX-kmers-t, -8/--block-size8-kmers-t, and -1/--block-size1-kmers-t to separately create indexes for inputs with huge number of k-mers, for precise control of database size. References: 1. COBS: https://arxiv.org/abs/1905.09624 Taxonomy data: 1. No taxonomy data are included in the database. 2. Taxonomy information are only needed in \"profile\" command. Performance tips: 1. The number of blocks (.uniki files) better to be smaller than or equal to the number of CPU cores for faster searching speed. We can set -j/--threads to control blocks number. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. 2. #threads files are simultaneously opened, and max number of opened files is limited by the flag -F/--max-open-files. You may use a small value of -F/--max-open-files for hard disk drive storages. 3. When the database is used in a new computer with more CPU cores, 'kmcp search' could automatically scale to utilize as many cores as possible. Examples: 1. For bacteria genomes: kmcp index -f 0.3 -n 1 -j 32 -I gtdb-k21-n10-l100/ -O gtdb.kmcp 2. For viruses, use -x and -8 to control index size of the largest chunks: kmcp index -f 0.05 -n 1 -j 32 -x 100K -8 1M \\ -I genbank-viral-k21-n5-l100/ -O genbank-viral.kmcp Usage: kmcp index [flags] [-f <fpr>] [-n <hashes>] [-j <blocks>] -I <compute output> -O <kmcp db> Flags: -a, --alias string \u25ba Database alias/name. (default: basename of --out-dir). You can also manually edit it in info file: ${outdir}/__db.yml. -b, --block-size int \u25ba Block size, better be multiple of 64 for large number of input files. (default: min(#.files/#theads, 8)) -1, --block-size1-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, an individual index is created for this file. Supported units: K, M, G. (default \"200M\") -8, --block-size8-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to 8. Supported units: K, M, G. (default \"20M\") -X, --block-sizeX int \u25ba If k-mers of single .unik file exceeds --block-sizeX-kmers-t, block size is changed to this value. (default 256) -x, --block-sizeX-kmers-t string \u25ba If k-mers of single .unik file exceeds this threshold, block size is changed to --block-sizeX. Supported units: K, M, G. (default \"10M\") --dry-run \u25ba Dry run, useful for adjusting parameters (highly recommended). -f, --false-positive-rate float \u25ba False positive rate of the bloom filters, range: (0, 1). (default 0.3) --file-regexp string \u25ba Regular expression for matching files in -I/--in-dir, case ignored. (default \".unik$\") --force \u25ba Overwrite existed output directory. -h, --help help for index -I, --in-dir string \u25ba Directory containing .unik files. Directory symlinks are followed. -F, --max-open-files int \u25ba Maximal number of opened files, please use a small value for hard disk drive storage. (default 256) -n, --num-hash int \u25ba Number of hash functions in bloom filters. (default 1) -O, --out-dir string \u25ba Output directory. (default: ${indir}.kmcp-db)","title":"index"},{"location":"usage/#search","text":"Search sequences against a database Attentions: 1. Input format should be (gzipped) FASTA or FASTQ from files or stdin. - Paired-end files should be given via -1/--read1 and -2/--read2. kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz - Single-end can be given as positional arguments or -1/-2. kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz **Single-end mode is recommended for paired-end reads, for higher sensitivity**. 2. A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold to remove duplicates. 3. For long reads or contigs, you should split them into short reads using \"seqkit sliding\", e.g., seqkit sliding -s 100 -W 300 Shared flags between \"search\" and \"profile\": 1. -t/--min-query-cov. Index files loading modes: 1. Using memory-mapped index files with mmap (default): - Faster startup speed when index files are buffered in memory. - Multiple KMCP processes can share the memory. 2. Loading the whole index files into memory (-w/--load-whole-db): - This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. - It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases. - Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storages (NAS). 3. Low memory mode (--low-mem): - Do not load all index files into memory nor use mmap, using file seeking. - It's much slower, >4X slower on SSD and would be much slower on HDD disks. - Only use this mode for small number of queries or a huge database that can't be loaded into memory. Output format: Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging The values of tCov and jacc in results only apply to databases built with a single size of k-mer. Performance tips: 1. Increase the value of -j/--threads for acceleratation, but values larger than the number of CPU cores won't bring extra speedup. 2. When more threads (>= 1.3 * #blocks) are given, extra workers are automatically created. Examples: 1. Single-end mode (recommended) kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz sample_1_unpaired.fq.gz sample_2_unpaired.fq.gz 2. Paired-end mode kmcp search -d gtdb.kmcp -o sample.kmcp@gtdb.kmcp.tsv.gz \\ -1 sample_1.fq.gz -2 sample_2.fq.gz 3. In computer cluster, where databases are saved in NAS storages. kmcp search -w -d gtdb.n16-00.kmcp -o sample.kmcp@gtdb.n16-00.kmcp.tsv.gz \\ sample_1.fq.gz sample_2.fq.gz Usage: kmcp search [flags] [-w] -d <kmcp db> [-t <min-query-cov>] [read1.fq.gz] [read2.fq.gz] [unpaired.fq.gz] [-o read.tsv.gz] Flags: -d, --db-dir string \u25ba Database directory created by \"kmcp index\". Please add -w/--load-whole-db for databases on network-attached storages (NAS), e.g., a computer cluster environment. -D, --default-name-map \u25ba Load ${db}/__name_mapping.tsv for mapping name first. -S, --do-not-sort \u25ba Do not sort matches of a query. -h, --help help for search -n, --keep-top-scores int \u25ba Keep matches with the top N scores for a query, 0 for all. -K, --keep-unmatched \u25ba Keep unmatched query sequence information. -u, --kmer-dedup-threshold int \u25ba Remove duplicated kmers for a query with >= X k-mers. (default 256) -w, --load-whole-db \u25ba Load all index files into memory, it's faster for small databases but needs more memory. Use this for databases on network-attached storages (NAS). Please read \"Index files loading modes\" in \"kmcp search -h\". --low-mem \u25ba Do not load all index files into memory nor use mmap, the searching would be very very slow for a large number of queries. Please read \"Index files loading modes\" in \"kmcp search -h\". -f, --max-fpr float \u25ba Maximal false positive rate of a query. (default 0.05) -c, --min-kmers int \u25ba Minimal number of matched k-mers (sketches). (default 10) -t, --min-query-cov float \u25ba Minimal query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. (default 0.55) -m, --min-query-len int \u25ba Minimal query length. (default 30) -T, --min-target-cov float \u25ba Minimal target coverage, i.e., proportion of matched k-mers and unique k-mers of a target. -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to user-defined values. Don't use this if you will use the result for metagenomic profiling which needs the original reference IDs. -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") --query-id string \u25ba Custom query Id when using the whole file as a query. -g, --query-whole-file \u25ba Use the whole file as a query, e.g., for genome similarity estimation against k-mer sketch database. -1, --read1 string \u25ba (Gzipped) read1 file. -2, --read2 string \u25ba (Gzipped) read2 file. -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\") --try-se \u25ba If paired-end reads have no hits, re-search with read1, if still fails, try read2. -G, --use-filename \u25ba Use file name as query ID when using the whole file as a query.","title":"search"},{"location":"usage/#merge","text":"Merge search results from multiple databases Input: *. Searching results of the same reads in different databases. *. The order of multiple input reads files should be the same during searching. *. When only one input given, we just copy and write to the input file. This is friendly to workflows which assume multiple inputs are given. Example: kmcp merge -o search.kmcp.tsv.gz search.kmcp@*.kmcp.tsv.gz Usage: kmcp merge [flags] [-o read.tsv.gz] [<search results> ...] Flags: -n, --field-hits int \u25ba Field of hits. (default 5) -f, --field-queryIdx int \u25ba Field of queryIdx. (default 15) -h, --help help for merge -H, --no-header-row \u25ba Do not print header row. -o, --out-file string \u25ba Out file, supports and recommends a \".gz\" suffix (\"-\" for stdout). (default \"-\") -s, --sort-by string \u25ba Sort hits by \"qcov\", \"tcov\" or \"jacc\" (Jaccard Index). (default \"qcov\")","title":"merge"},{"location":"usage/#profile","text":"Generate taxonomic profile from search results Methods: 1. Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimal proportion of matched chunks (-p/--min-chunks-fraction). (***highly recommended***) Another flag -d/--max-chunks-depth-stdev further reduces false positives. 2. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. 3. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positive of ambiguous matches. You can also disable this step by the flag --no-amb-corr. If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. 4. Multi-aligned queries are proportionally assigned to references with a similar strategy in Metalign. 5. Input files are parsed four times, therefore STDIN is not supported. Reference: 1. MegaPath: https://doi.org/10.1186/s12864-020-06875-6 2. Metalign: https://doi.org/10.1186/s13059-020-02159-0 Accuracy notes: *. Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate (-f/--max-fpr) of a query. *. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 *. -U/--min-hic-ureads, minimal number, >= 1 *. -H/--min-hic-ureads-qcov, minimal query coverage, >= -t/--min-qcov *. -P/--min-hic-ureads-prop, minimal proportion, higher values increase precision at the cost of sensitivity. *. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambiguous reads with the algorithm in MegaPath. *. --keep-perfect-matches is not recommended, which decreases sensitivity. *. --keep-main-matches is not recommended, which affects accuracy of abundance estimation. *. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation. Profiling modes: We preset six profiling modes, available with the flag -m/--mode: - 0 (for pathogen detection) - 1 (higher recall) - 2 (high recall) - 3 (default) - 4 (high precision) - 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.55 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use \"taxonkit create-taxdump\" (https://github.com/shenwei356/taxonkit) to create NCBI-style taxdump files, which also generates a TaxId mapping file. Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. *3. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results. Profiling output formats: 1. KMCP (-o/--out-prefix) Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please output CAMI or MetaPhlAn format. 2. CAMI (-M/--metaphlan-report, --metaphlan-report-version, -s/--sample-id, --taxonomy-id) Related tools (https://github.com/shenwei356/taxonkit): - taxonkit profile2cami: convert any metagenomic profile table with TaxIds to CAMI format. Use this if you forget to output CAMI format. - taxonkit cami-filter: remove taxa of given TaxIds and their descendants in CAMI metagenomic profile. 3. MetaPhlAn (-C/--cami-report, -s/--sample-id) KMCP format: Tab-delimited format with 16 columns: 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. score, The 90th percentile of qCov of uniquely matched reads 4. chunksFrac, Genome chunks fraction 5. chunksRelDepth, Relative depths of reference chunks 6. chunksRelDepthStd, The strandard deviation of chunksRelDepth 7. reads, Total number of matched reads of this reference 8. ureads, Number of uniquely matched reads 9. hicureads, Number of uniquely matched reads with high-confidence 10. refsize, Reference size 11. refname, Reference name, optional via name mapping file 12. taxid, TaxId of the reference 13. rank, Taxonomic rank 14. taxname, Taxonomic name 15. taxpath, Complete lineage 16. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Taxonomic binning formats: 1. CAMI (-B/--binning-result) Examples: 1. Default mode: kmcp profile -X taxdump/ -T taxid.map -m 3 \\ sample.kmcp.tsv.gz -o sample.k.profile \\ -C sample.c.profile -s sample 2. For pathogen detection (you may create databases with lower FPR, e.g., kmcp index -f 0.1 -n 2 for bacteria and fungi genomes, and search with low k-mer coverage threshold -t 0.4): kmcp profile -X taxdump/ -T taxid.map -m 3 -t 0.4 \\ sample.kmcp.tsv.gz -o sample.k.profile Usage: kmcp profile [flags] [-X <taxdump dir>] [-T <taxid.map>] [-m <mode>] [-o <kmcp profile>] <search results> Flags: -B, --binning-result string \u25ba Save extra binning result in CAMI report. -C, --cami-report string \u25ba Save extra CAMI-like report. --debug string \u25ba Debug output file. -F, --filter-low-pct float \u25ba Filter out predictions with the smallest relative abundances summing up X%. Range: [0,100). -h, --help help for profile --keep-main-matches \u25ba Only keep main matches, abandon matches with sharply decreased qcov (> --max-qcov-gap). --keep-perfect-matches \u25ba Only keep the perfect matches (qcov == 1) if there are. -n, --keep-top-qcovs int \u25ba Keep matches with the top N qcovs for a query, 0 for all. --level string \u25ba Level to estimate abundance at. Available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp profile -h\" for details. (default 5000) -d, --max-chunks-depth-stdev float \u25ba Maximal standard deviation of relative depths of all chunks. (default 2) -f, --max-fpr float \u25ba Maximal false positive rate of a read in search result. (default 0.05) -R, --max-mismatch-err float \u25ba Maximal error rate of a read being matched to a wrong reference, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) --max-qcov-gap float \u25ba Max qcov gap between adjacent matches. (default 0.4) -M, --metaphlan-report string \u25ba Save extra metaphlan-like report. --metaphlan-report-version string \u25ba Metaphlan report version (2 or 3) (default \"3\") -p, --min-chunks-fraction float \u25ba Minimal fraction of matched reference chunks with reads >= -r/--min-chunks-reads. (default 0.8) -r, --min-chunks-reads int \u25ba Minimal number of reads for a reference chunk. (default 50) -D, --min-dreads-prop float \u25ba Minimal proportion of distinct reads, for determing the right reference for ambiguous reads. Range: (0, 1). (default 0.05) -U, --min-hic-ureads int \u25ba Minimal number of high-confidence uniquely matched reads for a reference. (default 5) -P, --min-hic-ureads-prop float \u25ba Minimal proportion of high-confidence uniquely matched reads. (default 0.1) -H, --min-hic-ureads-qcov float \u25ba Minimal query coverage of high-confidence uniquely matched reads. (default 0.75) -t, --min-query-cov float \u25ba Minimal query coverage of a read in search result. (default 0.55) -u, --min-uniq-reads int \u25ba Minimal number of uniquely matched reads for a reference. (default 20) -m, --mode int \u25ba Profiling mode, type \"kmcp profile -h\" for details. available values: 0 (for pathogen detection), 1 (higherrecall), 2 (high recall), 3 (default), 4 (high precision), 5 (higher precision). (default 3) -N, --name-map strings \u25ba Tabular two-column file(s) mapping reference IDs to reference names. --no-amb-corr \u25ba Do not correct ambiguous reads. Use this flag to reduce analysis time if the stage 1/4 produces thousands of candidates. --norm-abund string \u25ba Method for normalize abundance of a reference by the mean/min/max abundance in all chunks, available values: mean, min, max. (default \"mean\") -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") --rank-prefix strings \u25ba Prefixes of taxon name in certain ranks, used with --metaphlan-report. (default [k__,p__,c__,o__,f__,g__,s__,t__]) -s, --sample-id string \u25ba Sample ID in result file. -S, --separator string \u25ba Separator of TaxIds and taxonomy names. (default \";\") --show-rank strings \u25ba Only show TaxIds and names of these ranks. (default [superkingdom,phylum,class,order,family,genus,species,strain]) -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds. --taxonomy-id string \u25ba Taxonomy ID in result file.","title":"profile"},{"location":"usage/#utils","text":"Some utilities Usage: kmcp utils [command] Available Commands: cov2simi Convert k-mer coverage to sequence similarity filter Filter search results and find species/assembly-specific queries index-info Print information of index file merge-regions Merge species/assembly-specific regions query-fpr Compute the maximal false positive rate of a query unik-info Print information of .unik file","title":"utils"},{"location":"usage/#filter","text":"Filter search results and find species/assembly-specific queries Taxonomy data: 1. Mapping references IDs to TaxIds: -T/--taxid-map 2. NCBI taxonomy dump files: -X/--taxdump Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils filter [flags] Flags: -h, --help help for filter --level string \u25ba Level to filter. available values: species, strain/assembly. (default \"species\") --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils filter -h\" for details. (default 5000) -f, --max-fpr float \u25ba Maximal false positive rate of a read in search result. (default 0.05) -t, --min-query-cov float \u25ba Minimal query coverage of a read in search result. (default 0.55) -H, --no-header-row \u25ba Do not print header row. -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") -X, --taxdump string \u25ba Directory of NCBI taxonomy dump files: names.dmp, nodes.dmp, optional with merged.dmp and delnodes.dmp. -T, --taxid-map strings \u25ba Tabular two-column file(s) mapping reference IDs to TaxIds.","title":"filter"},{"location":"usage/#index-info","text":"Print information of index file Usage: kmcp utils index-info [flags] Flags: -a, --all \u25ba Show all information. -b, --basename \u25ba Only output basenames of files. -h, --help help for index-info -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\")","title":"index-info"},{"location":"usage/#merge-regions","text":"Merge species/assembly-specific regions Steps: # 1. Simulating reads and searching on one or more databases. seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db1.kmcp -o ref.fna.gz.kmcp@db1.tsv.gz seqkit sliding --step 10 --window 100 ref.fna.gz \\ | kmcp search -d db2.kmcp -o ref.fna.gz.kmcp@db2.tsv.gz # 2. Merging and filtering searching results kmcp merge ref.fna.gz.kmcp@*.tsv.gz \\ | kmcp utils filter -X taxdump -T taxid.map \\ -o ref.fna.gz.kmcp.uniq.tsv.gz # 3. Merging regions. # Here the value of --min-overlap should be k-1. kmcp utils merge-regions --min-overlap 20 ref.fna.gz.kmcp.uniq.tsv.gz \\ -o ref.fna.gz.kmcp.uniq.tsv.gz.bed Output (BED6 format): 1. chrom - chromosome name 2. chromStart - starting position (0-based) 3. chromEnd - ending position (0-based) 4. name - \"species-specific\" or \"assembly-specific\" 5. score - 0-1000, 1000 for \"assembly-specific\", others for \"\"species-specific\" 6. strand - \".\" Performance notes: 1. Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size. 2. However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. Usage: kmcp utils merge-regions [flags] Flags: -h, --help help for merge-regions -I, --ignore-type \u25ba Merge species and assembly-specific regions. -f, --max-fpr float \u25ba Maximal false positive rate of a read in search result. (default 0.05) -g, --max-gap int \u25ba Maximal distance of starting positions of two adjacent regions, 0 for no limitation, 1 for no merging. --line-chunk-size int \u25ba Number of lines to process for each thread, and 4 threads is fast enough. Type \"kmcp utils merge-regions -h\" for details. (default 5000) -l, --min-overlap int \u25ba Minimal overlap of two adjacent regions, recommend K-1. (default 1) -t, --min-query-cov float \u25ba Minimal query coverage of a read in search result. (default 0.55) -a, --name-assembly string \u25ba Name of assembly-specific regions. (default \"assembly-specific\") -s, --name-species string \u25ba Name of species-specific regions. (default \"species-specific\") -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") -r, --regexp string \u25ba Regular expression for extract reference name and query locations. (default \"^(.+)_sliding:(\\\\d+)\\\\-(\\\\d+)$\")","title":"merge-regions"},{"location":"usage/#unik-info","text":"Print information of .unik file Tips: 1. For lots of small files (especially on SDD), use big value of '-j' to parallelize counting. Usage: kmcp utils unik-info [flags] Flags: -a, --all \u25ba All information, including the number of k-mers. -b, --basename \u25ba Only output basename of files. -h, --help help for unik-info -o, --out-file string \u25ba Out file (\"-\" for stdout, suffix .gz for gzipped out.) (default \"-\") -e, --skip-err \u25ba Skip error, only show warning message. --symbol-false string \u25ba Smybol for false. (default \"\u2715\") --symbol-true string \u25ba Smybol for true. (default \"\u2713\") -T, --tabular \u25ba Output in machine-friendly tabular format.","title":"unik-info"},{"location":"usage/#cov2simi","text":"Convert k-mer coverage to sequence similarity similarity = 87.456 + 26.410*qcov - 22.008*qcov*qcov + 7.325*qcov*qcov*qcov Usage: kmcp utils cov2simi [flags] Flags: -h, --help help for cov2simi -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\") -t, --query-cov float \u25ba K-mer query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. range: [0, 1]","title":"cov2simi"},{"location":"usage/#query-fpr","text":"Compute the the maximimal false positive rate of a query Solomon and Kingsford apply a Chernoff bound and show that the false positive probability for a query is: fpr \u2264 exp( -n(t-f)^2 / (2(1-f)) ) Where: f, the false positive rate of the bloom filters t, the minimal proportion of matched k-mers and unique k-mers of a query n, the number of unique k-mers of the query Reference: 1. SBT: https://doi.org/10.1038/nbt.3442 2. COBS: https://arxiv.org/abs/1905.09624v2 Usage: kmcp utils query-fpr [flags] Flags: -f, --false-positive-rate float \u25ba False positive rate of the bloom filters in the database. range: (0, 1) (default 0.3) -h, --help help for query-fpr -t, --min-query-cov float \u25ba Minimal query coverage, i.e., proportion of matched k-mers and unique k-mers of a query. range: [0, 1] (default 0.55) -n, --num-kmers int \u25ba Number of unique k-mers of the query. (default 80) -o, --out-prefix string \u25ba Out file prefix (\"-\" for stdout). (default \"-\")","title":"query-fpr"},{"location":"usage/#autocompletion","text":"Generate shell autocompletion script Supported shell: bash|zsh|fish|powershell Bash: # generate completion shell kmcp autocompletion --shell bash # configure if never did. # install bash-completion if the \"complete\" command is not found. echo \"for bcfile in ~/.bash_completion.d/* ; do source \\$bcfile; done\" >> ~/.bash_completion echo \"source ~/.bash_completion\" >> ~/.bashrc Zsh: # generate completion shell kmcp autocompletion --shell zsh --file ~/.zfunc/_kmcp # configure if never did echo 'fpath=( ~/.zfunc \"${fpath[@]}\" )' >> ~/.zshrc echo \"autoload -U compinit; compinit\" >> ~/.zshrc fish: kmcp autocompletion --shell fish --file ~/.config/fish/completions/kmcp.fish Usage: kmcp autocompletion [flags] Flags: --file string autocompletion file (default \"/home/shenwei/.bash_completion.d/kmcp.sh\") -h, --help help for autocompletion --shell string autocompletion type (bash|zsh|fish|powershell) (default \"bash\")","title":"autocompletion"},{"location":"benchmark/","text":"","title":"Index"},{"location":"benchmark/profiling/","text":"Benchmark Please read the preprint: KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. bioRxiv 2022.03.07.482835; doi: https://doi.org/10.1101/2022.03.07.482835","title":"Taxonomic profiling"},{"location":"benchmark/profiling/#benchmark","text":"Please read the preprint: KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping . Wei Shen, Hongyan Xiang, Tianquan Huang, Hui Tang, Mingli Peng, Dachuan Cai, Peng Hu, Hong Ren. bioRxiv 2022.03.07.482835; doi: https://doi.org/10.1101/2022.03.07.482835","title":"Benchmark"},{"location":"benchmark/searching/","text":"Searching benchmarks Software, datasets and commands details . Softwares COBS ( 1915fc0 ) Sourmash (v4.2.2) KMCP ( v0.8.0 ) GTDB r202 representative genomes are used for tests: file size: 46.26 GB files: 47,894 bases: 151.94 Gb KMCP vs COBS All k-mers are indexed and searched. Database size and building time: cobs kmcp database size 86.96GB 55.15GB building time 29m:55s 24min52s temporary files 160.76GB 1.19TB Searching with bacterial genomes or short reads (~1M reads). KMCP vs Mash and Sourmash Only FracMinHash (Scaled MinHash) (scale=1000 for Sourmash and KMCP) or MinHash (scale=3400 for Mash) are indexed and searched. Database size and building time: mash sourmash kmcp database size 743MB 5.19GB 1.52GB building time 11m39s 89m59s 7min02s temporary files - - 3.41GB Searching with bacterial genomes. Result","title":"Sequence and genome searching"},{"location":"benchmark/searching/#searching-benchmarks","text":"Software, datasets and commands details . Softwares COBS ( 1915fc0 ) Sourmash (v4.2.2) KMCP ( v0.8.0 ) GTDB r202 representative genomes are used for tests: file size: 46.26 GB files: 47,894 bases: 151.94 Gb","title":"Searching benchmarks"},{"location":"benchmark/searching/#kmcp-vs-cobs","text":"All k-mers are indexed and searched. Database size and building time: cobs kmcp database size 86.96GB 55.15GB building time 29m:55s 24min52s temporary files 160.76GB 1.19TB Searching with bacterial genomes or short reads (~1M reads).","title":"KMCP vs COBS"},{"location":"benchmark/searching/#kmcp-vs-mash-and-sourmash","text":"Only FracMinHash (Scaled MinHash) (scale=1000 for Sourmash and KMCP) or MinHash (scale=3400 for Mash) are indexed and searched. Database size and building time: mash sourmash kmcp database size 743MB 5.19GB 1.52GB building time 11m39s 89m59s 7min02s temporary files - - 3.41GB Searching with bacterial genomes.","title":"KMCP vs Mash and Sourmash"},{"location":"benchmark/searching/#result","text":"","title":"Result"},{"location":"tutorial/","text":"Tutorials Taxonomic profiling Sequence and genome searching","title":"Tutorials"},{"location":"tutorial/#tutorials","text":"Taxonomic profiling Sequence and genome searching","title":"Tutorials"},{"location":"tutorial/profiling/","text":"Metagenomic profiling Requirements Database Prebuilt databases are available. Or build custom databases . Hardware. CPU: \u2265 32 cores preferred. RAM: \u2265 64 GB, depends on file size of the maximal database. Datasets Short reads, single or paired end. Steps Step 1. Preprocessing reads For example, removing adapters and trimming using fastp : fastp -i in_1.fq.gz -I in_2.fq.gz \\ -o out_1.fq.gz -O out_2.fq.gz \\ -l 75 -q 20 -W 4 -M 20 -3 20 --thread 32 \\ --html out.fastp.html Step 2. Removing host reads Tools: bowtie2 is recommended for removing host reads. samtools is also used for processing reads mapping file. pigz is a parallel implementation of gzip , which is much faster than gzip . Host reference genomes: Human: CHM13 . We also provide a database of CHM13 for fast removing human reads. Building the index (~60min): bowtie2-build --threads 32 GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz chm13 Mapping and removing mapped reads: index=~/ws/db/bowtie2/chm13 bowtie2 --threads 32 -x $index -1 in_1.fq.gz -2 in_2.fq.gz \\ | samtools view -buS -f 4 - \\ | samtools fastq - \\ | gzip -c > sample.fq.gz Step 3. Searching Reads can be searched against multiple databases which can be built with different parameters , and the results can be fastly merged for downstream analysis. Attentions Input format should be (gzipped) FASTA or FASTQ from files or stdin. Paired-End reads should be given via -1/--read1 and -2/--read2 . kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz Single-end can be given as positional arguments or -1 / -2 . kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz Single-end mode is recommended for paired-end reads, for higher sensitivity . See benchmark . A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold (default 256 ) to remove duplicates. For long reads or contigs, you should split them into short reads using seqkit sliding , e.g., seqkit sliding -s 100 -W 300 The values of tCov and jacc in results only apply to databases built with a single size of k-mer. kmcp search and kmcp profile share some flags , therefore users can use stricter criteria in kmcp profile . -t/--min-query-cov , minimal query coverage, i.e., proportion of matched k-mers and unique k-mers of a query (default 0.55 , close to ~96.5% sequence similarity) Index files loading modes Using memory-mapped index files with mmap (default): Faster startup speed when index files are buffered in memory. Multiple KMCP processes can share the memory. Loading the whole index files into memory ( -w/--load-whole-db ): This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases . Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS) . Low memory mode ( --low-mem ): Do not load all index files into memory nor use mmap, using file seeking. It's much slower, >4X slower on SSD and would be much slower on HDD disks. Only use this mode for small number of queries or a huge database that can't be loaded into memory . Performance tips : Increase the value of -j/--threads for acceleratation, but values larger than the the number of CPU cores won't bring extra speedup. Commands Single-end mode is recommended for paired-end reads, for higher sensitivity : # --------------------------------------------------- # single-end (recommended) read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ $read1 $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz Pair-end reads: # --------------------------------------------------- # paired-end read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ --read1 $read1 \\ --read2 $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz Search result format Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging Note: The header line starts with # , you need to assign another comment charactor if using csvtk for analysis. e.g., csvtk filter2 -C '$' -t -f '$qCov > 0.7' mock.fastq.gz.kmcp.gz Demo result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx NC_000913.3_sliding:1244941-1245090 150 120 1.5955e-26 6 NC_012971.2 2 10 4558953 31 120 1.0000 0.0003 0.0003 0 NC_000913.3_sliding:1244941-1245090 150 120 1.5955e-26 6 NC_000913.3 2 10 4641652 31 120 1.0000 0.0003 0.0003 0 NC_000913.3_sliding:1244941-1245090 150 120 1.5955e-26 6 NC_018658.1 5 10 5273097 31 120 1.0000 0.0002 0.0002 0 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_012971.2 0 10 4558953 31 120 1.0000 0.0003 0.0003 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_000913.3 0 10 4641652 31 120 1.0000 0.0003 0.0003 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_013654.1 0 10 4717338 31 120 1.0000 0.0003 0.0003 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NZ_CP007592.1 1 10 5104557 31 120 1.0000 0.0002 0.0002 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_018658.1 7 10 5273097 31 120 1.0000 0.0002 0.0002 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NZ_CP028116.1 0 10 5648177 31 120 1.0000 0.0002 0.0002 1 Searching on a computer cluster Here, we split genomes of GTDB into 16 partitions and build a database for every partition, so we can use computer cluster to accelerate the searching. The genbank-viral genomes are also diveded into 4 partition. A helper script easy_sbatch is used for batch submitting Slurm jobs via script templates. # --------------------------------------------------- # searching j=32 reads=reads # ----------------- # gtdb dbprefix=~/ws/db/kmcp/gtdb.n16- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # viral dbprefix=~/ws/db/kmcp/genbank-viral.n4- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # fungi dbprefix=~/ws/db/kmcp/refseq-fungi for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # --------------------------------------------------- # wait all job being done # --------------------------------------------------- # merge result and profiling # merge results # there's no need to submit to slurm, which could make it slower, cause the bottleneck is file IO for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') echo $prefix; date kmcp merge $prefix.kmcp@*.tsv.gz --out-file $prefix.kmcp.tsv.gz \\ --quiet --log $prefix.kmcp.tsv.gz.merge.log done # profiling X=taxdump/ T=taxid.map fd kmcp.tsv.gz$ $reads/ \\ | rush -v X=$X -v T=$T \\ 'kmcp profile -X {X} -T {T} {} -o {}.k.profile -C {}.c.profile -s {%:} \\ --log {}.k.profile.log' Step 4. Profiling Input TaxId mapping file(s). Taxdump files. KMCP search results. Methods Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimal proportion of matched chunks ( -p/--min-chunks-fraction ). ( highly recommended ) Another flag -d/--max-chunks-cov-stdev further reduces false positives. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positive of ambiguous matches. You can also disable this step by the flag --no-amb-corr . If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. Multi-aligned queries are proportionally assigned to references with a similar strategy in Metalign . Input files are parsed 4 times, therefore STDIN is not supported. Three-rounds profiling: Accuracy notes : Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate ( -f/--max-fpr ) of a query. And we require part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence to decrease the false positive. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 -U/--min-hic-ureads , minimal number, >= 1 -H/--min-hic-ureads-qcov , minimal query coverage, >= -t/--min-qcov -P/--min-hic-ureads-prop , minimal proportion, higher values increase precision at the cost of sensitivity. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambigous reads with the algorithm in MegaPath. --keep-perfect-match is not recommended, which decreases sensitivity. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation. Profiling modes We preset six profiling modes, available with the flag -m/--mode . 0 (for pathogen detection) 1 (higher recall) 2 (high recall) 3 (default) 4 (high precision) 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.55 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data : Mapping references IDs to TaxIds: -T/--taxid-map NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use taxonkit create-taxdump to create NCBI-style taxdump files, which also generates a TaxId mapping file. Performance notes : Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size . However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results. Commands # taxid mapping files, multiple files supported. taxid_map=gtdb.kmcp/taxid.map,refseq-viral.kmcp/taxid.map,refseq-fungi.kmcp/taxid.map # or concatenate them into a big taxid.map # cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map # taxid_map=taxid.map # taxdump directory taxdump=taxdump sfile=$file.kmcp.tsv.gz kmcp profile \\ --taxid-map $taxid_map \\ --taxdump $taxdump/ \\ --level species \\ --min-query-cov 0.55 \\ --min-chunks-reads 50 \\ --min-chunks-fraction 0.8 \\ --max-chunks-depth-stdev 2 \\ --min-uniq-reads 20 \\ --min-hic-ureads 5 \\ --min-hic-ureads-qcov 0.75 \\ --min-hic-ureads-prop 0.1 \\ $sfile \\ --out-prefix $sfile.kmcp.profile \\ --metaphlan-report $sfile.metaphlan.profile \\ --cami-report $sfile.cami.profile \\ --sample-id \"0\" \\ --binning-result $sfile.binning.gz Profiling result formats Supported profiling output formats : KMCP ( -o/--out-prefix ). Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please output CAMI or MetaPhlAn format . CAMI ( -M/--metaphlan-report , --metaphlan-report-version , sample name: -s/--sample-id , taxonomy data: --taxonomy-id ) MetaPhlAn ( -C/--cami-report , sample name: -s/--sample-id )) Supported taxonomic binning formats : CAMI ( -B/--binning-result ) KMCP format : Tab-delimited format with 16 columns: 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. score, The 90th percentile of qCov of uniquely matched reads 4. chunksFrac, Genome chunks fraction 5. chunksRelDepth, Relative depths of reference chunks 6. chunksRelDepthStd, The strandard deviation of chunksRelDepth 7. reads, Total number of matched reads of this reference 8. ureads, Number of uniquely matched reads 9. hicureads, Number of uniquely matched reads with high-confidence 10. refsize, Reference size 11. refname, Reference name, optional via name mapping file 12. taxid, TaxId of the reference 13. rank, Taxonomic rank 14. taxname, Taxonomic name 15. taxpath, Complete lineage 16. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Demo output: ref percentage score chunksFrac chunksRelDepth chunksRelDepthStd reads ureads hicureads refsize refname taxid rank taxname taxpath taxpathsn NC_013654.1 48.321535 100.00 1.00 0.99;1.00;0.99;1.00;0.99;0.99;0.99;0.98;1.05;1.02 0.02 287936 226225 226225 4717338 431946 strain Escherichia coli SE15 Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli;Escherichia coli SE15 2;1224;1236;91347;543;561;562;431946 NC_000913.3 46.194629 100.00 1.00 1.04;0.99;1.00;1.00;0.99;0.99;0.99;0.97;1.04;0.98 0.02 270846 175686 175686 4641652 511145 no rank Escherichia coli str. K-12 substr. MG1655 Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli;Escherichia coli K-12 2;1224;1236;91347;543;561;562;83333 NC_002695.2 5.014025 100.00 1.00 0.97;0.98;0.92;1.12;1.01;0.95;1.00;1.01;1.03;1.00 0.05 34825 22945 22945 5498578 386585 strain Escherichia coli O157:H7 str. Sakai Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli;Escherichia coli O157:H7 str. Sakai 2;1224;1236;91347;543;561;562;386585 NC_010655.1 0.469811 100.00 1.00 1.03;0.87;0.90;0.98;1.15;1.17;0.90;0.96;0.96;1.09 0.10 1581 1581 1581 2664102 349741 strain Akkermansia muciniphila ATCC BAA-835 Bacteria;Verrucomicrobia;Verrucomicrobiae;Verrucomicrobiales;Akkermansiaceae;Akkermansia;Akkermansia muciniphila;Akkermansia muciniphila ATCC BAA-835 2;74201;203494;48461;1647988;239934;239935;349741 CAMI format : @SampleID: @Version:0.10.0 @Ranks:superkingdom|phylum|class|order|family|genus|species|strain @TaxonomyID:ncbi-taxonomy @@TAXID RANK TAXPATH TAXPATHSN PERCENTAGE 2 superkingdom 2 Bacteria 100.000000 1224 phylum 2|1224 Bacteria|Proteobacteria 99.530189 74201 phylum 2|74201 Bacteria|Verrucomicrobia 0.469811 1236 class 2|1224|1236 Bacteria|Proteobacteria|Gammaproteobacteria 99.530189 203494 class 2|74201|203494 Bacteria|Verrucomicrobia|Verrucomicrobiae 0.469811 91347 order 2|1224|1236|91347 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales 99.530189 48461 order 2|74201|203494|48461 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales 0.469811 543 family 2|1224|1236|91347|543 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae 99.530189 1647988 family 2|74201|203494|48461|1647988 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae 0.469811 561 genus 2|1224|1236|91347|543|561 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia 99.530189 239934 genus 2|74201|203494|48461|1647988|239934 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia 0.469811 562 species 2|1224|1236|91347|543|561|562 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli 99.530189 239935 species 2|74201|203494|48461|1647988|239934|239935 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila 0.469811 431946 strain 2|1224|1236|91347|543|561|562|431946 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli SE15 48.321535 83333 strain 2|1224|1236|91347|543|561|562|83333 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli K-12 46.194629 386585 strain 2|1224|1236|91347|543|561|562|386585 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli O157:H7 str. Sakai 5.014025 349741 strain 2|74201|203494|48461|1647988|239934|239935|349741 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila|Akkermansia muciniphila ATCC BAA-835 0.469811 Related tools: taxonkit profile2cami can convert any metagenomic profile table with TaxIds to CAMI format. taxonkit cami-filter can remove taxa of given TaxIds and their descendants in CAMI metagenomic profile. Metaphlan3 format ( --metaphlan-report ): #SampleID #clade_name NCBI_tax_id relative_abundance additional_species k__Bacteria 2 100.000000 k__Bacteria|p__Proteobacteria 1224 99.530189 k__Bacteria|p__Verrucomicrobia 74201 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 1236 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 203494 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 91347 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 48461 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 543 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 1647988 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 561 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 239934 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 562 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 239935 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 431946 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 83333 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 386585 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 349741 0.469811 Metaphlan2 format ( --metaphlan-report-version 2 --metaphlan-report ): #SampleID k__Bacteria 100.000000 k__Bacteria|p__Proteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 0.469811 Binning result : # This is the bioboxes.org binning output format at # https://github.com/bioboxes/rfc/tree/master/data-format @Version:0.10.0 @SampleID: @@SEQUENCEID TAXID NC_000913.3_sliding:1244941-1245090 511145 NC_013654.1_sliding:344871-345020 562 NC_000913.3_sliding:3801041-3801190 511145 NC_013654.1_sliding:752751-752900 562 NC_000913.3_sliding:4080871-4081020 562 NC_000913.3_sliding:3588091-3588240 511145 NC_000913.3_sliding:2249621-2249770 562 NC_013654.1_sliding:2080171-2080320 431946 NC_000913.3_sliding:2354841-2354990 511145 NC_013654.1_sliding:437671-437820 431946","title":"Taxonomic profiling"},{"location":"tutorial/profiling/#metagenomic-profiling","text":"","title":"Metagenomic profiling"},{"location":"tutorial/profiling/#requirements","text":"Database Prebuilt databases are available. Or build custom databases . Hardware. CPU: \u2265 32 cores preferred. RAM: \u2265 64 GB, depends on file size of the maximal database.","title":"Requirements"},{"location":"tutorial/profiling/#datasets","text":"Short reads, single or paired end.","title":"Datasets"},{"location":"tutorial/profiling/#steps","text":"","title":"Steps"},{"location":"tutorial/profiling/#step-1-preprocessing-reads","text":"For example, removing adapters and trimming using fastp : fastp -i in_1.fq.gz -I in_2.fq.gz \\ -o out_1.fq.gz -O out_2.fq.gz \\ -l 75 -q 20 -W 4 -M 20 -3 20 --thread 32 \\ --html out.fastp.html","title":"Step 1. Preprocessing reads"},{"location":"tutorial/profiling/#step-2-removing-host-reads","text":"Tools: bowtie2 is recommended for removing host reads. samtools is also used for processing reads mapping file. pigz is a parallel implementation of gzip , which is much faster than gzip . Host reference genomes: Human: CHM13 . We also provide a database of CHM13 for fast removing human reads. Building the index (~60min): bowtie2-build --threads 32 GCA_009914755.3_CHM13_T2T_v1.1_genomic.fna.gz chm13 Mapping and removing mapped reads: index=~/ws/db/bowtie2/chm13 bowtie2 --threads 32 -x $index -1 in_1.fq.gz -2 in_2.fq.gz \\ | samtools view -buS -f 4 - \\ | samtools fastq - \\ | gzip -c > sample.fq.gz","title":"Step 2. Removing host reads"},{"location":"tutorial/profiling/#step-3-searching","text":"Reads can be searched against multiple databases which can be built with different parameters , and the results can be fastly merged for downstream analysis.","title":"Step 3. Searching"},{"location":"tutorial/profiling/#attentions","text":"Input format should be (gzipped) FASTA or FASTQ from files or stdin. Paired-End reads should be given via -1/--read1 and -2/--read2 . kmcp search -d db -1 read_1.fq.gz -2 read_2.fq.gz -o read.tsv.gz Single-end can be given as positional arguments or -1 / -2 . kmcp search -d db file1.fq.gz file2.fq.gz -o result.tsv.gz Single-end mode is recommended for paired-end reads, for higher sensitivity . See benchmark . A long query sequence may contain duplicated k-mers, which are not removed for short sequences by default. You may modify the value of -u/--kmer-dedup-threshold (default 256 ) to remove duplicates. For long reads or contigs, you should split them into short reads using seqkit sliding , e.g., seqkit sliding -s 100 -W 300 The values of tCov and jacc in results only apply to databases built with a single size of k-mer. kmcp search and kmcp profile share some flags , therefore users can use stricter criteria in kmcp profile . -t/--min-query-cov , minimal query coverage, i.e., proportion of matched k-mers and unique k-mers of a query (default 0.55 , close to ~96.5% sequence similarity)","title":"Attentions"},{"location":"tutorial/profiling/#index-files-loading-modes","text":"Using memory-mapped index files with mmap (default): Faster startup speed when index files are buffered in memory. Multiple KMCP processes can share the memory. Loading the whole index files into memory ( -w/--load-whole-db ): This mode occupies a little more memory. And multiple KMCP processes can not share the database in memory. It's slightly faster due to the use of physically contiguous memory. The speedup is more significant for smaller databases . Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS) . Low memory mode ( --low-mem ): Do not load all index files into memory nor use mmap, using file seeking. It's much slower, >4X slower on SSD and would be much slower on HDD disks. Only use this mode for small number of queries or a huge database that can't be loaded into memory . Performance tips : Increase the value of -j/--threads for acceleratation, but values larger than the the number of CPU cores won't bring extra speedup.","title":"Index files loading modes"},{"location":"tutorial/profiling/#commands","text":"Single-end mode is recommended for paired-end reads, for higher sensitivity : # --------------------------------------------------- # single-end (recommended) read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ $read1 $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz Pair-end reads: # --------------------------------------------------- # paired-end read1=sample_1.fq.gz read2=sample_2.fq.gz sample=sample # 1. searching results against multiple databases for db in refseq-fungi.kmcp genbank-viral.kmcp gtdb.kmcp ; do dbname=$(basename $db) kmcp search \\ --threads 32 \\ --db-dir $db \\ --min-kmers 10 \\ --min-query-len 30 \\ --min-query-cov 0.55 \\ --read1 $read1 \\ --read2 $read2 \\ --out-file $sample.kmcp@$dbname.tsv.gz \\ --log $sample.kmcp@$dbname.tsv.gz.log done # 2. Merging search results against multiple databases kmcp merge $sample.kmcp@*.tsv.gz --out-file $sample.kmcp.tsv.gz","title":"Commands"},{"location":"tutorial/profiling/#search-result-format","text":"Tab-delimited format with 15 columns: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging Note: The header line starts with # , you need to assign another comment charactor if using csvtk for analysis. e.g., csvtk filter2 -C '$' -t -f '$qCov > 0.7' mock.fastq.gz.kmcp.gz Demo result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx NC_000913.3_sliding:1244941-1245090 150 120 1.5955e-26 6 NC_012971.2 2 10 4558953 31 120 1.0000 0.0003 0.0003 0 NC_000913.3_sliding:1244941-1245090 150 120 1.5955e-26 6 NC_000913.3 2 10 4641652 31 120 1.0000 0.0003 0.0003 0 NC_000913.3_sliding:1244941-1245090 150 120 1.5955e-26 6 NC_018658.1 5 10 5273097 31 120 1.0000 0.0002 0.0002 0 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_012971.2 0 10 4558953 31 120 1.0000 0.0003 0.0003 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_000913.3 0 10 4641652 31 120 1.0000 0.0003 0.0003 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_013654.1 0 10 4717338 31 120 1.0000 0.0003 0.0003 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NZ_CP007592.1 1 10 5104557 31 120 1.0000 0.0002 0.0002 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NC_018658.1 7 10 5273097 31 120 1.0000 0.0002 0.0002 1 NC_013654.1_sliding:344871-345020 150 120 1.5955e-26 8 NZ_CP028116.1 0 10 5648177 31 120 1.0000 0.0002 0.0002 1","title":"Search result format"},{"location":"tutorial/profiling/#searching-on-a-computer-cluster","text":"Here, we split genomes of GTDB into 16 partitions and build a database for every partition, so we can use computer cluster to accelerate the searching. The genbank-viral genomes are also diveded into 4 partition. A helper script easy_sbatch is used for batch submitting Slurm jobs via script templates. # --------------------------------------------------- # searching j=32 reads=reads # ----------------- # gtdb dbprefix=~/ws/db/kmcp/gtdb.n16- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # viral dbprefix=~/ws/db/kmcp/genbank-viral.n4- for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # ----------------- # fungi dbprefix=~/ws/db/kmcp/refseq-fungi for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') read1=$file read2=$(echo $file | sed 's/left.fq.gz/right.fq.gz/') ls -d $dbprefix*.kmcp \\ | easy_sbatch \\ -c $j -J $(basename $prefix) \\ \"kmcp search \\ --load-whole-db \\ --threads $j \\ --db-dir {} \\ $read1 $read2 \\ --out-file $prefix.kmcp@\\$(basename {}).tsv.gz \\ --log $prefix.kmcp@\\$(basename {}).tsv.gz.log \\ --quiet \" done # --------------------------------------------------- # wait all job being done # --------------------------------------------------- # merge result and profiling # merge results # there's no need to submit to slurm, which could make it slower, cause the bottleneck is file IO for file in $reads/*.left.fq.gz; do prefix=$(echo $file | sed 's/.left.fq.gz//') echo $prefix; date kmcp merge $prefix.kmcp@*.tsv.gz --out-file $prefix.kmcp.tsv.gz \\ --quiet --log $prefix.kmcp.tsv.gz.merge.log done # profiling X=taxdump/ T=taxid.map fd kmcp.tsv.gz$ $reads/ \\ | rush -v X=$X -v T=$T \\ 'kmcp profile -X {X} -T {T} {} -o {}.k.profile -C {}.c.profile -s {%:} \\ --log {}.k.profile.log'","title":"Searching on a computer cluster"},{"location":"tutorial/profiling/#step-4-profiling","text":"","title":"Step 4. Profiling"},{"location":"tutorial/profiling/#input","text":"TaxId mapping file(s). Taxdump files. KMCP search results.","title":"Input"},{"location":"tutorial/profiling/#methods","text":"Reference genomes can be split into chunks when computing k-mers (sketches), which could help to increase the specificity via a threshold, i.e., the minimal proportion of matched chunks ( -p/--min-chunks-fraction ). ( highly recommended ) Another flag -d/--max-chunks-cov-stdev further reduces false positives. We require a part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence for decreasing the false positive rate. We also use the two-stage taxonomy assignment algorithm in MegaPath to reduce the false positive of ambiguous matches. You can also disable this step by the flag --no-amb-corr . If stage 1/4 produces thousands of candidates, you can use the flag --no-amb-corr to reduce analysis time, which has very little effect on the results. Multi-aligned queries are proportionally assigned to references with a similar strategy in Metalign . Input files are parsed 4 times, therefore STDIN is not supported. Three-rounds profiling: Accuracy notes : Smaller -t/--min-qcov increase sensitivity at the cost of higher false positive rate ( -f/--max-fpr ) of a query. And we require part of the uniquely matched reads of a reference having high similarity, i.e., with high confidence to decrease the false positive. E.g., -H >= 0.8 and -P >= 0.1 equals to 90th percentile >= 0.8 -U/--min-hic-ureads , minimal number, >= 1 -H/--min-hic-ureads-qcov , minimal query coverage, >= -t/--min-qcov -P/--min-hic-ureads-prop , minimal proportion, higher values increase precision at the cost of sensitivity. -R/--max-mismatch-err and -D/--min-dreads-prop is for determing the right reference for ambigous reads with the algorithm in MegaPath. --keep-perfect-match is not recommended, which decreases sensitivity. -n/--keep-top-qcovs is not recommended, which affects accuracy of abundance estimation.","title":"Methods"},{"location":"tutorial/profiling/#profiling-modes","text":"We preset six profiling modes, available with the flag -m/--mode . 0 (for pathogen detection) 1 (higher recall) 2 (high recall) 3 (default) 4 (high precision) 5 (higher precision) You can still change the values of some options below as usual. options m=0 m=1 m=2 m=3 m=4 m=5 --------------------------- ---- --- --- ---- --- ---- -r/--min-chunks-reads 1 5 10 50 100 100 -p/--min-chunks-fraction 0.2 0.6 0.7 0.8 1 1 -d/--max-chunks-depth-stdev 10 2 2 2 2 1.5 -u/--min-uniq-reads 1 2 5 20 50 50 -U/--min-hic-ureads 1 1 2 5 10 10 -H/--min-hic-ureads-qcov 0.55 0.7 0.7 0.75 0.8 0.8 -P/--min-hic-ureads-prop 0.01 0.1 0.2 0.1 0.1 0.15 --keep-main-matches true --max-qcov-gap 0.4 Taxonomy data : Mapping references IDs to TaxIds: -T/--taxid-map NCBI taxonomy dump files: -X/--taxdump For databases built with a custom genome collection, you can use taxonkit create-taxdump to create NCBI-style taxdump files, which also generates a TaxId mapping file. Performance notes : Searching results are parsed in parallel, and the number of lines proceeded by a thread can be set by the flag --line-chunk-size . However using a lot of threads does not always accelerate processing, 4 threads with a chunk size of 500-5000 is fast enough. If stage 1/4 produces thousands of candidates, then stage 2/4 would be very slow. You can use the flag --no-amb-corr to disable ambiguous reads correction which has very little effect on the results.","title":"Profiling modes"},{"location":"tutorial/profiling/#commands_1","text":"# taxid mapping files, multiple files supported. taxid_map=gtdb.kmcp/taxid.map,refseq-viral.kmcp/taxid.map,refseq-fungi.kmcp/taxid.map # or concatenate them into a big taxid.map # cat gtdb.kmcp/taxid.map refseq-viral.kmcp/taxid.map refseq-fungi.kmcp/taxid.map > taxid.map # taxid_map=taxid.map # taxdump directory taxdump=taxdump sfile=$file.kmcp.tsv.gz kmcp profile \\ --taxid-map $taxid_map \\ --taxdump $taxdump/ \\ --level species \\ --min-query-cov 0.55 \\ --min-chunks-reads 50 \\ --min-chunks-fraction 0.8 \\ --max-chunks-depth-stdev 2 \\ --min-uniq-reads 20 \\ --min-hic-ureads 5 \\ --min-hic-ureads-qcov 0.75 \\ --min-hic-ureads-prop 0.1 \\ $sfile \\ --out-prefix $sfile.kmcp.profile \\ --metaphlan-report $sfile.metaphlan.profile \\ --cami-report $sfile.cami.profile \\ --sample-id \"0\" \\ --binning-result $sfile.binning.gz","title":"Commands"},{"location":"tutorial/profiling/#profiling-result-formats","text":"Supported profiling output formats : KMCP ( -o/--out-prefix ). Note that: abundances are only computed for target references rather than each taxon at all taxonomic ranks, so please output CAMI or MetaPhlAn format . CAMI ( -M/--metaphlan-report , --metaphlan-report-version , sample name: -s/--sample-id , taxonomy data: --taxonomy-id ) MetaPhlAn ( -C/--cami-report , sample name: -s/--sample-id )) Supported taxonomic binning formats : CAMI ( -B/--binning-result ) KMCP format : Tab-delimited format with 16 columns: 1. ref, Identifier of the reference genome 2. percentage, Relative abundance of the reference 3. score, The 90th percentile of qCov of uniquely matched reads 4. chunksFrac, Genome chunks fraction 5. chunksRelDepth, Relative depths of reference chunks 6. chunksRelDepthStd, The strandard deviation of chunksRelDepth 7. reads, Total number of matched reads of this reference 8. ureads, Number of uniquely matched reads 9. hicureads, Number of uniquely matched reads with high-confidence 10. refsize, Reference size 11. refname, Reference name, optional via name mapping file 12. taxid, TaxId of the reference 13. rank, Taxonomic rank 14. taxname, Taxonomic name 15. taxpath, Complete lineage 16. taxpathsn, Corresponding TaxIds of taxa in the complete lineage Demo output: ref percentage score chunksFrac chunksRelDepth chunksRelDepthStd reads ureads hicureads refsize refname taxid rank taxname taxpath taxpathsn NC_013654.1 48.321535 100.00 1.00 0.99;1.00;0.99;1.00;0.99;0.99;0.99;0.98;1.05;1.02 0.02 287936 226225 226225 4717338 431946 strain Escherichia coli SE15 Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli;Escherichia coli SE15 2;1224;1236;91347;543;561;562;431946 NC_000913.3 46.194629 100.00 1.00 1.04;0.99;1.00;1.00;0.99;0.99;0.99;0.97;1.04;0.98 0.02 270846 175686 175686 4641652 511145 no rank Escherichia coli str. K-12 substr. MG1655 Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli;Escherichia coli K-12 2;1224;1236;91347;543;561;562;83333 NC_002695.2 5.014025 100.00 1.00 0.97;0.98;0.92;1.12;1.01;0.95;1.00;1.01;1.03;1.00 0.05 34825 22945 22945 5498578 386585 strain Escherichia coli O157:H7 str. Sakai Bacteria;Proteobacteria;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia;Escherichia coli;Escherichia coli O157:H7 str. Sakai 2;1224;1236;91347;543;561;562;386585 NC_010655.1 0.469811 100.00 1.00 1.03;0.87;0.90;0.98;1.15;1.17;0.90;0.96;0.96;1.09 0.10 1581 1581 1581 2664102 349741 strain Akkermansia muciniphila ATCC BAA-835 Bacteria;Verrucomicrobia;Verrucomicrobiae;Verrucomicrobiales;Akkermansiaceae;Akkermansia;Akkermansia muciniphila;Akkermansia muciniphila ATCC BAA-835 2;74201;203494;48461;1647988;239934;239935;349741 CAMI format : @SampleID: @Version:0.10.0 @Ranks:superkingdom|phylum|class|order|family|genus|species|strain @TaxonomyID:ncbi-taxonomy @@TAXID RANK TAXPATH TAXPATHSN PERCENTAGE 2 superkingdom 2 Bacteria 100.000000 1224 phylum 2|1224 Bacteria|Proteobacteria 99.530189 74201 phylum 2|74201 Bacteria|Verrucomicrobia 0.469811 1236 class 2|1224|1236 Bacteria|Proteobacteria|Gammaproteobacteria 99.530189 203494 class 2|74201|203494 Bacteria|Verrucomicrobia|Verrucomicrobiae 0.469811 91347 order 2|1224|1236|91347 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales 99.530189 48461 order 2|74201|203494|48461 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales 0.469811 543 family 2|1224|1236|91347|543 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae 99.530189 1647988 family 2|74201|203494|48461|1647988 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae 0.469811 561 genus 2|1224|1236|91347|543|561 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia 99.530189 239934 genus 2|74201|203494|48461|1647988|239934 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia 0.469811 562 species 2|1224|1236|91347|543|561|562 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli 99.530189 239935 species 2|74201|203494|48461|1647988|239934|239935 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila 0.469811 431946 strain 2|1224|1236|91347|543|561|562|431946 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli SE15 48.321535 83333 strain 2|1224|1236|91347|543|561|562|83333 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli K-12 46.194629 386585 strain 2|1224|1236|91347|543|561|562|386585 Bacteria|Proteobacteria|Gammaproteobacteria|Enterobacterales|Enterobacteriaceae|Escherichia|Escherichia coli|Escherichia coli O157:H7 str. Sakai 5.014025 349741 strain 2|74201|203494|48461|1647988|239934|239935|349741 Bacteria|Verrucomicrobia|Verrucomicrobiae|Verrucomicrobiales|Akkermansiaceae|Akkermansia|Akkermansia muciniphila|Akkermansia muciniphila ATCC BAA-835 0.469811 Related tools: taxonkit profile2cami can convert any metagenomic profile table with TaxIds to CAMI format. taxonkit cami-filter can remove taxa of given TaxIds and their descendants in CAMI metagenomic profile. Metaphlan3 format ( --metaphlan-report ): #SampleID #clade_name NCBI_tax_id relative_abundance additional_species k__Bacteria 2 100.000000 k__Bacteria|p__Proteobacteria 1224 99.530189 k__Bacteria|p__Verrucomicrobia 74201 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 1236 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 203494 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 91347 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 48461 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 543 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 1647988 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 561 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 239934 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 562 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 239935 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 431946 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 83333 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 386585 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 349741 0.469811 Metaphlan2 format ( --metaphlan-report-version 2 --metaphlan-report ): #SampleID k__Bacteria 100.000000 k__Bacteria|p__Proteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli 99.530189 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila 0.469811 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli SE15 48.321535 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli K-12 46.194629 k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Enterobacterales|f__Enterobacteriaceae|g__Escherichia|s__Escherichia coli|t__Escherichia coli O157:H7 str. Sakai 5.014025 k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Akkermansiaceae|g__Akkermansia|s__Akkermansia muciniphila|t__Akkermansia muciniphila ATCC BAA-835 0.469811 Binning result : # This is the bioboxes.org binning output format at # https://github.com/bioboxes/rfc/tree/master/data-format @Version:0.10.0 @SampleID: @@SEQUENCEID TAXID NC_000913.3_sliding:1244941-1245090 511145 NC_013654.1_sliding:344871-345020 562 NC_000913.3_sliding:3801041-3801190 511145 NC_013654.1_sliding:752751-752900 562 NC_000913.3_sliding:4080871-4081020 562 NC_000913.3_sliding:3588091-3588240 511145 NC_000913.3_sliding:2249621-2249770 562 NC_013654.1_sliding:2080171-2080320 431946 NC_000913.3_sliding:2354841-2354990 511145 NC_013654.1_sliding:437671-437820 431946","title":"Profiling result formats"},{"location":"tutorial/searching/","text":"Sequence and genome searching Using cases Searching sequences against raws reads. For checking existence: Building database with raws reads and searching with query sequences, optionally using k-mer sketches for long query sequences. For abundance estimation: For long sequences (similar to taxonomic profiling): Building database with query sequences, searching with raws reads, and profiling. For short (shorter than reads length) sequences: Not capable. Searching sequences against assemblies/genomes. For checking existence: For short sequences (short reads, e.g., detecting host contamination): Building database with assemblies/genomes and searching with raws reads. For long sequences: Building database with assemblies/genomes using k-mer sketches. For genome similarity estimation: Building database with assemblies/genomes using k-mer sketches. For taxonomic profiling: For short reads: Building database with assemblies/genomes, searching with raws reads, and profiling. For long reads: Split long reads into short reads before searching. Sequence search KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. KMCP reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a small database size and much faster searching speed (check the benchmark ). Step 1. Building databases The database building process is similar to that of metagenomic profiling, with one difference: Reference genomes are not splitted into chunks. Taken GTDB for example: # mask low-complexity region (optional) mkdir -p gtdb.masked find gtdb/ -name \"*.fna.gz\" \\ | rush 'dustmasker -in <(zcat {}) -outfmt fasta \\ | sed -e \"/^>/!s/[a-z]/n/g\" \\ | gzip -c > gtdb.masked/{%}' # compute k-mers # sequences containing \"plasmid\" in name are ignored, # k = 21 kmcp compute -I gtdb.masked/ -k 21 -B plasmid -O gtdb-r202-k21 # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-r202-k21 -O gtdb-r202-k21 -n 1 -f 0.3 # cp name mapping file to database directory cp name.map gtdb-r202-k21.kmcp/ The size of database is 56GB. Step 2. Searching By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS) kmcp search supports FASTA/Q format from STDIN or files ( usage ). kmcp search -d gtdb.kmcp/ test.fq.gz -o result.tsv.gz 22:21:26.017 [INFO] kmcp v0.8.0 22:21:26.017 [INFO] https://github.com/shenwei356/kmcp 22:21:26.017 [INFO] 22:21:26.017 [INFO] checking input files ... 22:21:26.017 [INFO] 1 input file(s) given 22:21:26.018 [INFO] loading database with mmap enabled ... 22:21:26.018 [INFO] number of extra workers for every index file: 4 22:21:26.328 [INFO] database loaded: gtdb.kmcp/ 22:21:26.328 [INFO] 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] minimum query length: 30 22:21:26.328 [INFO] minimum matched k-mers: 10 22:21:26.328 [INFO] minimum query coverage: 0.550000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] 22:21:26.328 [INFO] searching ... 22:21:26.328 [INFO] reading sequence file: test.fq.gz 22:21:26.376 [INFO] 22:21:26.376 [INFO] processed queries: 10, speed: 0.012 million queries per minute 22:21:26.376 [INFO] 90.0000% (9/10) queries matched 22:21:26.376 [INFO] done searching 22:21:26.423 [INFO] 22:21:26.423 [INFO] elapsed time: 405.849288ms 22:21:26.423 [INFO] The result is tab-delimited. #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx S0R0/1 150 130 3.1777e-06 2 GCF_002872255.1 5 10 2583551 21 87 0.6692 0.0003 0.0003 0 S0R0/1 150 130 1.1936e-03 2 GCF_001434585.1 6 10 2221511 21 74 0.5692 0.0003 0.0003 0 S0R0/2 150 130 6.2145e-07 4 GCF_002872255.1 5 10 2583551 21 90 0.6923 0.0004 0.0004 1 S0R0/2 150 130 3.1777e-06 4 GCF_001434585.1 6 10 2221511 21 87 0.6692 0.0004 0.0004 1 S0R0/2 150 130 2.4002e-05 4 GCF_001438655.1 5 10 2038820 21 83 0.6385 0.0004 0.0004 1 Reference IDs (column target ) can be optionally mapped to their names during searching: kmcp search -d gtdb-r202-k21.kmcp/ -N name.map test.fq.gz -o result.tsv.gz Or after searching using csvtk : csvtk replace -t -C $ -f target -k name.map -p '(.+)' -r '{kv}' result.tsv.gz Print the main columns only: csvtk head -t -C $ -n 5 result.tsv.gz \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,FPR,qCov,target \\ | csvtk csv2md -t query FPR qCov target S0R0/1 3.1777e-06 0.6692 GCF_002872255.1 S0R0/1 1.1936e-03 0.5692 GCF_001434585.1 S0R0/2 6.2145e-07 0.6923 GCF_002872255.1 S0R0/2 3.1777e-06 0.6692 GCF_001434585.1 S0R0/2 2.4002e-05 0.6385 GCF_001438655.1 Genome similarity estimation KMCP can be used for fast similarity estimation of newly assembled genome against known reference genomes . BLAST is an option while it focuses on local similarity. Besides, the database is big and the searching speed is relative slow. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Scaled MinHash) (Sourmash). Here KMCP utilizes multiple sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ) and Syncmers ) for genome similarity estimation. Prebuilt databases are available and users can also build custom databases following steps below. Step 1. Building databases The database building process is similar to that of metagenomic profiling, with few differences: K-mer sketches instead of all k-mers are computed. Reference genomes are not splitted into chunks. Smaller false positive rate ( -f ) and more than one hash functions ( -n ) are used to improve accuracy. Using a bigger k-mer size: 31. Supported k-mer (sketches) types: K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): FracMinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Taken GTDB for example: # compute FracMinHash (Scaled MinHash) with scale 1000 # sequence containing \"plasmid\" in name are ignored, # k = 31 kmcp compute -I gtdb/ -k 31 -D 1000 -B plasmid -O gtdb-r202-minhash # build database # number of index files: 8, for server with >= 8 CPU cores # bloom filter parameter: # number of hash function: 3 # false positive rate: 0.001 kmcp index -j 8 -I gtdb-r202-minhash -O gtdb.minhash.kmcp -n 3 -f 0.001 # cp name mapping file to database directory cp taxid.map name.map gtdb.minhash.kmcp/ Attention: For small genomes like viruses, sketching parameters should be adjusted. For examples, setting a smaller scale like 10. Step 2. Searching The searching process is simple and very fast (<1 second). kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genomme1 contigs.fasta -o result.tsv The output is in tab-delimited format: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging A full search result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx genomme1 9488952 18737 0.0000e+00 2 GCF_000742135.1 0 1 5545784 31 8037 0.4289 0.7365 0.3719 0 genomme1 9488952 18737 3.1964e-183 2 GCF_000392875.1 0 1 2881400 31 3985 0.2127 0.7062 0.1954 0 Reference IDs can be optionally mapped to their names, let's print the main columns only: kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genomme1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target \\ > result.tsv query jacc target genomme1 0.3719 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genomme1 0.1954 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence Using closed syncmer: kmcp search --query-whole-file -d gtdb.syncmer.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genomme1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target query jacc target genomme1 0.3712 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genomme1 0.1974 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence","title":"Sequence and genome searching"},{"location":"tutorial/searching/#sequence-and-genome-searching","text":"","title":"Sequence and genome searching"},{"location":"tutorial/searching/#using-cases","text":"Searching sequences against raws reads. For checking existence: Building database with raws reads and searching with query sequences, optionally using k-mer sketches for long query sequences. For abundance estimation: For long sequences (similar to taxonomic profiling): Building database with query sequences, searching with raws reads, and profiling. For short (shorter than reads length) sequences: Not capable. Searching sequences against assemblies/genomes. For checking existence: For short sequences (short reads, e.g., detecting host contamination): Building database with assemblies/genomes and searching with raws reads. For long sequences: Building database with assemblies/genomes using k-mer sketches. For genome similarity estimation: Building database with assemblies/genomes using k-mer sketches. For taxonomic profiling: For short reads: Building database with assemblies/genomes, searching with raws reads, and profiling. For long reads: Split long reads into short reads before searching.","title":"Using cases"},{"location":"tutorial/searching/#sequence-search","text":"KMCP can be used for fast sequence search against large scales of genomic datasets as BIGSI and COBS do. KMCP reimplemented and modified the Compact Bit-Sliced Signature index (COBS) algorithm, bringing a small database size and much faster searching speed (check the benchmark ).","title":"Sequence search"},{"location":"tutorial/searching/#step-1-building-databases","text":"The database building process is similar to that of metagenomic profiling, with one difference: Reference genomes are not splitted into chunks. Taken GTDB for example: # mask low-complexity region (optional) mkdir -p gtdb.masked find gtdb/ -name \"*.fna.gz\" \\ | rush 'dustmasker -in <(zcat {}) -outfmt fasta \\ | sed -e \"/^>/!s/[a-z]/n/g\" \\ | gzip -c > gtdb.masked/{%}' # compute k-mers # sequences containing \"plasmid\" in name are ignored, # k = 21 kmcp compute -I gtdb.masked/ -k 21 -B plasmid -O gtdb-r202-k21 # build database # number of index files: 32, for server with >= 32 CPU cores # bloom filter parameter: # number of hash function: 1 # false positive rate: 0.3 kmcp index -j 32 -I gtdb-r202-k21 -O gtdb-r202-k21 -n 1 -f 0.3 # cp name mapping file to database directory cp name.map gtdb-r202-k21.kmcp/ The size of database is 56GB.","title":"Step 1. Building databases"},{"location":"tutorial/searching/#step-2-searching","text":"By default, kmcp search loads the whole database into main memory (RAM) for fast searching. Optionally, the flag --low-mem can be set to avoid loading the whole database, while it's much slower, >10X slower on SSD and should be much slower on HDD disks. Please switch on this flag when searching on computer clusters, where the default mmap mode would be very slow for network-attached storage (NAS) kmcp search supports FASTA/Q format from STDIN or files ( usage ). kmcp search -d gtdb.kmcp/ test.fq.gz -o result.tsv.gz 22:21:26.017 [INFO] kmcp v0.8.0 22:21:26.017 [INFO] https://github.com/shenwei356/kmcp 22:21:26.017 [INFO] 22:21:26.017 [INFO] checking input files ... 22:21:26.017 [INFO] 1 input file(s) given 22:21:26.018 [INFO] loading database with mmap enabled ... 22:21:26.018 [INFO] number of extra workers for every index file: 4 22:21:26.328 [INFO] database loaded: gtdb.kmcp/ 22:21:26.328 [INFO] 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] minimum query length: 30 22:21:26.328 [INFO] minimum matched k-mers: 10 22:21:26.328 [INFO] minimum query coverage: 0.550000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] minimum target coverage: 0.000000 22:21:26.328 [INFO] -------------------- [main parameters] -------------------- 22:21:26.328 [INFO] 22:21:26.328 [INFO] searching ... 22:21:26.328 [INFO] reading sequence file: test.fq.gz 22:21:26.376 [INFO] 22:21:26.376 [INFO] processed queries: 10, speed: 0.012 million queries per minute 22:21:26.376 [INFO] 90.0000% (9/10) queries matched 22:21:26.376 [INFO] done searching 22:21:26.423 [INFO] 22:21:26.423 [INFO] elapsed time: 405.849288ms 22:21:26.423 [INFO] The result is tab-delimited. #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx S0R0/1 150 130 3.1777e-06 2 GCF_002872255.1 5 10 2583551 21 87 0.6692 0.0003 0.0003 0 S0R0/1 150 130 1.1936e-03 2 GCF_001434585.1 6 10 2221511 21 74 0.5692 0.0003 0.0003 0 S0R0/2 150 130 6.2145e-07 4 GCF_002872255.1 5 10 2583551 21 90 0.6923 0.0004 0.0004 1 S0R0/2 150 130 3.1777e-06 4 GCF_001434585.1 6 10 2221511 21 87 0.6692 0.0004 0.0004 1 S0R0/2 150 130 2.4002e-05 4 GCF_001438655.1 5 10 2038820 21 83 0.6385 0.0004 0.0004 1 Reference IDs (column target ) can be optionally mapped to their names during searching: kmcp search -d gtdb-r202-k21.kmcp/ -N name.map test.fq.gz -o result.tsv.gz Or after searching using csvtk : csvtk replace -t -C $ -f target -k name.map -p '(.+)' -r '{kv}' result.tsv.gz Print the main columns only: csvtk head -t -C $ -n 5 result.tsv.gz \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,FPR,qCov,target \\ | csvtk csv2md -t query FPR qCov target S0R0/1 3.1777e-06 0.6692 GCF_002872255.1 S0R0/1 1.1936e-03 0.5692 GCF_001434585.1 S0R0/2 6.2145e-07 0.6923 GCF_002872255.1 S0R0/2 3.1777e-06 0.6692 GCF_001434585.1 S0R0/2 2.4002e-05 0.6385 GCF_001438655.1","title":"Step 2. Searching"},{"location":"tutorial/searching/#genome-similarity-estimation","text":"KMCP can be used for fast similarity estimation of newly assembled genome against known reference genomes . BLAST is an option while it focuses on local similarity. Besides, the database is big and the searching speed is relative slow. Genome sketching is a method of utilizing small and approximate summaries of genomic data for fast searching and comparison. Mash and Sourmash provide fast genome distance estimation using MinHash (Mash) or FracMinHash (Scaled MinHash) (Sourmash). Here KMCP utilizes multiple sketches ( Minimizer , FracMinHash (previously named Scaled MinHash ) and Syncmers ) for genome similarity estimation. Prebuilt databases are available and users can also build custom databases following steps below.","title":"Genome similarity estimation"},{"location":"tutorial/searching/#step-1-building-databases_1","text":"The database building process is similar to that of metagenomic profiling, with few differences: K-mer sketches instead of all k-mers are computed. Reference genomes are not splitted into chunks. Smaller false positive rate ( -f ) and more than one hash functions ( -n ) are used to improve accuracy. Using a bigger k-mer size: 31. Supported k-mer (sketches) types: K-mer: ntHash of k-mer ( -k ) K-mer sketchs (all using ntHash): FracMinHash ( -k -D ), previously named Scaled MinHash Minimizer ( -k -W ), optionally scaling/down-sampling ( -D ) Closed Syncmer ( -k -S ), optionally scaling/down-sampling ( -D ) Taken GTDB for example: # compute FracMinHash (Scaled MinHash) with scale 1000 # sequence containing \"plasmid\" in name are ignored, # k = 31 kmcp compute -I gtdb/ -k 31 -D 1000 -B plasmid -O gtdb-r202-minhash # build database # number of index files: 8, for server with >= 8 CPU cores # bloom filter parameter: # number of hash function: 3 # false positive rate: 0.001 kmcp index -j 8 -I gtdb-r202-minhash -O gtdb.minhash.kmcp -n 3 -f 0.001 # cp name mapping file to database directory cp taxid.map name.map gtdb.minhash.kmcp/ Attention: For small genomes like viruses, sketching parameters should be adjusted. For examples, setting a smaller scale like 10.","title":"Step 1. Building databases"},{"location":"tutorial/searching/#step-2-searching_1","text":"The searching process is simple and very fast (<1 second). kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genomme1 contigs.fasta -o result.tsv The output is in tab-delimited format: 1. query, Identifier of the query sequence 2. qLen, Query length 3. qKmers, K-mer number of the query sequence 4. FPR, False positive rate of the match 5. hits, Number of matches 6. target, Identifier of the target sequence 7. chunkIdx, Index of reference chunk 8. chunks, Number of reference chunks 9. tLen, Reference length 10. kSize, K-mer size 11. mKmers, Number of matched k-mers 12. qCov, Query coverage, equals to: mKmers / qKmers 13. tCov, Target coverage, equals to: mKmers / K-mer number of reference chunk 14. jacc, Jaccard index 15. queryIdx, Index of query sequence, only for merging A full search result: #query qLen qKmers FPR hits target chunkIdx chunks tLen kSize mKmers qCov tCov jacc queryIdx genomme1 9488952 18737 0.0000e+00 2 GCF_000742135.1 0 1 5545784 31 8037 0.4289 0.7365 0.3719 0 genomme1 9488952 18737 3.1964e-183 2 GCF_000392875.1 0 1 2881400 31 3985 0.2127 0.7062 0.1954 0 Reference IDs can be optionally mapped to their names, let's print the main columns only: kmcp search --query-whole-file -d gtdb.minhash.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genomme1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target \\ > result.tsv query jacc target genomme1 0.3719 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genomme1 0.1954 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence Using closed syncmer: kmcp search --query-whole-file -d gtdb.syncmer.kmcp/ \\ --name-map name.map \\ --query-whole-file --sort-by jacc --min-query-cov 0.2 \\ --query-id genomme1 contigs.fasta \\ | csvtk rename -t -C $ -f 1 -n query \\ | csvtk cut -t -f query,jacc,target query jacc target genomme1 0.3712 NZ_KN046818.1 Klebsiella pneumoniae strain ATCC 13883 scaffold1, whole genome shotgun sequence genomme1 0.1974 NZ_KB944588.1 Enterococcus faecalis ATCC 19433 acAqW-supercont1.1, whole genome shotgun sequence","title":"Step 2. Searching"}]}